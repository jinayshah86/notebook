{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mickey Mouse\u2019s Notebook","text":"<p>This notebook contains notes from wide variety of books or video tutorials.</p>"},{"location":"python/concurrency/","title":"Concurrent Execution","text":""},{"location":"python/concurrency/#introduction","title":"Introduction","text":"<p>A thread can be defined as a sequence of instructions that can be run by a scheduler, which is that part of the operating system that decides which chunk of work will receive the necessary resources to be carried out. Typically, a thread lives within a process. A process can be defined as an instance of a computer program that is being executed.</p>"},{"location":"python/concurrency/#types-of-thread","title":"Types of thread","text":"<ul> <li>User-level threads: Threads that we can create and manage in order to perform a task.</li> <li>Kernel-level threads: Low-level threads that run in kernel mode and act on behalf of the operating system.</li> </ul>"},{"location":"python/concurrency/#states-of-thread","title":"States of thread","text":"<ul> <li>New thread: A thread that hasn\u2019t started yet, and hasn\u2019t been allocated any resources.</li> <li>Runnable: The thread is waiting to run. It has all the resources needed to run, and as soon as the scheduler gives it the green light, it will be run.</li> <li>Running :A thread whose stream of instructions is being executed. From this state, it can go back to a non-running state, or die.</li> <li>Not-running: A thread that has been paused. This could be due to another thread taking precedence over it, or simply because the thread is waiting for a long-running IO operation to finish.</li> <li>Dead: A thread that has died because it has reached the natural end of its stream of execution, or it has been killed.</li> </ul>"},{"location":"python/concurrency/#context-switching","title":"Context switching","text":"<p>The scheduler can decide when a thread can run, or is paused, and so on. Any time a running thread needs to be suspended so that another can be run, the scheduler saves the state of the running thread in a way that it will be possible, at a later time, to resume execution exactly where it was paused. This act is called context-switching.</p> <p>Context-switching is a marvelous ability of modern computers, but it can become troublesome if you generate too many threads. The scheduler then will try to give each of them a chance to run for a little time, and there will be a lot of time spent saving and recovering the state of the threads that are respectively paused and restarted.</p>"},{"location":"python/concurrency/#the-global-interpreter-lock-gil","title":"The Global Interpreter Lock (GIL)","text":"<p>The GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This means that even though you can write multithreaded code in Python, there is only one thread per process running at any point in time.</p> <p>In computer programming, a mutual exclusion object (mutex) is a program object that allows multiple program threads to share the same resource,  such as file access, but not simultaneously.</p>"},{"location":"python/concurrency/#starting-a-thread","title":"Starting a thread","text":"<p>Example 1: <pre><code>import threading\n\ndef sum_and_product(a, b):\n    s, p = a + b, a * b\n    print(f'{a}+{b}={s}, {a}*{b}={p}')\n\nt = threading.Thread(\n    target=sum_and_product, name='SumProd', args=(3, 7)\n)\nt.start()\n</code></pre></p> <p>Output: <pre><code>3+7=10, 3*7=21\n</code></pre></p> <p>Example 2: <pre><code>import threading\nfrom time import sleep\n\ndef sum_and_product(a, b):\n    # There is a reason why I put .2 seconds of sleeping time within the\n    # function. When the thread starts, its first instruction is to sleep for a\n    # moment. The sneaky scheduler will catch that, and switch execution back\n    # to the main thread.     \n    sleep(.2)\n    print_current()\n    s, p = a + b, a * b\n    print(f'{a}+{b}={s}, {a}*{b}={p}')\n\ndef status(t):\n    if t.is_alive():\n        print(f'Thread {t.name} is alive.')\n    else:\n        print(f'Thread {t.name} has terminated.')\n\ndef print_current():\n    print('The current thread is {}.'.format(\n        threading.current_thread()\n    ))\n    print('Threads: {}'.format(list(threading.enumerate())))\n\nprint_current()\nt = threading.Thread(\n    target=sum_and_product, name='SumPro', args=(3, 7)\n)\nt.start()\nstatus(t)\n# `t.join()` instructs Python to block until the thread has completed. The\n# reason for that is because I want the last call to status(t) to tell us\n# that the thread is gone.\nt.join()\nstatus(t) \n</code></pre></p> <p>Output: <pre><code>The current thread is &lt;_MainThread(MainThread, started 140735733822336)&gt;.\nThreads: [&lt;_MainThread(MainThread, started 140735733822336)&gt;]\nThread SumProd is alive.\nThe current thread is &lt;Thread(SumProd, started 123145375604736)&gt;.\nThreads: [\n    &lt;_MainThread(MainThread, started 140735733822336)&gt;,\n    &lt;Thread(SumProd, started 123145375604736)&gt;\n]\n3+7=10, 3*7=21\nThread SumProd has terminated.\n</code></pre></p>"},{"location":"python/concurrency/#starting-a-process","title":"Starting a process","text":"<p>Example: <pre><code># start_proc.py\nimport multiprocessing\n\ndef sum_and_product(a, b):\n    s, p = a + b, a * b\n    print(f'{a}+{b}={s}, {a}*{b}={p}')\n\np = multiprocessing.Process(\n    target=sum_and_product, name='SumProdProc', args=(7, 9)\n)\np.start()\n</code></pre> The output is also the same, except the numbers are different.</p>"},{"location":"python/concurrency/#stopping-a-thread","title":"Stopping a thread","text":"<p>Stopping a thread is a bad idea, and the same goes for a process. Being sure you\u2019ve taken care to dispose and close everything that is open can be quite difficult. However, there are situations in which you might want to be able to stop a thread. </p> <p>Example: <pre><code>import threading\nfrom time import sleep\n\nclass Fibo(threading.Thread):\n    def __init__(self, *a, **kwa):\n        super().__init__(*a, **kwa)\n        self._running = True\n\n    def stop(self):\n        self._running = False\n\n    def run(self):\n        a, b = 0, 1\n        while self._running:\n            print(a, end=' ')\n            a, b = b, a + b\n            sleep(0.07)\n        print()\n\nfibo = Fibo()\nfibo.start()\nsleep(1)\nfibo.stop()\nfibo.join()\nprint('All done.')\n</code></pre></p> <p>Output: <pre><code>0 1 1 2 3 5 8 13 21 34 55 89 144 233\nAll done.\n</code></pre></p> <p>When you write a thread using class, instead of giving it a target function, you simply override the <code>run</code> method in the class. When we call <code>fibo.stop()</code>,  we aren\u2019t actually stopping the thread. We simply set our flag to <code>False</code>,  and this allows the code within <code>run</code> to reach its natural end. This means that the thread will die organically.</p> <p>This is basically a workaround technique that allows you to stop a thread.  If you design your code correctly according to multithreading paradigms, you  shouldn\u2019t have to kill threads all the time, so let that need become your alarm bell that something could be designed better.</p>"},{"location":"python/concurrency/#stopping-a-process","title":"Stopping a process","text":"<p>When it comes to stopping a process, things are different, and fuss-free. You can use either the <code>terminate</code> or <code>kill</code> method, but please make sure you know what you\u2019re doing, as all the preceding considerations about open resources left hanging are still true.</p>"},{"location":"python/concurrency/#swapning-multiple-threads","title":"Swapning multiple threads","text":"<p>Example: <pre><code>import threading\nfrom time import sleep\nfrom random import random\n\ndef run(n):\n    t = threading.current_thread()\n    for count in range(n):\n        print(f'Hello from {t.name}! ({count})')\n        sleep(0.2 * random())\n\nobi = threading.Thread(target=run, name='Obi-Wan', args=(4, ))\nani = threading.Thread(target=run, name='Anakin', args=(3, ))\nobi.start()\nani.start()\nobi.join()\nani.join()\n</code></pre></p> <p>Output: <pre><code>$ python starwars.py\nHello from Obi-Wan! (0)\nHello from Anakin! (0)\nHello from Obi-Wan! (1)\nHello from Obi-Wan! (2)\nHello from Anakin! (1)\nHello from Obi-Wan! (3)\nHello from Anakin! (2)\n</code></pre></p>"},{"location":"python/concurrency/#race-conditions","title":"Race conditions","text":"<p>A race condition is a behavior of a system where the output of a procedure depends on the sequence or timing of other uncontrollable events. When these events don\u2019t unfold in the order intended by the programmer, a race condition becomes a bug.</p> <p>Imagine you have two threads running. Both are performing the same task,  which consists of reading a value from a location, performing an action with  that value, incrementing the value by 1 unit, and saving it back. Say that  the action is to post that value to an API.</p> <p>Scenario A \u2013 race condition not happening</p> <p>Thread A reads the value (1), posts 1 to the API, then increments  it to 2, and saves it back. Right after this, the scheduler pauses  Thread A, and runs Thread B. Thread B reads the value (now 2),  posts 2 to the API, increments it to 3, and saves it back.</p> <p>At this point, after the operation has happened twice, the value stored is  correct: 1 + 2 = 3. Moreover, the API has been called with both 1 and  2, correctly.</p> <p>Scenario B \u2013 race condition happening</p> <p>Thread A reads the value (1), posts it to the API, increments it to 2, but before it can save it back, the scheduler decides to pause thread A in favor of Thread B. Thread B reads the value (still 1!), posts it to the API,increments  it to 2, and saves it back. The scheduler then switches over to Thread A again. Thread A resumes its stream of work by simply saving the value it was holding after incrementing, which is 2. After this scenario, even though the operation has happened twice as in Scenario A, the value saved is 2, and the API has been called twice with 1.</p>"},{"location":"python/concurrency/#simulate-race-condition","title":"Simulate race condition","text":"<p>Code: <pre><code>import threading\nfrom time import sleep\nfrom random import random\n\ncounter = 0\nrandsleep = lambda: sleep(0.1 * random())\n\ndef incr(n):\n    global counter\n    for count in range(n):\n        current = counter\n        randsleep()\n        counter = current + 1\n        randsleep()\n\nn = 5\nt1 = threading.Thread(target=incr, args=(n, ))\nt2 = threading.Thread(target=incr, args=(n, ))\nt1.start()\nt2.start()\nt1.join()\nt2.join()\nprint(f'Counter: {counter}')\n</code></pre></p> <p>Here <code>counter</code> is the shared resource between threads <code>t1</code> and <code>t2</code>. <code>randsleep()</code> function is invoked after reading and writing to shared resource to mimic the real-life cost attached to dealing with resources. The value of <code>counter</code> variable should be 10, but it rarely becomes 10  due to race condition.</p> <p>Output: <pre><code>Counter: 6\n</code></pre></p>"},{"location":"python/concurrency/#fix-for-race-condition","title":"Fix for race condition","text":"<p>Code: <pre><code>import threading\nfrom time import sleep\nfrom random import random\n\nincr_lock = threading.Lock()\ncounter = 0\nrandsleep = lambda: sleep(0.1 * random())\n\ndef incr(n):\n    global counter\n    for count in range(n):\n        with incr_lock:\n            current = counter\n            randsleep()\n            counter = current + 1\n            randsleep()\n\nn = 5\nt1 = threading.Thread(target=incr, args=(n, ))\nt2 = threading.Thread(target=incr, args=(n, ))\nt1.start()\nt2.start()\nt1.join()\nt2.join()\nprint(f'Counter: {counter}')\n</code></pre></p> <p>This time we have created a lock, from the <code>threading.Lock</code> class. We could call its <code>acquire</code> and <code>release</code> methods manually, or we can be Pythonic and use it within a context manager, which looks much nicer, and does the whole acquire/release business for us.</p> <p>Output: <pre><code>Counter: 10\n</code></pre></p>"},{"location":"python/concurrency/#a-threads-local-data","title":"A thread\u2019s local data","text":"<p>Example: <pre><code>import threading\nfrom random import randint\n\nlocal = threading.local()\n\ndef run(local, barrier):\n    local.my_value = randint(0, 10**2)\n    t = threading.current_thread()\n    print(f'Thread {t.name} has value {local.my_value}')\n    barrier.wait()\n    print(f'Thread {t.name} still has value {local.my_value}')\n\ncount = 3\nbarrier = threading.Barrier(count)\nthreads = [\n    threading.Thread(\n        target=run, name=f'T{name}', args=(local, barrier)\n    ) for name in range(count)\n]\nfor t in threads:\n    t.start()\n</code></pre></p> <p>We start by defining <code>local</code>. That is the special object that holds  thread-specific data. We run three threads. Each of them will assign a random value to <code>local.my_value</code>, and print it. Then the thread reaches a <code>Barrier</code> object, which is programmed to hold three threads in total. When the barrier is hit by the third thread, they all can pass. It\u2019s basically a nice way to make sure that N amount of threads reach a certain point and they all wait until every single one of them has arrived.</p> <p>Now, if <code>local</code> was a normal, dummy object, the second thread would override the value of <code>local.my_value</code>, and the third would do the same. This means that we would see them printing different values in the first set of prints,  but they would show the same value (the last one) in the second round of prints. But that doesn\u2019t happen, thanks to <code>local</code>. The output shows the following:</p> <p>Output: <pre><code>Thread T0 has value 61\nThread T1 has value 52\nThread T2 has value 38\nThread T2 still has value 38\nThread T0 still has value 61\nThread T1 still has value 52\n</code></pre></p>"},{"location":"python/concurrency/#thread-communication-using-queues","title":"Thread communication using queues","text":"<p>Code: <pre><code>import threading\nfrom queue import Queue\n\nSENTINEL = object()\n\ndef producer(q, n):\n    a, b = 0, 1\n    while a &lt;= n:\n        q.put(a)\n        a, b = b, a + b\n    q.put(SENTINEL)\n\ndef consumer(q):\n    while True:\n        num = q.get()\n        q.task_done()\n        if num is SENTINEL:\n            break\n        print(f'Got number {num}')\n\nq = Queue()\ncns = threading.Thread(target=consumer, args=(q, ))\nprd = threading.Thread(target=producer, args=(q, 35))\ncns.start()\nprd.start()\nq.join()\n</code></pre></p> <p>The producer generates fibonacci numbers till <code>n</code> and puts them in the <code>queue</code> and adds a <code>SENTINEL</code> object at the last. A <code>SENTINEL</code> is any object that is used to signal something, and in our case, it signals to the consumer that the producer is done. The consumer receives objects from queue and marks the object as processed using <code>q.task_done()</code> and when object is <code>SENTINEL</code> it stops. The purpose of using <code>q.task_done()</code> is to allow the final instruction in the code to unblock when all elements have been acknowledged, so that the  execution can end.</p> <p>Output: <pre><code>Got numer 0\nGot numer 1\n...\nGot numer 34\n</code></pre></p>"},{"location":"python/concurrency/#thread-communication-using-sending-events","title":"Thread communication using sending events","text":"<p>Another way to make threads communicate is to fire events.</p> <p>Example: <pre><code>import threading\n\ndef fire():\n    print('Firing event...')\n    event.set()\n\ndef listen():\n    event.wait()\n    print('Event has been fired')\n\nevent = threading.Event()\nt1 = threading.Thread(target=fire)\nt2 = threading.Thread(target=listen)\nt2.start()\nt1.start()\n</code></pre></p> <p>Here we have two threads that run <code>fire</code> and <code>listen</code>, respectively firing and listening for an event. To fire an event, call the <code>set</code> method on it. The <code>t2</code> thread, which is started first, is already listening to the event, and will sit there until the event is fired. Events are great in some situations. Think about having threads that are waiting on a connection object to be ready, before they can actually start using it. They could be waiting on an event, and one thread could be checking that connection, and firing the event when it\u2019s ready.</p> <p>Output: <pre><code>Firing event...\nEvent has been fired\n</code></pre></p>"},{"location":"python/concurrency/#inter-process-communication-with-queues","title":"Inter-process communication with queues","text":"<p>Example: <pre><code>import multiprocessing\n\nSENTINEL = 'STOP'\n\ndef producer(q, n):\n    a, b = 0, 1\n    while a &lt;= n:\n        q.put(a)\n        a, b = b, a + b\n    q.put(SENTINEL)\n\ndef consumer(q):\n    while True:\n        num = q.get()\n        if num == SENTINEL:\n            break\n        print(f'Got number {num}')\n\nq = multiprocessing.Queue()\ncns = multiprocessing.Process(target=consumer, args=(q, ))\nprd = multiprocessing.Process(target=producer, args=(q, 35))\ncns.start()\nprd.start()\n</code></pre></p> <p>The code is very similar to the thread communication using queues. In this case, we have to use a queue that is an instance of <code>multiprocessing.Queue</code>, which doesn\u2019t expose a <code>task_done</code> method. However,  because of the way this queue is designed, it automatically joins the main thread, therefore we only need to start the two processes and all will work.</p> <p>When it comes to inter-process communication objects are pickled when they enter the queue, so IDs get lost. That\u2019s why an <code>object</code> is not used as a <code>SENTINEL</code> and a string <code>\"STOP\"</code> is used to do the trick as we\u2019re dealing with numbers. The key is to use something we\u2019re not expecting at all.</p> <p>Queues aren\u2019t the only way to communicate between processes. You can also use pipes (<code>multiprocessing.Pipe</code>), which provide a connection (as in, a pipe , clearly) from one process to another, and vice versa.</p>"},{"location":"python/concurrency/#thread-and-process-pools","title":"Thread and process pools","text":"<p>Pools are structures designed to hold N objects (threads, processes,  and so on). When the usage reaches capacity, no work is assigned to a thread  (or process) until one of those currently working becomes available again. Pools, therefore, are a great way to limit the number of threads (or processes) that can be alive at the same time, preventing the system from starving due to resource exhaustion, or the computation time from being affected by too much context switching.</p> <p>The <code>ThreadPoolExecutor</code> and <code>ProcessPoolExecutor</code> are two classes from  <code>concurrent.futures</code>, use a pool of threads (and processes, respectively),  to execute calls asynchronously. They both accept a parameter, <code>max_workers</code>,  which sets the upper limit to how many threads (or processes) can be used at the same time by the executor.</p>"},{"location":"python/concurrency/#thread-pools","title":"Thread pools","text":"<p>Example: <pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom random import randint\nimport threading\nfrom time import sleep\n\ndef run(name):\n    sleep(.05)\n    value = randint(0, 10**2)\n    tname = threading.current_thread().name\n    print(f'Hi, I am {name} ({tname}) and my value is {value}')\n    return (name, value)\n\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    futures = [\n        executor.submit(run, f'T{name}') for name in range(5)\n    ]\n    for future in as_completed(futures):\n        name, value = future.result()\n        print(f'Thread {name} returned {value}')\n</code></pre></p> <p>We define a list of future objects by making a list comprehension, in which we call <code>submit</code> on our executor object. We instruct the executor to run the <code>run</code> function, with a name that will go from <code>T0</code> to <code>T4</code>. A <code>future</code> is an  object that encapsulates the asynchronous execution of a callable. Then we loop over the <code>future</code> objects, as they are done. To do this, we use <code>as_completed</code> to get an iterator of the <code>future</code> instances that returns them as soon as they complete (finish or were cancelled). We grab the result of each <code>future</code> by calling the <code>result</code> method, and simply print it.  We sleep for 50 milliseconds at the beginning of each <code>run</code>. This is to exacerbate the behavior and have the output clearly show the size of the pool, which is still three.</p> <p>Output: <pre><code>Hi, I am T0 (ThreadPoolExecutor-0_0) and my value is 81\nHi, I am T1 (ThreadPoolExecutor-0_1) and my value is 44\nThread T0 returned 81\nThread T1 returned 44\nHi, I am T2 (ThreadPoolExecutor-0_2) and my value is 26\nThread T2 returned 26\nHi, I am T3 (ThreadPoolExecutor-0_0) and my value is 15\nHi, I am T4 (ThreadPoolExecutor-0_1) and my value is 87\nThread T3 returned 15\nThread T4 returned 87\n</code></pre></p> <p>So, what goes on is that three threads start running, so we get three  <code>Hi, I am...</code> messages printed out. Once all three of them are running, the  pool is at capacity, so we need to wait for at least one thread to complete  before anything else can happen. In the example run, <code>T0</code> and <code>T1</code> complete  (which is signaled by the printing of what they returned), so they return to  the pool and can be used again. They get run with names <code>T3</code> and <code>T4</code>, and  finally all three, <code>T2</code>, <code>T3</code>, and <code>T4</code> complete. It can be seen from the output how the threads are actually reused, and how the first two are  reassigned to <code>T3</code> and <code>T4</code> after they complete.</p>"},{"location":"python/concurrency/#process-pools","title":"Process pools","text":"<p>Example: <pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom random import randint\nfrom time import sleep\n\ndef run(name):\n    sleep(.05)\n    value = randint(0, 10**2)\n    print(f'Hi, I am {name} and my value is {value}')\n    return (name, value)\n\nwith ProcessPoolExecutor(max_workers=3) as executor:\n    futures = [\n        executor.submit(run, f'P{name}') for name in range(5)\n    ]\n    for future in as_completed(futures):\n        name, value = future.result()\n        print(f'Process {name} returned {value}')\n</code></pre></p> <p>The difference is truly minimal. We use <code>ProcessPoolExecutor</code> this time, and the <code>run</code> function is exactly the same.</p> <p>Output: <pre><code>Hi, I am P0 and my value is 19\nHi, I am P1 and my value is 97\nHi, I am P2 and my value is 74\nProcess P0 returned 19\nProcess P1 returned 97\nProcess P2 returned 74\nHi, I am P3 and my value is 80\nHi, I am P4 and my value is 68\nProcess P3 returned 80\nProcess P4 returned 68\n</code></pre></p>"},{"location":"python/concurrency/#examples","title":"Examples","text":""},{"location":"python/concurrency/#add-timeout-to-a-function","title":"Add timeout to a function","text":"<p>Most, if not all, libraries that expose functions to make HTTP requests,  provide the ability to specify a timeout when performing the request. This means that if after X seconds (X being the timeout), the request hasn\u2019t completed, the whole operation is aborted and execution resumes from the next instruction. Not all functions expose this feature though, so, when a function doesn\u2019t provide the ability to being interrupted, we can use a process to simulate that behavior. In this example, we\u2019ll be trying to translate a hostname into an IPv4 address. The <code>gethostbyname</code> function, from the <code>socket</code> module, doesn\u2019t allow us to put a timeout on the operation though, so we use a process to do that artificially.</p> <p>Code:</p> <pre><code>import socket\nfrom multiprocessing import Process, Queue\n\ndef gethostbyname(hostname, queue):\n    ip = socket.gethostbyname(hostname)\n    queue.put(ip)\n\ndef resolve_host(hostname, timeout):\n    queue = Queue()\n    proc = Process(target=gethostbyname, args=(hostname, queue))\n    proc.start()\n    proc.join(timeout=timeout)\n\n    if queue.empty():\n        proc.terminate()\n        ip = None\n    else:\n        ip = queue.get()\n    return proc.exitcode, ip\n\ndef resolve(hostname, timeout=5):\n    exitcode, ip = resolve_host(hostname, timeout)\n    if exitcode == 0:\n        return ip\n    else:\n        return hostname\n</code></pre> <p>In the successful scenario, the call to <code>socket.gethostbyname</code> succeeds quickly, the IP is in the queue, the process terminates well before its timeout time, and when we get to the <code>if</code> part, the queue will not be empty.  We fetch the IP from it, and return it, alongside the process exit code.</p> <p>In the unsuccessful scenario, the call to <code>socket.gethostbyname</code> takes too long, and the process is killed after its timeout has expired. Because the call failed, no IP has been inserted in the queue, and therefore it will be empty. In the <code>if</code> logic, we therefore set the IP to <code>None</code>, and return as before. The <code>resolve</code> function will find that the exit code is not <code>0</code> (as the process didn\u2019t terminate happily, but was killed instead), and will correctly return the hostname instead of the IP, which we couldn\u2019t get anyway.</p>"},{"location":"python/concurrency/#multithreadedmultiprocessing-mergesort","title":"Multithreaded/Multiprocessing mergesort","text":"<pre><code>from random import shuffle\nfrom functools import reduce\nfrom concurrent.futures import (\n    ProcessPoolExecutor,\n    ThreadPoolExecutor,\n    as_completed\n)\nfrom time import process_time\n\n\ndef sort(lt: list, parts: int = 6):\n    assert parts &gt; 1\n    if len(lt) &lt;= 1:\n        return lt\n    chunk_size = max(1, len(lt) // parts)\n    sorted_chunks = [sort(lt[k:k + chunk_size]) for k in\n                     range(0, len(lt), chunk_size)]\n    return reduce(merge, sorted_chunks)\n\n\ndef sort_multithreading(lt, workers=6):\n    if len(lt) &lt;= 1:\n        return lt\n    chunk_size = max(1, len(lt) // workers)\n    chunks = [lt[k:k + chunk_size] for k in range(0, len(lt), chunk_size)]\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(sort, chunk) for chunk in chunks\n        ]\n    return reduce(merge, (future.result() for future in as_completed(futures)))\n\n\ndef sort_multiprocessing(lt, workers=6):\n    if len(lt) &lt;= 1:\n        return lt\n    chunk_size = max(1, len(lt) // workers)\n    chunks = [lt[k:k + chunk_size] for k in range(0, len(lt), chunk_size)]\n    with ProcessPoolExecutor(max_workers=workers) as executor:\n        futures = [\n            executor.submit(sort, chunk) for chunk in chunks\n        ]\n    return reduce(merge, (future.result() for future in as_completed(futures)))\n\n\ndef merge(lt1, lt2):\n    lt = []\n    start_1 = start_2 = 0\n    while start_1 &lt; len(lt1) and start_2 &lt; len(lt2):\n        if lt1[start_1] &lt;= lt2[start_2]:\n            lt.append(lt1[start_1])\n            start_1 += 1\n        else:\n            lt.append(lt2[start_2])\n            start_2 += 1\n    if start_1 &lt; len(lt1):\n        lt.extend(lt1[start_1:])\n    elif start_2 &lt; len(lt2):\n        lt.extend(lt2[start_2:])\n    return lt\n\n\ndef calculate_time(func):\n    def wrapper(*args, **kwargs):\n        start_time = process_time()\n        result = func(*args, **kwargs)\n        elapsed_time = (process_time() - start_time) * 1000\n        print(f\"{func.__name__} elapsed in {elapsed_time} milliseconds\")\n        return result\n\n    return wrapper\n\n\ndef performance_measurement_and_testing():\n    for lt_length in (4, 1000, 1000000,):\n        lt_orig = list(range(lt_length))\n        lt = list(lt_orig)\n        print(f\"List length: {lt_length}\")\n        shuffle(lt)\n        for func in (sort, sort_multithreading, sort_multiprocessing):\n            sorted_lt = calculate_time(func)(lt)\n            assert sorted_lt == lt_orig\n\nperformance_measurement_and_testing()\n</code></pre> <p>Output: <pre><code>List length: 4\nsort elapsed in 0.029448999999986958 milliseconds\nsort_multithreading elapsed in 2.796734000000023 milliseconds\nsort_multiprocessing elapsed in 8.03862999999999 milliseconds\nList length: 1000\nsort elapsed in 3.742769000000007 milliseconds\nsort_multithreading elapsed in 6.403607000000005 milliseconds\nsort_multiprocessing elapsed in 9.161322 milliseconds\nList length: 1000000\nsort elapsed in 8895.442821999999 milliseconds\nsort_multithreading elapsed in 8808.045813 milliseconds\nsort_multiprocessing elapsed in 1428.9983950000008 milliseconds\n</code></pre></p> <p>This is a good example to show you a consequence of choosing multiprocessing approach over multithreading. Because the code is CPU-intensive, and there is no IO going on, splitting the list and having threads working the chunks doesn\u2019t add any advantage. On the other hand, using processes does. I used six workers in the multiprocessing version, Remember to parallelize proportionately to the amount of cores your processor has.</p>"},{"location":"python/concurrency/#links","title":"Links","text":"<ul> <li>Python library</li> <li>Raymond Hettinger, Keynote on Concurrency, PyBay 2017</li> </ul>"},{"location":"python/custom-json/","title":"Custom JSON Encoder/Decoder","text":""},{"location":"python/custom-json/#example","title":"Example","text":"<p>Code:</p> <pre><code>import json\nimport pytz\nfrom datetime import datetime\nfrom pprint import pprint\n\ndatetimeobj_key = \"_datetime\"\n\nobj = {\n    \"date\": datetime.now(),\n    \"datetz\": datetime.now(pytz.utc),\n    \"string\": \"hello world!\",\n    \"dict\": {\n        \"_meta\": datetimeobj_key\n    },\n    \"list\": list(range(5)),\n}\n# using classes\nprint(\"using classes\".center(50, '-'))\n\n\nclass DateTimeJSONEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, datetime):\n            return {\"_meta\": datetimeobj_key, \"value\": obj.isoformat()}\n        return super().default(obj)\n\n\ndump_obj = json.dumps(obj, cls=DateTimeJSONEncoder, indent=2)\nprint(dump_obj)\n\n\nclass DateTimeJSONDecoder(json.JSONDecoder):\n    def __init__(self, *args, **kwargs):\n        super().__init__(object_hook=self.object_hook, *args, **kwargs)\n\n    def object_hook(self, obj):\n        try:\n            if obj.get(\"_meta\") == datetimeobj_key:\n                value = obj[\"value\"]\n                return datetime.fromisoformat(value)\n        except KeyError:\n            return obj\n        return obj\n\n\nload_obj = json.loads(dump_obj, cls=DateTimeJSONDecoder)\npprint(load_obj)\n\ndel dump_obj\ndel load_obj\n\n# using functions\nprint(\"using functions\".center(50, '-'))\n\n\ndef datetime_json_encoder(obj):\n    if isinstance(obj, datetime):\n        return {\"_meta\": datetimeobj_key, \"value\": obj.isoformat()}\n    return obj\n\n\ndump_obj = json.dumps(obj, default=datetime_json_encoder, indent=2)\nprint(dump_obj)\n\n\ndef datetime_json_decoder(obj):\n    try:\n        if obj.get(\"_meta\") == datetimeobj_key:\n            value = obj[\"value\"]\n            return datetime.fromisoformat(value)\n    except (KeyError, ):\n        return obj\n    return obj\n\n\nload_obj = json.loads(dump_obj, object_hook=datetime_json_decoder)\npprint(load_obj)\n</code></pre> <p>Output: <pre><code>------------------using classes-------------------\n{\n  \"date\": {\n    \"_meta\": \"_datetime\",\n    \"value\": \"2020-03-28T07:00:09.446641\"\n  },\n  \"datetz\": {\n    \"_meta\": \"_datetime\",\n    \"value\": \"2020-03-28T07:00:09.446651+00:00\"\n  },\n  \"string\": \"hello world!\",\n  \"dict\": {\n    \"_meta\": \"_datetime\"\n  },\n  \"list\": [\n    0,\n    1,\n    2,\n    3,\n    4\n  ]\n}\n{'date': datetime.datetime(2020, 3, 28, 7, 0, 9, 446641),\n 'datetz': datetime.datetime(2020, 3, 28, 7, 0, 9, 446651, tzinfo=datetime.timezone.utc),\n 'dict': {'_meta': '_datetime'},\n 'list': [0, 1, 2, 3, 4],\n 'string': 'hello world!'}\n-----------------using functions------------------\n{\n  \"date\": {\n    \"_meta\": \"_datetime\",\n    \"value\": \"2020-03-28T07:00:09.446641\"\n  },\n  \"datetz\": {\n    \"_meta\": \"_datetime\",\n    \"value\": \"2020-03-28T07:00:09.446651+00:00\"\n  },\n  \"string\": \"hello world!\",\n  \"dict\": {\n    \"_meta\": \"_datetime\"\n  },\n  \"list\": [\n    0,\n    1,\n    2,\n    3,\n    4\n  ]\n}\n{'date': datetime.datetime(2020, 3, 28, 7, 0, 9, 446641),\n 'datetz': datetime.datetime(2020, 3, 28, 7, 0, 9, 446651, tzinfo=datetime.timezone.utc),\n 'dict': {'_meta': '_datetime'},\n 'list': [0, 1, 2, 3, 4],\n 'string': 'hello world!'}\n</code></pre></p>"},{"location":"python/custom-json/#links","title":"Links","text":"<ul> <li>Python docs</li> </ul>"},{"location":"python/decorators/","title":"Decorators","text":""},{"location":"python/decorators/#introduction","title":"Introduction","text":"<p>A decorator is a function that takes another function and extends the  behavior of the latter function without explicitly modifying it.</p>"},{"location":"python/decorators/#decorators-on-function","title":"Decorators on function","text":"<p>The <code>@functools.wraps</code> decorator uses the function  <code>functools.update_wrapper()</code> to update special attributes like <code>__name__</code> and  <code>__doc__</code> that are used in the introspection.</p> <p>Decorator template: <pre><code>import functools\n\ndef decorator(func):\n  @functools.wraps(func)\n  def wrapper_decorator(*args, **kwargs):\n      # Do something before\n      value = func(*args, **kwargs)\n      # Do something after\n      return value\n  return wrapper_decorator\n</code></pre></p> <p>Example: Calculate execution time decorator <pre><code>import functools\nimport time\n\ndef timer(func):\n\"\"\"Print the runtime of the decorated function\"\"\"\n  @functools.wraps(func)\n  def wrapper(*args, **kwargs):\n    start_time = time.perf_counter()\n    result = func(*args,**kwargs)\n    exec_time = time.perf_counter() - start_time\n    print(f\"Finished {func.__name__!r} in {exec_time:.4f} seconds\")\n    return result\n  return wrapper\n</code></pre></p>"},{"location":"python/decorators/#decorators-on-class","title":"Decorators on class","text":"<p>Decoratos can be applied to a function as well a class.</p> <p>Example: <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass PlayingCard:\n    rank: str\n    suit: str\n</code></pre></p>"},{"location":"python/decorators/#decorator-with-arguements","title":"Decorator with arguements","text":"<p>Example: <pre><code>import functools\n\ndef repeat(func=None, *, num_of_times=2):\n  if func is None: # decorator called with arguements\n    return functools.partial(repeat, num_of_times=num_of_times)\n\n  @functools.wraps(func)\n  def wrapper(*args, **kwargs):\n    for _ in range(num_of_times):\n      value = func(*args, **kwargs)\n    return value\n  return wrapper\n\n@repeat\ndef greet(name):\n  print(f\"Hello {name}\")\n\n@repeat(num_of_times=3)\ndef greet2(name):\n  print(f\"Greetings {name}!\")\n</code></pre></p>"},{"location":"python/decorators/#stateful-decorators","title":"Stateful decorators","text":"<p>Stateful decorators: a decorator that can keep track of state</p> <p>Example: <pre><code>import functools\n\ndef count_calls(func):\n  @functools.wraps(func)\n  def wrapper(*args, **kwargs):\n    wrapper.count += 1\n    print(f\"Call {wrapper.count} of {func.__name__!r}\")\n    return func(*args, **kwargs)\n  wrapper.count = 0\n  return wrapper\n\n@count_calls\ndef say_whee():\n  print(\"Whee!!\")\n\nsay_whee() # prints: Call 1 of 'say_whee' \\nWhee!!\nsay_whee() # prints: Call 2 of 'say_whee' \\nWhee!!\n</code></pre></p>"},{"location":"python/decorators/#class-decorator","title":"Class decorator","text":"<p>Example: <pre><code>import functools\nclass Counter:\ndef __init__(self, func):\n  functools.update_wrapper(self, func)\n  self.func = func\n  self.count = 0\n\ndef __call__(self, *args, **kwargs):\n  self.count += 1\n  print(f\"Call {self.count} of {self.func.__name__!r}\")\n  return self.func(*args, **kwargs)\n\n@Counter\ndef say_whee():\nprint(\"Whee!!\")\n\nsay_whee() # prints: Call 1 of 'say_whee' \\nWhee!!\nsay_whee() # prints: Call 2 of 'say_whee' \\nWhee!!\n</code></pre></p>"},{"location":"python/decorators/#helpful-buil-in-decoratos","title":"Helpful buil-in decoratos","text":""},{"location":"python/decorators/#caching","title":"Caching","text":"<p>You should use <code>@functools.lru_cache</code> instead of writing your own cache decorator. The <code>maxsize</code> parameter specifies how many recent calls are cached. The default value is 128, but you can specify <code>maxsize=None</code> to cache all function calls. However, be aware that this can cause memory problems if you are caching many large objects. You can use the <code>.cache_info()</code> method to see how the cache performs, and you can tune it if needed. In our example, we used an artificially small <code>maxsize</code> to see the effect of elements being removed from the cache:</p> <p>Example: <pre><code>import functools\n\n@functools.lru_cache(maxsize=4)\ndef fibonacci(num):\n    print(f\"Calculating fibonacci({num})\")\n    if num &lt; 2:\n        return num\n    return fibonacci(num - 1) + fibonacci(num - 2)\n\nprint(fibonacci(10))\nprint(fibonacci(8))\nprint(fibonacci(5))\nprint(fibonacci(8))\nprint(fibonacci(5))\nprint(fibonacci.cache_info())\n</code></pre> Output: <pre><code>Calculating fibonacci(10)\nCalculating fibonacci(9)\nCalculating fibonacci(8)\nCalculating fibonacci(7)\nCalculating fibonacci(6)\nCalculating fibonacci(5)\nCalculating fibonacci(4)\nCalculating fibonacci(3)\nCalculating fibonacci(2)\nCalculating fibonacci(1)\nCalculating fibonacci(0)\n55\n\n21\n\nCalculating fibonacci(5)\nCalculating fibonacci(4)\nCalculating fibonacci(3)\nCalculating fibonacci(2)\nCalculating fibonacci(1)\nCalculating fibonacci(0)\n5\n\n\nCalculating fibonacci(8)\nCalculating fibonacci(7)\nCalculating fibonacci(6)\n21\n\n\n5\n\nCacheInfo(hits=17, misses=20, maxsize=4, currsize=4)\n</code></pre></p>"},{"location":"python/decorators/#links","title":"Links","text":"<ul> <li>RealPython - Primer on Python decorators</li> </ul>"},{"location":"python/exceptions/","title":"Exceptions","text":""},{"location":"python/exceptions/#introduction","title":"Introduction","text":"<p>When an error is detected during execution, it is called an exception.  Exceptions are not necessarily lethal; in fact, <code>StopIteration</code> is deeply integrated in the Python generator and iterator mechanisms.</p> <pre><code>&gt;&gt;&gt; gen = (n for n in range(2))\n&gt;&gt;&gt; next(gen)\n0\n&gt;&gt;&gt; next(gen)\n1\n&gt;&gt;&gt; next(gen)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nStopIteration\n&gt;&gt;&gt; print(undefined_name)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nNameError: name 'undefined_name' is not defined\n&gt;&gt;&gt; mylist = [1, 2, 3]\n&gt;&gt;&gt; mylist[5]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nIndexError: list index out of range\n&gt;&gt;&gt; mydict = {'a': 'A', 'b': 'B'}\n&gt;&gt;&gt; mydict['c']\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nKeyError: 'c'\n&gt;&gt;&gt; 1 / 0\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nZeroDivisionError: division by zero\n</code></pre> <p>When an exception has occurred a regular program or script would normally die.</p> <p>To handle an exception, Python gives you the <code>try</code> statement. When you enter the <code>try</code> clause, Python will watch out for one or more different types of exceptions (according to how you instruct it), and if they are raised, it will allow you to react. The <code>try</code> statement is composed of the <code>try</code> clause , which opens the statement, one or more <code>except</code> clauses (all optional) that define what to do when an exception is caught, an <code>else</code> clause (optional ), which is executed when the <code>try</code> clause is exited without any exception raised, and a <code>finally</code> clause (optional), whose code is executed regardless of whatever happened in the other clauses. The <code>finally</code> clause is typically used to clean up resources.</p>"},{"location":"python/exceptions/#try-except-else-finally","title":"try-except-else-finally","text":"<p>Mind the order\u2014it\u2019s important. Also, <code>try</code> must be followed by at least one <code>except</code> clause or a <code>finally</code> clause. </p> <p>Example: <pre><code>def try_syntax(numerator, denominator):\n    try:\n        print(f'In the try block: {numerator}/{denominator}')\n        result = numerator / denominator\n    except ZeroDivisionError as zde:\n        print(zde)\n    else:\n        print('The result is:', result)\n        return result\n    finally:\n        print('Exiting')\n\nprint(try_syntax(12, 4))\nprint(try_syntax(11, 0))\n</code></pre></p> <p>Output: <pre><code>$ python try.syntax.py\nIn the try block: 12/4     # try\nThe result is: 3.0         # else\nExiting                    # finally\n3.0                        # return within else\n\nIn the try block: 11/0     # try\ndivision by zero           # except\nExiting                    # finally\nNone                       # implicit return end of function\n</code></pre></p>"},{"location":"python/exceptions/#handling-multiple-exceptions-the-same-way","title":"Handling multiple exceptions the same way","text":"<pre><code>import json\njson_data = '{}'\n\ntry:\n    data = json.loads(json_data)\nexcept (ValueError, TypeError) as e:\n    print(type(e), e)\n</code></pre>"},{"location":"python/exceptions/#handling-multiple-exceptions-differently","title":"Handling multiple exceptions differently","text":"<p><pre><code># exceptions/multiple.except.py\ntry:\n    # some code\nexcept Exception1:\n    # react to Exception1\nexcept (Exception2, Exception3):\n    # react to Exception2 or Exception3\nexcept Exception4:\n    # react to Exception4\n...\n</code></pre> Keep in mind that an exception is handled in the first block that defines that exception class or any of its bases. Therefore, when you stack multiple <code>except</code> clauses like we\u2019ve just done, make sure that you put specific exceptions at the top and generic ones at the bottom. In OOP terms , children on top, grandparents at the bottom. Moreover, remember that only one <code>except</code> handler is executed when an exception is raised.</p>"},{"location":"python/exceptions/#built-in-exceptions-hierarchy","title":"Built-in exceptions hierarchy","text":"<pre><code>BaseException\n +-- SystemExit\n +-- KeyboardInterrupt\n +-- GeneratorExit\n +-- Exception\n      +-- StopIteration\n      +-- StopAsyncIteration\n      +-- ArithmeticError\n      |    +-- FloatingPointError\n      |    +-- OverflowError\n      |    +-- ZeroDivisionError\n      +-- AssertionError\n      +-- AttributeError\n      +-- BufferError\n      +-- EOFError\n      +-- ImportError\n      |    +-- ModuleNotFoundError\n      +-- LookupError\n      |    +-- IndexError\n      |    +-- KeyError\n      +-- MemoryError\n      +-- NameError\n      |    +-- UnboundLocalError\n      +-- OSError\n      |    +-- BlockingIOError\n      |    +-- ChildProcessError\n      |    +-- ConnectionError\n      |    |    +-- BrokenPipeError\n      |    |    +-- ConnectionAbortedError\n      |    |    +-- ConnectionRefusedError\n      |    |    +-- ConnectionResetError\n      |    +-- FileExistsError\n      |    +-- FileNotFoundError\n      |    +-- InterruptedError\n      |    +-- IsADirectoryError\n      |    +-- NotADirectoryError\n      |    +-- PermissionError\n      |    +-- ProcessLookupError\n      |    +-- TimeoutError\n      +-- ReferenceError\n      +-- RuntimeError\n      |    +-- NotImplementedError\n      |    +-- RecursionError\n      +-- SyntaxError\n      |    +-- IndentationError\n      |         +-- TabError\n      +-- SystemError\n      +-- TypeError\n      +-- ValueError\n      |    +-- UnicodeError\n      |         +-- UnicodeDecodeError\n      |         +-- UnicodeEncodeError\n      |         +-- UnicodeTranslateError\n      +-- Warning\n           +-- DeprecationWarning\n           +-- PendingDeprecationWarning\n           +-- RuntimeWarning\n           +-- SyntaxWarning\n           +-- UserWarning\n           +-- FutureWarning\n           +-- ImportWarning\n           +-- UnicodeWarning\n           +-- BytesWarning\n           +-- ResourceWarning\n</code></pre>"},{"location":"python/exceptions/#raising-an-exception","title":"Raising an Exception","text":"<p>We can use <code>raise</code> to throw an exception if a condition occurs. The statement can be complemented with a custom exception. </p> <p>Example: <pre><code>x = 10\nif x &gt; 5:\n    raise Exception('x should not exceed 5. The value of x was: {}'.format(x))\n</code></pre></p> <p>Output: <pre><code>Traceback (most recent call last):\n  File \"&lt;input&gt;\", line 4, in &lt;module&gt;\nException: x should not exceed 5. The value of x was: 10\n</code></pre></p>"},{"location":"python/exceptions/#exception-chaining","title":"Exception Chaining","text":"<p>Syntax: <pre><code>raise ExceptionClass from Cause\n</code></pre></p> <p>Example: <pre><code>class ValidatorError(Exception):\n\"\"\"Raised when accessing a dict results in KeyError. \"\"\"\n\n\nd = {'some': 'key'}\nmandatory_key = 'some-other'\ntry:\n    print(d[mandatory_key])\nexcept KeyError as err:\n    raise ValidatorError(\n        f'`{mandatory_key}` not found in d.'\n    ) from err\n</code></pre></p> <p>Without being able to chain exceptions, we would lose information about <code>KeyError</code>.</p> <p>Output: <pre><code>Traceback (most recent call last):\n  File \"traceback_validator.py\", line 7, in &lt;module&gt;\n    print(d[mandatory_key])\nKeyError: 'some-other'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"traceback_validator.py\", line 10, in &lt;module&gt;\n    '`{}` not found in d.'.format(mandatory_key)) from err\n__main__.ValidatorError: `some-other` not found in d.\n</code></pre></p> <p>This is helpful, because we can see the traceback of the exception that led us to raise <code>ValidationError</code>, as well as the traceback for the <code>ValidationError</code> itself.</p>"},{"location":"python/exceptions/#guidelines","title":"Guidelines","text":"<ul> <li>Only put the code that may cause the exception(s) that you want to handle in the <code>try</code> clause.</li> <li>When you write <code>except</code> clauses, be as specific as you can, don\u2019t just resort to except <code>Exception</code> because it\u2019s easy.</li> <li>Use tests to make sure your code handles edge cases in a way that requires the least possible amount of exception handling.</li> <li>Writing an <code>except</code> statement without specifying any exception would catch any exception, therefore exposing your code to the same risks you incur when you derive your custom exceptions from <code>BaseException</code>.</li> </ul>"},{"location":"python/files/","title":"Working with files","text":""},{"location":"python/files/#opening-and-reading-from-file","title":"Opening and Reading from file","text":"<pre><code>file_fullpath = 'temp.txt'\nfilemode = 'rt'  # r: read, t: text\nwith open(file_fullpath, filemode) as fh:\n    for line in fh.readlines():\n        print(line.strip())  # remove whitespace and print\n</code></pre> <p>\u201crt\u201d is the default mode</p>"},{"location":"python/files/#writing-to-files","title":"Writing to files","text":"<pre><code>with open('print_example.txt', 'w') as fw:\n    print('Hey I am printing into a file!!!', file=fw)\n</code></pre>"},{"location":"python/files/#file-modes","title":"File modes","text":"<ul> <li>r: read file</li> <li>w: file is overwritten with an empty file, and the original content is lost. </li> <li>x: open the file for writing only if it doesn\u2019t exists, if it exists raise <code>FileExistsError</code></li> </ul>"},{"location":"python/files/#check-existence","title":"Check existence","text":"<pre><code>import os\n\nfilename = 'temp.txt'\npath = os.path.dirname(os.path.abspath(filename))\n\nprint(os.path.isfile(filename))  # True\nprint(os.path.isdir(path))  # True\nprint(path)  # /Users/mickey/tutorials/python/files\n</code></pre> <p>Should you ever need to work with paths in a different way, you can check out <code>pathlib</code>. While <code>os.path</code> works with strings,  <code>pathlib</code> offers classes representing filesystem paths with semantics appropriate for different operating systems. </p>"},{"location":"python/files/#manipulating-pathname","title":"Manipulating pathname","text":"<p>Code: <pre><code># files/paths.py\nimport os\n\nfilename = 'fear.txt'\npath = os.path.abspath(filename)\n\nprint(path)\nprint(os.path.basename(path))\nprint(os.path.dirname(path))\nprint(os.path.splitext(path))\nprint(os.path.split(path))\n\nreadme_path = os.path.join(\n    os.path.dirname(path), '..', '..', 'README.rst')\nprint(readme_path)\nprint(os.path.normpath(readme_path))\n</code></pre> Output: <pre><code>/Users/mickey/tutorials/python/files/fear.txt           # path\nfear.txt                                                # basename\n/Users/mickey/tutorials/python/files                    # dirname\n('/Users/mickey/tutorials/python/files/fear', '.txt')   # splitext\n('/Users/mickey/tutorials/python/files', 'fear.txt')    # split\n/Users/mickey/tutorials/python/files/../../README.rst   # readme_path\n/Users/mickey/tutorials/README.rst                      # normalized\n</code></pre></p>"},{"location":"python/files/#temporary-files-and-directories","title":"Temporary files and directories","text":"<p>Sometimes, it\u2019s very useful to be able to create a temporary directory or file when running some code. For example, when writing tests that affect the disk, you can use temporary files and directories to run your logic and assert that it\u2019s correct, and to be sure that at the end of the test run , the test folder has no leftovers.</p> <p>Example: <pre><code>import os\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\n\nwith TemporaryDirectory(dir='.') as td:\n    print('Temp directory:', td)\n    with NamedTemporaryFile(dir=td) as t:\n        name = t.name\n        print(os.path.abspath(name))\n</code></pre> Output: <pre><code>Temp directory: ./tmpwa9bdwgo\n/Users/mickey/tutorials/python/files/tmpwa9bdwgo/tmp3d45hm46\n</code></pre></p>"},{"location":"python/functions/","title":"Functions","text":""},{"location":"python/functions/#useful-tips","title":"Useful Tips","text":"<p>Functions should do one thing: Functions that do one thing are easy to  describe in one short sentence. Functions that do multiple things can be split  into smaller functions that do one thing. These smaller functions are usually  easier to read and understand.</p> <p>Functions should be small: The smaller they are, the easier it is to test  them and to write them so that they do one thing.</p> <p>The fewer input parameters, the better: Functions that take a lot of  arguments quickly become harder to manage</p> <p>Functions should be consistent in their return values: Returning <code>False</code> or  <code>None</code> is not the same thing, even if within a Boolean context they both  evaluate to <code>False</code>. <code>False</code> means that we have information (<code>False</code>), while  <code>None</code> means that there is no information. Try writing functions that return  in a consistent way , no matter what happens in their body.</p> <p>Functions shouldn\u2019t have side effects: In other words, functions should  not affect the values you call them with. The <code>list.sort()</code> method is acting on the numbers object itself, and that is fine because it is a method (a  function that belongs to an object and therefore has the rights to modify it)</p> <p>When writing recursive functions, always consider how many nested calls you  make, since there is a limit. For further information on this, check out   <code>sys.getrecursionlimit()</code> and <code>sys.setrecursionlimit(limit)</code>.</p>"},{"location":"python/functions/#lambda","title":"Lambda","text":"<p><pre><code>func_name = lambda [parameter_list]: expression\n</code></pre> is equivalent to  <pre><code>def func_name([parameter_list]): return expression\n</code></pre></p> <p>Should you want to see all the attributes of an object, just call  <code>dir(object_name)</code> and you\u2019ll be given the list of all of its attributes.</p>"},{"location":"python/functions/#map","title":"Map","text":"<p><code>map(function, iterable, ...)</code> returns an iterator that applies function to  every item of iterable, yielding the results. If additional iterable arguments  are passed, function must take that many arguments and is applied to the items  from all iterables in parallel. With multiple iterables, the iterator stops  when the shortest iterable is exhausted.</p>"},{"location":"python/functions/#zip","title":"Zip","text":"<p><code>zip(*iterables)</code> returns an iterator of tuples, where the i-th tuple contains  the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted. With a single iterable  argument, it returns an iterator of 1-tuples. With no arguments, it returns an empty iterator.</p>"},{"location":"python/functions/#filter","title":"Filter","text":"<p><code>filter(function, iterable)</code> construct an iterator from those elements of  iterable for which function returns True. <code>iterable</code> may be either a sequence, a  container which supports iteration, or an iterator. If function is <code>None</code>, the  identity function is assumed, that is, all elements of iterable that are false  are removed.</p>"},{"location":"python/functions/#list-comprehension","title":"List comprehension","text":"<p>A list comprehension consists of brackets containing an expression followed by  a <code>for</code> clause, then zero or more <code>for</code> or <code>if</code> clauses. The result will be a  new list resulting from evaluating the expression in the context of the <code>for</code> and <code>if</code> clauses which follow it. Syntax: <pre><code>new_list = [expression for member in iterable (if condition)]\n</code></pre> is equivalent to: <pre><code>new_list = []\nfor member in iterable:\n    if conditon:\n        new_list.append(expression)\n</code></pre></p>"},{"location":"python/functions/#dict-comprehension","title":"Dict comprehension","text":"<p>A dict comprehension consists of brackets containing an expression followed by  a for clause, then zero or more <code>for</code> or <code>if</code> clauses. The result will be a  new dict resulting from evaluating the expression in the context of the <code>for</code> and <code>if</code> clauses which follow it. Syntax: <pre><code>new_dict = {key:value for member in iterable (if condition)}\n</code></pre> is equivalent to: <pre><code>new_dict = dict()\nfor member in iterable:\n    if condition:\n        new_dict[key] = value\n</code></pre></p>"},{"location":"python/functions/#set-comprehension","title":"Set comprehension","text":"<p>Similar to list comprehension but instead of making a list,it makes a set, and  instead of using square brackets curly brackets are to be used. Syntax: <pre><code>new_set = {expression for member in iterable (if condition)}\n</code></pre> is equivalent to: <pre><code>new_set = set()\nfor member in iterable:\n    if condition:\n        new_set.add(value)\n</code></pre></p>"},{"location":"python/functions/#generator-functions","title":"Generator functions","text":"<p>Generator functions are very similar to regular functions, but  instead of returning results through <code>return</code> statements, they use <code>yield</code>, which allows them to suspend and resume their state between each call.</p> <p>It\u2019s also worth noting that you can use the <code>return</code> statement in a generator  function. It will produce a <code>StopIteration</code> exception to be raised, effectively  ending the iteration. If a <code>return</code> statement were actually to make the  function return something, it would break the iteration protocol. Python\u2019s consistency prevents this, and allows us great ease when coding.</p> <p>Example: <pre><code># fibonacci.elegant.py\ndef fibonacci(N):\n\"\"\"Return all fibonacci numbers up to N. \"\"\"\n    a, b = 0, 1\n    while a &lt;= N:\n        yield a\n        a, b = b, a + b\n</code></pre></p> <p>Generator objects have also three other methods that allow us to control their  behavior: <code>send</code>, <code>throw</code>, and <code>close</code>. <code>send</code> allows us to communicate a value  back to the generator object, while <code>throw</code> and <code>close</code>, respectively, allow   us to raise an exception within the generator and close it.</p> <p>Example: <pre><code># gen.send.py\ndef counter(start=0):\n  n = start\n  while True:\n      result = yield n             # A\n      print(type(result), result)  # B\n      if result == 'Q':\n          break\n      n += 1\n\nc = counter()\nprint(next(c))         # C\nprint(c.send('Wow!'))  # D\nprint(next(c))         # E\nprint(c.send('Q'))     # F\n</code></pre></p> <p>Output: <pre><code>0\n&lt;class 'str'&gt; Wow!\n1\n&lt;class 'NoneType'&gt; None\n2\n&lt;class 'str'&gt; Q\nTraceback (most recent call last):\nFile \"gen.send.py\", line 14, in &lt;module&gt;\n  print(c.send('Q')) # F\nStopIteration\n</code></pre></p> <p>The <code>yield from</code> expression allows you to yield values from a sub iterator.</p> <p>Example: <pre><code># gen.yield.from.py\ndef print_squares(start, end):\n  yield from (n ** 2 for n in range(start, end))\n\nfor n in print_squares(2, 5):\n  print(n)\n</code></pre></p>"},{"location":"python/functions/#generator-expression","title":"Generator expression","text":"<p>Generator expression behaves like list comprehensions, but generators  allow for one iteration only, then they will be exhausted. The syntax is  exactly the same as list comprehensions, only, instead of wrapping the  comprehension with square brackets, you wrap it with round brackets. Choose  generators for computation on large datasets.</p> <p>Example: <pre><code># generator.expressions.py\ncubes = [k**3 for k in range(5)]  # regular list\ncubes # Output: [0, 1, 8, 27, 64]\ntype(cubes) # Output: &lt;class 'list'&gt;\ncubes_gen = (k**3 for k in range(5))  # create as generator\ncubes_gen # Output: &lt;generator object &lt;genexpr&gt; at 0x103fb5a98&gt;\ntype(cubes_gen) # Output: &lt;class 'generator'&gt;\nlist(cubes_gen)  # this will exhaust the generator  # Output: [0, 1, 8, 27, 64]\nlist(cubes_gen)  # nothing more to give  # Output: []\n</code></pre></p> <p>Python 3.* localizes loop variables in all four forms of comprehensions:  <code>list</code>, <code>dict</code>, <code>set</code>, and <code>generator</code> expressions.</p> <p>Example: <pre><code># scopes.py\nA = 100\nex1 = [A for A in range(5)]\nprint(A)  # prints: 100\n\nex2 = list(A for A in range(5))\nprint(A)  # prints: 100\n\nex3 = dict((A, 2 * A) for A in range(5))\nprint(A)  # prints: 100\n\nex4 = set(A for A in range(5))\nprint(A)  # prints: 100\n\ns = 0\nfor A in range(5):\n  s += A\nprint(A)  # prints: 4\n\n# scopes_noglobal.py\nex1 = [A for A in range(5)]\nprint(A)  # breaks: NameError: name 'A' is not defined\n\n# scopes_for.py\ns = 0\nfor A in range(5):\n  s += A\nprint(A) # prints: 4\n</code></pre></p>"},{"location":"python/iterators/","title":"Iterators","text":""},{"location":"python/iterators/#iterable","title":"Iterable","text":"<p>Iterable: An object is said to be iterable if it\u2019s capable of returning its members one at a time. Lists, tuples, strings, and dictionaries are all iterables. Custom objects that define either of the <code>__iter__</code> or <code>__getitem__</code> methods are also iterables.</p>"},{"location":"python/iterators/#iterator","title":"Iterator","text":"<p>Iterator: An object is said to be an iterator if it represents a stream of data. A custom iterator is required to provide an implementation for <code>__iter__</code> that returns the object itself, and an implementation for <code>__next__</code> that returns the next item of the data stream until the stream is exhausted, at which point all successive calls to <code>__next__</code> simply raise the <code>StopIteration</code> exception. Built-in functions, such as <code>iter</code> and <code>next</code>, are mapped to call <code>__iter__</code> and <code>__next__</code> on an object, behind the scenes.</p> <p>Example of custom iterator: <pre><code>class Splitter:\n\n    def __init__(self, data, split_by=2):\n        self._data = data\n        self.indexes = []\n        for i in range(split_by):\n          self.indexes.extend(list(range(i, len(data), split_by)))\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.indexes:\n            return self._data[self.indexes.pop(0)]\n        raise StopIteration\n\noddeven = Splitter('ThIsIsCoOl!')\nprint(''.join(o for o in oddeven))  # TIICO!hssol\n\nsplitter = Splitter('HoleWdlo!lr', split_by=3)\nprint(''.join(list(splitter)))  # HelloWorld!\n\n\noddeven = Splitter('HoLa')  # or manually...\nit = iter(oddeven)  # this calls oddeven.__iter__ internally\nprint(next(it))  # H\nprint(next(it))  # L\nprint(next(it))  # o\nprint(next(it))  # a\n</code></pre></p>"},{"location":"python/logging/","title":"Logging","text":""},{"location":"python/logging/#intorduction","title":"Intorduction","text":"<p>Logging is performed by calling methods on instances of the <code>Logger</code> class.  Each line you log has a level. The levels normally used are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, and <code>CRITICAL</code>. You can import them from the <code>logging</code>  module.  They are in order of severity and it\u2019s very important to use them properly because they will help you filter the contents of a log file based on what you\u2019re searching for.</p>"},{"location":"python/logging/#basic-configurations","title":"Basic Configurations","text":"<p>You can use the <code>basicConfig(**kwargs)</code> method to configure the logging.</p> <p>Some of the commonly used parameters for basicConfig() are the following:</p> <ul> <li>level: The root logger will be set to the specified severity level. </li> <li>filename: This specifies the file.</li> <li>filemode: If filename is given, the file is opened in this mode. The default is <code>a</code>, which means append.</li> <li>format: This is the format of the log message.</li> </ul> <p>By using the <code>level</code> parameter, you can set what level of log messages you want to record. This can be done by passing one of the constants available in the class, and this would enable all logging calls at or above that level to be logged. By default, it\u2019s set to <code>logging.WARNING</code> Similarly, for logging to a file rather than the console, <code>filename</code> and <code>filemode</code> can be used, and you can decide the format of the message using <code>format</code>.</p> <p>Example: <pre><code>import logging\n\nlogging.basicConfig(\n    filename='temp.log',\n    filemode='w',   # opened in writing mode\n    level=logging.DEBUG,  # minimum level capture in the file\n    format='[%(asctime)s] %(levelname)s: %(message)s',\n    datefmt='%m/%d/%Y %I:%M:%S %p')\n\nmylist = [1, 2, 3]\nlogging.info('Starting to process `mylist`...')\n\nfor position in range(4):\n    try:\n        logging.debug(\n            'Value at position %s is %s', position, mylist[position]\n        )\n    except IndexError:\n        logging.exception('Faulty position: %s', position)\n\nlogging.info('Done parsing `mylist`.')\n</code></pre></p> <p>Log file: <pre><code>[04/05/2020 11:13:48 AM] INFO:Starting to process `mylist`...\n[04/05/2020 11:13:48 AM] DEBUG:Value at position 0 is 1\n[04/05/2020 11:13:48 AM] DEBUG:Value at position 1 is 2\n[04/05/2020 11:13:48 AM] DEBUG:Value at position 2 is 3\n[04/05/2020 11:13:48 AM] ERROR:Faulty position: 3\nTraceback (most recent call last):\n  File \"log.py\", line 15, in &lt;module&gt;\n    position, mylist[position]))\nIndexError: list index out of range\n[04/05/2020 11:13:48 AM] INFO:Done parsing `mylist`.\n</code></pre></p> <p>You can log to a file but you can also log to a network location, to a queue, to a console, and so on. In general, if you have an architecture that is deployed on one machine, logging to a file is acceptable, but when your architecture spans over multiple machines (such as in the case of service-oriented or microservice architectures), it\u2019s very useful to implement a centralized solution for logging so that all log messages coming from each service can be stored and investigated in a single place.</p> <p>It should be noted that calling <code>basicConfig()</code> to configure the root logger works only if the root logger has not been configured before. Basically,  this function can only be called once.</p> <p><code>debug()</code>, <code>info()</code>, <code>warning()</code>, <code>error()</code>, and <code>critical()</code> also call  <code>basicConfig()</code> without arguments automatically if it has not been called  before. This means that after the first time one of the above functions is  called, you can no longer configure the root logger because they would have  called the <code>basicConfig()</code> function internally.</p>"},{"location":"python/logging/#formatting-the-output","title":"Formatting the Output","text":"<p>While you can pass any variable that can be represented as a string from your program as a message to your logs, there are some basic elements that are already a part of the <code>LogRecord</code> and can be easily added to the output format.</p> Attribute name Format Description args You shouldn\u2019t need to format this yourself. The tuple of arguments merged into msg to produce message, or a dict whose values are used for the merge (when there is only one argument, and it is a dictionary). asctime %(asctime)s Human-readable time when the LogRecord was created. By default this is of the form 2003-07-08 16:49:45,896 (the numbers after the comma are millisecond portion of the time). created %(created)f Time when the <code>LogRecord</code> was created (as returned by <code>time.time()</code>). exc_info You shouldn\u2019t need to format this yourself. Exception tuple (\u00e0 la <code>sys.exc_info</code>) or, if no exception has occurred, <code>None</code>. filename %(filename)s Filename portion of pathname. funcName %(funcName)s Name of function containing the logging call. levelname %(levelname)s Text logging level for the message (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). levelno %(levelno)s Numeric logging level for the message (<code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code>). lineno %(lineno)d Source line number where the logging call was issued (if available). message %(message)s The logged message, computed as msg % args. This is set when Formatter.format() is invoked. module %(module)s Module (name portion of filename). msecs %(msecs)d Millisecond portion of the time when the <code>LogRecord</code> was created. msg You shouldn\u2019t need to format this yourself. The format string passed in the original logging call. Merged with args to produce message, or an arbitrary object. name %(name)s Name of the logger used to log the call. pathname %(pathname)s Full pathname of the source file where the logging call was issued (if available). process %(process)d Process ID (if available). processName %(processName)s Process name (if available). relativeCreated %(relativeCreated)d Time in milliseconds when the <code>LogRecord</code> was created, relative to the time the logging module was loaded. stack_info You shouldn\u2019t need to format this yourself. Stack frame information (where available) from the bottom of the stack in the current thread, up to and including the stack frame of the logging call which resulted in the creation of this record. thread %(thread)d Thread ID (if available). threadName %(threadName)s Thread name (if available). <p>Example: <pre><code>import logging\n\nlogging.basicConfig(format='%(process)d-%(levelname)s-%(message)s')\nlogging.warning('This is a Warning')\n</code></pre></p> <p>Output: <pre><code>18472-WARNING-This is a Warning\n</code></pre></p>"},{"location":"python/logging/#logging-variable-data","title":"Logging Variable Data","text":"<p>The arguments passed to the method would be included as variable data in the message.</p> <p>Example: <pre><code>import logging\n\nname = 'John'\nlogging.error('%s raised an error', name)\n</code></pre></p> <p>Output: <pre><code>ERROR:root:John raised an error\n</code></pre></p>"},{"location":"python/logging/#capturing-stack-traces","title":"Capturing Stack Traces","text":"<p>The logging module also allows you to capture the full stack traces in an application. Exception information can be captured if the <code>exc_info</code> parameter is passed as <code>True</code>. This works with all the level methods.</p> <p>Example: <pre><code>import logging\n\na = 5\nb = 0\n\ntry:\n  c = a / b\nexcept Exception as e:\n  logging.error(\"Exception occurred\", exc_info=True)\n  # or one should use `logging.execption(\"Exception occured\")`\n</code></pre></p> <p>Output: <pre><code>ERROR:root:Exception occurred\nTraceback (most recent call last):\n  File \"exceptions.py\", line 6, in &lt;module&gt;\n    c = a / b\nZeroDivisionError: division by zero\n</code></pre></p> <p>If <code>exc_info</code> is not set to <code>True</code>, the output of the above program would not tell us anything about the exception, which, in a real-world scenario, might  not be as simple as a <code>ZeroDivisionError</code>. Imagine trying to debug an error in  a complicated codebase with a log that shows only this:</p> <pre><code>ERROR:root:Exception occurred\n</code></pre> <p>If you\u2019re logging from an exception handler, use the <code>logging.exception()</code>  method, which logs a message with level <code>ERROR</code> and adds exception information  to the message. To put it more simply, calling <code>logging.exception()</code> is like  calling <code>logging.error(exc_info=True)</code>. But since this method always dumps  exception information, it should only be called from an exception handler. </p>"},{"location":"python/logging/#classes-and-functions","title":"Classes and Functions","text":"<p>You can (and should) define your own logger by creating an object of the <code>Logger</code> class, especially if your application has multiple modules. The most commonly used classes defined in the logging module are the following:</p> <ul> <li> <p>Logger: This is the class whose objects will be used in the application code directly to call the functions.</p> </li> <li> <p>LogRecord: Loggers automatically create LogRecord objects that have all the information related to the event being logged, like the name of the logger,  the function, the line number, the message, and more.</p> </li> <li> <p>Handler: Handlers send the LogRecord to the required output destination,  like the console or a file. Handler is a base for subclasses like <code>StreamHandler</code>, <code>FileHandler</code>, <code>SMTPHandler</code>, <code>HTTPHandler</code>, and more. These subclasses send the logging outputs to corresponding destinations, like  <code>sys.stdout</code> or a disk file.</p> </li> <li> <p>Formatter: This is where you specify the format of the output by specifying a string format that lists out the attributes that the output should contain.</p> </li> </ul> <p>Out of these, we mostly deal with the objects of the <code>Logger</code> class, which are instantiated using the module-level function <code>logging.getLogger(name)</code>.  Multiple calls to <code>getLogger()</code> with the same name will return a reference to the same <code>Logger</code> object, which saves us from passing the logger objects to every part where it\u2019s needed. </p> <p>Example: <pre><code>import logging\n\nlogger = logging.getLogger('example_logger')\nlogger.warning('This is a warning')\n</code></pre></p> <p>Output: <pre><code>This is a warning\n</code></pre></p> <p>This creates a custom logger named <code>example_logger</code>, but unlike the root logger, the name of a custom logger is not part of the default output format and has to be added to the configuration. Again, unlike the root logger, a custom logger can\u2019t be configured using <code>basicConfig()</code>. You have to configure it using Handlers and Formatters:</p> <p>It is recommended that we use module-level loggers by passing <code>__name__</code> as the name parameter to <code>getLogger()</code> to create a logger object as the name of the logger itself would tell us from where the events are being logged.  <code>__name__</code> is a special built-in variable in Python which evaluates to the name of the current module.</p>"},{"location":"python/logging/#using-handlers","title":"Using handlers","text":"<p>A logger that you create can have more than one handler, which means you can set it up to be saved to a log file and also send it over email.</p> <p>Like loggers, you can also set the severity level in handlers. This is useful if you want to set multiple handlers for the same logger but want different severity levels for each of them. For example, you may want logs with level <code>WARNING</code> and above to be logged to the console, but everything with level <code>ERROR</code> and above should also be saved to a file.</p> <p>Example: <pre><code>import logging\n\n# Create a custom logger\nlogger = logging.getLogger(__name__)\n\n# Create handlers\nc_handler = logging.StreamHandler()\nf_handler = logging.FileHandler('file.log')\nc_handler.setLevel(logging.WARNING)\nf_handler.setLevel(logging.ERROR)\n\n# Create formatters and add it to handlers\nc_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\nf_format = logging.Formatter(\n    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nc_handler.setFormatter(c_format)\nf_handler.setFormatter(f_format)\n\n# Add handlers to the logger\nlogger.addHandler(c_handler)\nlogger.addHandler(f_handler)\n\nlogger.warning('This is a warning')\nlogger.error('This is an error')\n</code></pre></p> <p>Console output: <pre><code>__main__ - WARNING - This is a warning\n__main__ - ERROR - This is an error\n</code></pre></p> <p>File output: <pre><code>2020-04-05 16:12:21,723 - __main__ - ERROR - This is an error\n</code></pre></p>"},{"location":"python/logging/#list-of-handlers","title":"List of handlers","text":"<ul> <li> <p>StreamHandler instances send messages to streams (file-like objects).</p> </li> <li> <p>FileHandler instances send messages to disk files.</p> </li> <li> <p>BaseRotatingHandler is the base class for handlers that rotate log files at a certain point. It is not meant to be instantiated directly. Instead, use <code>RotatingFileHandler</code> or <code>TimedRotatingFileHandler</code>.</p> </li> <li> <p>RotatingFileHandler instances send messages to disk files, with support for maximum log file sizes and log file rotation.</p> </li> <li> <p>TimedRotatingFileHandler instances send messages to disk files, rotating the log file at certain timed intervals.</p> </li> <li> <p>SocketHandler instances send messages to TCP/IP sockets. Since 3.4, Unix domain sockets are also supported.</p> </li> <li> <p>DatagramHandler instances send messages to UDP sockets. Since 3.4, Unix domain sockets are also supported.</p> </li> <li> <p>SMTPHandler instances send messages to a designated email address.</p> </li> <li> <p>SysLogHandler instances send messages to a Unix syslog daemon, possibly on a remote machine.</p> </li> <li> <p>NTEventLogHandler instances send messages to a Windows NT/2000/XP event log.</p> </li> <li> <p>MemoryHandler instances send messages to a buffer in memory, which is flushed whenever specific criteria are met.</p> </li> <li> <p>HTTPHandler instances send messages to an HTTP server using either GET or POST semantics.</p> </li> <li> <p>WatchedFileHandler instances watch the file they are logging to. If the file changes, it is closed and reopened using the file name. This handler is only useful on Unix-like systems; Windows does not support the underlying mechanism used.</p> </li> <li> <p>QueueHandler instances send messages to a queue, such as those implemented in the queue or multiprocessing modules.</p> </li> <li> <p>NullHandler instances do nothing with error messages. They are used by library developers who want to use logging, but want to avoid the \u2018No handlers could be found for logger XXX\u2018 message which can be displayed if the library user has not configured logging.</p> </li> </ul>"},{"location":"python/logging/#other-configuration-methods","title":"Other configuration methods","text":"<p>You can configure logging as shown above using the module and class functions or by creating a config file or a dictionary and loading it using <code>fileConfig()</code> or <code>dictConfig()</code> respectively. These are useful in case you want to change your logging configuration in a running application.</p>"},{"location":"python/logging/#conf-file-approach","title":"Conf file approach","text":"<p>Example config file: <code>log.conf</code> <pre><code>[loggers]\nkeys=root,sampleLogger\n\n[handlers]\nkeys=consoleHandler\n\n[formatters]\nkeys=sampleFormatter\n\n[logger_root]\nlevel=DEBUG\nhandlers=consoleHandler\n\n[logger_sampleLogger]\nlevel=DEBUG\nhandlers=consoleHandler\nqualname=sampleLogger\npropagate=0\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=DEBUG\nformatter=sampleFormatter\nargs=(sys.stdout,)\n\n[formatter_sampleFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n</code></pre></p> <p>In the above file, there are two loggers, one handler, and one formatter.  After their names are defined, they are configured by adding the words logger, handler, and formatter before their names separated by an underscore.</p> <p>To load this config file, you have to use <code>fileConfig()</code>: <pre><code>import logging\nimport logging.config\n\nlogging.config.fileConfig(fname='log.conf', disable_existing_loggers=False)\n\n# Get the logger specified in the file\nlogger = logging.getLogger(__name__)\n\nlogger.debug('This is a debug message')\n</code></pre></p> <p>Output: <pre><code>2018-07-13 13:57:45,467 - __main__ - DEBUG - This is a debug message\n</code></pre></p> <p>The path of the config file is passed as a parameter to the <code>fileConfig()</code>  method, and the <code>disable_existing_loggers</code> parameter is used to keep or disable the loggers that are present when the function is called. It defaults to <code>True</code> if not mentioned. If <code>propagate</code> attribute evaluates to true, events logged to this logger will be passed to the handlers of higher level (ancestor) loggers, in addition to any handlers attached to this logger.</p>"},{"location":"python/logging/#dictionary-approach","title":"Dictionary approach","text":"<p>Here\u2019s the same configuration in a YAML format for the dictionary approach:</p> <pre><code>version: 1\nformatters:\nsimple:\nformat: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nhandlers:\nconsole:\nclass: logging.StreamHandler\nlevel: DEBUG\nformatter: simple\nstream: ext://sys.stdout\nloggers:\nsampleLogger:\nlevel: DEBUG\nhandlers: [console]\npropagate: no\nroot:\nlevel: DEBUG\nhandlers: [console]\n</code></pre> <p>Load config from a <code>yaml</code> file:</p> <p>Example: <pre><code>import logging\nimport logging.config\nimport yaml\n\nwith open('config.yaml', 'r') as f:\n    config = yaml.safe_load(f.read())\n    logging.config.dictConfig(config)\n\nlogger = logging.getLogger(__name__)\n\nlogger.debug('This is a debug message')\n</code></pre></p> <p>Output: <pre><code>2018-07-13 14:05:03,766 - __main__ - DEBUG - This is a debug message\n</code></pre></p>"},{"location":"python/logging/#links","title":"Links","text":"<ul> <li>RealPython - Logging</li> <li>Basic logging tutorial - Python Docs</li> <li>Logging Cookbook - Python Docs</li> </ul>"},{"location":"python/oop/","title":"Object-oriented programming (OOP)","text":"<p>Everything in Python is an <code>object</code>.</p>"},{"location":"python/oop/#class","title":"Class","text":"<p>Class attributes are shared among all instances, while instance attributes are not; therefore, you should use class attributes to provide the states and  behaviors to be shared by all instances, and use instance attributes for data  that belongs just to one specific object.</p> <p>When you search for an attribute in an <code>object</code>, if it is not found, Python  keeps searching in the class that was used to create that object (and keeps  searching until it\u2019s either found or the end of the inheritance chain is  reached).</p> <p>Python class does not have a constructor but it has an initializer. It\u2019s  called an initializer since it works on an already-created instance, and  therefore it\u2019s called <code>__init__</code>.  It\u2019s a magic method, which is run right  after the object is created. Python objects also have a <code>__new__</code> method,  which is the actual constructor. In practice, it\u2019s not so common to have to  override it though, it\u2019s a practice that is mostly used when coding  metaclasses.</p> <p>Class names are always written using <code>CapWords</code></p> <p>To check whether an object is an instance of a class, use the <code>isinstance</code>  method. It is recommended over sheer type comparison:  <code>(type(object) == Class)</code>.</p> <p>A class is a subclass of itself.</p> <p>Also, when in the code, you want to use a name that is a Python-reserved  keyword or a built-in function or class, the convention is to add a trailing  underscore to the name.</p> <p>To check whether a class <code>class1</code> is a subclass of another class <code>class2</code>, use the <code>issubclass</code> method, Example: <code>issubclass(class1, class2)</code>.</p> <p><code>super</code> is a function that returns a proxy object that delegates method calls  to a parent or sibling class. </p> <p>Example: <pre><code>class Book:\n    def __init__(self, title, publisher, pages):\n        self.title = title\n        self.publisher = publisher\n        self.pages = pages\n\nclass Ebook(Book):\n    def __init__(self, title, publisher, pages, format_):\n        super().__init__(title, publisher, pages)\n        # Another way to do the same thing is:\n        # super(Ebook, self).__init__(title, publisher, pages)\n        self.format_ = format_\n\nebook = Ebook('Learn Python Programming', 'Dummy Publishing', 500, 'PDF')\nprint(ebook.title) # Learn Python Programming\nprint(ebook.publisher) # Dummy Publishing\nprint(ebook.pages) # 500\nprint(ebook.format_) # PDF\n</code></pre></p> <p>Singleton classes are not really used as often in Python as in other  languages. The effect of a singleton is usually better implemented as a global  variable in a module.</p> <p>When analyzing a system, objects typically represent nouns in the original problem, while methods are normally verbs. Attributes may show up as adjectives or more nouns. Name your classes, attributes, and methods accordingly.</p>"},{"location":"python/oop/#the-method-resolution-ordermro","title":"The Method Resolution Order(MRO)","text":"<p>The Method Resolution Order(MRO) is the order in which base classes are  searched for a member during lookup. The list of the ancestors of a class C, including the class itself, ordered  from the nearest ancestor to the furthest, is called the class precedence list  or the linearization of C. C3 algorithm to compute the linearization L[C] of  the class C is as follows: The linearization of C is the sum of C plus the  merge of the linearizations of the parents and the list of the parents. In symbolic notation:</p> <p>L[C(B1 \u2026 BN)] = C + merge(L[B1], \u2026 L[BN], B1, \u2026 BN)</p> <p>Computation of merge: Take the head of the first list, i.e L[B1][0]; if this  head is not in the tail of any of the other lists, then add it to the  linearization of C and remove it from the lists in the merge, otherwise look at the head of the next list and take it, if it is a good head. Then repeat the  operation until all the class are removed or it is impossible to find good  heads.</p> <p>Example 1:</p> <pre><code>O = object\nclass F(O): pass\nclass E(O): pass\nclass D(O): pass\nclass C(D,F): pass\nclass B(D,E): pass\nclass A(B,C): pass\n</code></pre> <p>In this case the inheritance graph can be drawn as</p> graph TD;     F--&gt;O;     E--&gt;O;     D--&gt;O;     C--&gt;D;     C--&gt;F;     B--&gt;D;     B--&gt;E;     A--&gt;B;     A--&gt;C; <p>The linearizations of O,D,E and F are trivial:</p> <p>L[O] = O</p> <p>L[D] = D O</p> <p>L[E] = E O</p> <p>L[F] = F O</p> <p>The linearization of B can be computed as</p> <p>L[B] = B + merge(DO, EO, DE)</p> <p>We see that D is a good head, therefore we take it and we are reduced to  compute <code>merge(O,EO,E)</code>. Now O is not a good head, since it is in the tail of  the sequence EO. In this case the rule says that we have to skip to the next  sequence. Then we see that E is a good head; we take it and we are reduced to  compute <code>merge(O,O)</code> which gives O. Therefore</p> <p>L[B] =  B D E O</p> <p>Using the same procedure one finds:</p> <p>L[C] = C + merge(DO,FO,DF)</p> <p>L[C] = C + D + merge(O,FO,F)</p> <p>L[C] = C + D + F + merge(O,O)</p> <p>L[C] = C D F O</p> <p>Now we can compute:</p> <p>L[A] = A + merge(BDEO,CDFO,BC)</p> <p>L[A] = A + B + merge(DEO,CDFO,C)</p> <p>L[A] = A + B + C + merge(DEO,DFO)</p> <p>L[A] = A + B + C + D + merge(EO,FO)</p> <p>L[A] = A + B + C + D + E + merge(O,FO)</p> <p>L[A] = A + B + C + D + E + F + merge(O,O)</p> <p>L[A] = A B C D E F O</p> <p>Example 2:</p> <pre><code>O = object\nclass X(O): pass\nclass Y(O): pass\nclass D(O): pass\nclass A(X,Y): pass\nclass B(Y,X): pass\nclass C(A,B): pass\n</code></pre> <p>In this case the inheritance graph can be drawn as</p> graph TD;     X--&gt;O;     Y--&gt;O;     A--&gt;X;     A--&gt;Y;     B--&gt;Y;     B--&gt;X;     C--&gt;A;     C--&gt;B; <p>The linearizations of O,X,Y,A and B are as follows:</p> <p>L[O] = O</p> <p>L[X] = X O</p> <p>L[Y] = Y O</p> <p>L[A] = A X Y O</p> <p>L[B] = B Y X O</p> <p>The linearization of C can be computed as</p> <p>L[C] = C + merge(AXYO, BYXO, AB)</p> <p>L[C] = C + A + merge(XYO, BYXO, B)</p> <p>L[C] = C + A + B + merge(XYO, YXO)</p> <p>At this point we cannot merge the lists <code>XYO</code> and <code>YXO</code>, since X is in the tail of <code>YXO</code> whereas Y is in the tail of <code>XYO</code>: therefore there are no good heads  and the C3 algorithm stops. Python&gt;=2.3 raises an error and refuses to create  the class C.</p> <p>To get the MRO of a class C use: <code>C.__mro__</code></p>"},{"location":"python/oop/#static-methods","title":"Static methods","text":"<p>Static methods are methods which need to be in the namespace of the class. They cannot access class or instance attributes. They can be invoked by class and  instances both. To create a static method use <code>staticmethod</code> decorator on a  method. Static methods are actually quite helpful in breaking up the logic of  a method to improve its layout. Cannot call <code>super()</code> method</p>"},{"location":"python/oop/#class-methods","title":"Class methods","text":"<p>Class methods are methods which need to operate only on the class level and  not on the instance level. They can access class attributes but not instance  attributes. They can be invoked by class and instances both. They are generally used to provide factory capability to a class. To create a class method use  <code>classmethod</code> decorator on a method.</p>"},{"location":"python/oop/#instance-methods","title":"Instance methods","text":"<p>Instance methods are methods which need to operate on the instance level. They  can access class attributes as well as instance attributes. They can be invoked by instances only.</p>"},{"location":"python/oop/#private-methods-and-name-mangling","title":"Private methods and name mangling","text":"<p>In python everything is public. Therefore, we rely on conventions and on a  mechanism called name mangling.</p> <ul> <li>Public attribute -&gt; no leading underscore in the attribute\u2019s name</li> <li>Private attribute -&gt; two leading underscore in the attribute\u2019s name</li> </ul> <p>Private attributes are meant to be for internal use only, they should not be used or modified from outside.</p> <p>Example: <pre><code>class Parent:\n    def __init__(self, name):\n        self._name = name\n\n    def get_name(self):\n        print('Name is {}'.format(self._name))\n\nclass Child(Parent):\n    def set_new_name(self, new_name):\n        self._name = new_name\n        print('New name is {}'.format(self._name))\n\n\nobj = Child('mickey')\nobj.get_name()    # Name is mickey\nobj.set_new_name('minnie')  # New name is minnie\nobj.get_name()    # Name is minnie &lt;- Unexpected behaviour\nprint(vars(obj).keys()) # dict_keys(['_name'])\n</code></pre></p> <p>After adding another leading underscore</p> <pre><code>class Parent:\n    def __init__(self, name):\n        self.__name = name\n\n    def get_name(self):\n        print('Name is {}'.format(self.__name))\n\nclass Child(Parent):\n    def set_new_name(self, new_name):\n        self.__name = new_name\n        print('New name is {}'.format(self.__name))\n\n\nobj = Child('mickey')\nobj.get_name()    # Name is mickey\nobj.set_new_name('minnie')  # New name is minnie\nobj.get_name()    # Name is mickey &lt;- Expected behaviour\nprint(vars(obj).keys()) # dict_keys(['_Parent__name', '_Child__name'])\n</code></pre> <p>Name mangling means that any attribute name that has at least two leading underscores and at most one trailing underscore, such as <code>__my_attr</code>, is replaced with a name that includes an underscore and the class name before the actual name, such as <code>_ClassName__my_attr</code>.</p> <p>This means that when you inherit from a class, the mangling mechanism gives your private attribute two different names in the base and child classes so that name collision is avoided.</p> <p>Every class and instance object stores references to their attributes in a special attribute called <code>__dict__</code>. Built in function <code>vars</code> is mapped to call <code>__dict__</code> on an object, behind  the scenes. <code>vars(obj)</code> is equivalent to <code>obj.__dict__</code>.</p>"},{"location":"python/oop/#property-decorator","title":"Property decorator","text":"<p>In Python, <code>property()</code> is a built-in function that creates and returns a property object. The signature of this function is <pre><code>property(fget=None, fset=None, fdel=None, doc=None)\n</code></pre> where, <code>fget</code> is function to get value of the attribute, <code>fset</code> is function to set value of the attribute, <code>fdel</code> is function to delete the attribute and <code>doc</code> is a string (like a comment). As seen from the implementation, these function arguments are optional. So, a property object can simply be created as follows.</p> <p>When you decorate a method with <code>property</code>, you can use the name of the method as if it were a data attribute. Because of this, it\u2019s always best to refrain from putting logic that would take a while to complete in such methods because, by accessing them as attributes, we are not expecting to wait.</p> <p>Example: <pre><code>class PropertyTest:\n    def __init__(self, test):\n        self._test = test\n\n    def get_test(self):\n      print(\"Getting test var\")\n      return self._test\n\n    def set_test(self, test):\n      print(f\"Setting test var to {test}\")\n      self._test = test\n\n    def del_test(self):\n      print(\"Deleting test var\")\n\n    test = property(get_test, set_test, del_test, \"Some test variable\")\n\nobj = PropertyTest('mickey')\nprint(obj.__class__.test.__doc__)   # Some test variable\nprint(obj.test)    # Getting test var | mickey\nobj.test = 'minnie' # Setting test var to minnie\ndel obj.test    # Deleting test var\n</code></pre></p> <p>A property object has three methods, <code>getter()</code>, <code>setter()</code>, and <code>deleter()</code> to specify <code>fget</code>, <code>fset</code> and <code>fdel</code> at a later point. This means, the line</p> <p><pre><code>test = property(get_test, set_test, del_test, \"Some test variable\")\n</code></pre> is equivalent to <pre><code>test = property(doc=\"Some test variable\")\n\n# assign fget\ntest = test.getter(get_test)\n# assign fset\ntest = test.setter(set_test)\n# assign fdel\ntest = test.deleter(det_test)\n</code></pre></p>"},{"location":"python/oop/#links","title":"Links","text":"<ul> <li>Python MRO</li> <li>Magic methods</li> </ul>"},{"location":"python/pickling/","title":"Pickling","text":""},{"location":"python/pickling/#pickling-and-unpickling","title":"Pickling and Unpickling","text":"<p>The <code>pickle</code> module, is not human readable, translates to bytes, is Python specific, and, thanks to the wonderful Python introspection capabilities, it supports an extremely large amount of data types.</p> <p>I think that the most important concern regarding <code>pickle</code> lies in the security threats you are exposed to when you use it. Unpickling erroneous or malicious data from an untrusted source can be very dangerous, so if you decide to adopt it in your application, you need to be extra careful.</p> <p>Code: <pre><code>import pickle\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n    first_name: str\n    last_name: str\n    id: int\n\n    def greet(self):\n        print(f'Hi, I am {self.first_name} {self.last_name}'\n              f' and my ID is {self.id}'\n        )\n\npeople = [\n    Person('Obi-Wan', 'Kenobi', 123),\n    Person('Anakin', 'Skywalker', 456),\n]\n\n# save data in binary format to a file\nwith open('data.pickle', 'wb') as stream:\n    pickle.dump(people, stream)\n\n# load data from a file\nwith open('data.pickle', 'rb') as stream:\n    peeps = pickle.load(stream)\n\nfor person in peeps:\n    person.greet()\n</code></pre></p> <p>Output: <pre><code>Hi, I am Obi-Wan Kenobi and my ID is 123\nHi, I am Anakin Skywalker and my ID is 456  \n</code></pre></p>"},{"location":"python/profiling/","title":"Profiling","text":""},{"location":"python/profiling/#introduction","title":"Introduction","text":"<p>Profiling means having the application run while keeping track of several different parameters, such as the number of times a function is called and the amount of time spent inside it. Profiling can help us find the bottlenecks in our application, so that we can improve only what is really slowing us down.</p>"},{"location":"python/profiling/#implementations","title":"Implementations","text":"<ul> <li>cProfile is recommended for most users, it\u2019s a C extension with reasonable  overhead that makes it suitable for profiling long-running programs.</li> <li>profile is a pure Python module whose interface is imitated by  cProfile , but which adds significant overhead to profiled programs.</li> </ul> <p>This interface does determinist profiling, which means that all function calls, function returns, and exception events are monitored, and precise timings are made for the intervals between these events. Another approach , called statistical profiling, randomly samples the effective instruction pointer, and deduces where time is being spent.</p> <p>Example: <pre><code># calculate pythagorean triples\ndef calc_triples(mx):\n    triples = []\n    for a in range(1, mx + 1):\n        for b in range(a, mx + 1):\n            hypotenuse = calc_hypotenuse(a, b)\n            if is_int(hypotenuse):\n                triples.append((a, b, int(hypotenuse)))\n    return triples\n\ndef calc_hypotenuse(a, b):\n    return (a**2 + b**2) ** .5\n\ndef is_int(n):  # n is expected to be a float\n    return n.is_integer()\n\ntriples = calc_triples(1000)\n</code></pre></p> <p>Output: <pre><code>$ python -m cProfile triples.py\n1502538 function calls in 0.465 seconds\n\nOrdered by: standard name\n\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n500500    0.262    0.000    0.262    0.000 triples.py:11(calc_hypotenuse)\n500500    0.062    0.000    0.086    0.000 triples.py:14(is_int)\n     1    0.000    0.000    0.465    0.465 triples.py:2(&lt;module&gt;)\n     1    0.117    0.117    0.465    0.465 triples.py:2(calc_triples)\n     1    0.000    0.000    0.465    0.465 {built-in method builtins.exec}\n  1034    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n     1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n500500    0.024    0.000    0.024    0.000 {method 'is_integer' of 'float' objects}\n</code></pre></p> <p>Taking a look at the profiling report, we notice that the algorithm has spent <code>0.262</code> seconds inside <code>calc_hypotenuse</code>, which is way more than the 0 <code>.062</code> seconds spent inside <code>is_int</code>, given that they were called the same number of times, so let\u2019s see whether we can boost <code>calc_hypotenuse</code> a little. The <code>**</code> power operator is quite expensive, and in <code>calc_hypotenuse</code>, we\u2019re using it three times. Fortunately, we can easily transform two of those into simple multiplications, like this:</p> <pre><code>def calc_hypotenuse(a, b): \n    return (a*a + b*b) ** .5 \n</code></pre> <p>This simple change should improve things. If we run the profiling again, we see that <code>0.262</code> is now down to <code>0.111</code>. Not bad! This means now we\u2019re spending only about 42% of the time inside calc_hypotenuse that we were before.</p>"},{"location":"python/profiling/#guidelines","title":"Guidelines","text":"<ul> <li>It\u2019s quite important to be able to profile software on a system that is as close as possible to the one the software is deployed on, if not actually on that one.</li> </ul>"},{"location":"python/profiling/#when-to-profile","title":"When to profile?","text":"<p>Profiling is super cool, but we need to know when it is appropriate to do it, and in what measure we need to address the results we get from it.</p> <p>So, first and foremost: correctness. You want your code to deliver the correct results, therefore write tests, find edge cases, and stress your code in every way you think makes sense. Don\u2019t be protective, don\u2019t put things in the back of your brain for later because you think they\u2019re not likely to happen. Be thorough.</p> <p>Second, take care of coding best practices. Remember the following \u2014readability, extensibility, loose coupling, modularity, and design. Apply OOP principles: encapsulation, abstraction, single responsibility, open /closed, and so on. Read up on these concepts. They will open horizons for you, and they will expand the way you think about code.</p> <p>Third, refactor like a beast!</p> <p>And, finally, when all of this has been taken care of, then and only then , take care of optimizing and profiling.</p>"},{"location":"python/references/","title":"References","text":"Title Author Publishing Date Learn Python Programming - Second Edition Fabrizio Romano June 2018 Python 3 Object-Oriented Programming - Third Edition Dusty Phillips October 2018"},{"location":"python/testing/coverage/","title":"Coverage","text":""},{"location":"python/testing/coverage/#introduction","title":"Introduction","text":"<p>Coverage.py is a tool for measuring code coverage of Python programs. It monitors your program, noting which parts of the code have been executed,  then analyzes the source to identify code that could have been executed but was not.</p> <p>Coverage measurement is typically used to gauge the effectiveness of tests.  It can show which parts of your code are being exercised by tests, and which are not.</p>"},{"location":"python/testing/coverage/#installation","title":"Installation","text":"<pre><code>pip install coverage\n</code></pre>"},{"location":"python/testing/coverage/#basic-example","title":"Basic example","text":"<p>Use coverage run to run your test suite and gather data. However you normally run your test suite, you can run your test runner under coverage.  If your test runner command starts with python, just replace the initial python with coverage run.</p> <p>Instructions for pytest:</p> <p>If you usually use: <pre><code>pytest arg1 arg2 arg3\n</code></pre></p> <p>then you can run your tests under coverage with: <pre><code>coverage run -m pytest arg1 arg2 arg3\n</code></pre></p> <p>To limit coverage measurement to code in the current directory, and also find files that weren\u2019t executed at all, add the <code>--source=.</code> argument to your coverage command line.</p>"},{"location":"python/testing/coverage/#command-line-usage","title":"Command line usage","text":"<p>Coverage.py has a number of commands which determine the action performed:</p> <ul> <li>run: Run a Python program and collect execution data.</li> <li>report: Report coverage results.</li> <li>html: Produce annotated HTML listings with coverage results.</li> <li>json: Produce a JSON report with coverage results.</li> <li>xml: Produce an XML report with coverage results.</li> <li>annotate: Annotate source files with coverage results.</li> <li>erase: Erase previously collected coverage data.</li> <li>combine: Combine together a number of data files.</li> <li>debug: Get diagnostic information.</li> </ul>"},{"location":"python/testing/coverage/#reporting","title":"Reporting","text":"<p>Use <code>coverage report</code> to report on the results: <pre><code>coverage report -m\nName                      Stmts   Miss  Cover   Missing\n-------------------------------------------------------\nmy_program.py                20      4    80%   33-35, 39\nmy_other_module.py           56      6    89%   17-23\n-------------------------------------------------------\nTOTAL                        76     10    87%\n</code></pre></p>"},{"location":"python/testing/coverage/#html-report","title":"HTML Report","text":"<p>For a nicer presentation, use <code>coverage html</code> to get annotated HTML listings detailing missed lines. Then open <code>htmlcov/index.html</code> in your browser, to see a report like this.</p>"},{"location":"python/testing/coverage/#links","title":"Links","text":"<ul> <li>Official Docs</li> </ul>"},{"location":"python/testing/introduction/","title":"Introduction","text":""},{"location":"python/testing/introduction/#overview-of-testing-categories","title":"Overview of testing categories","text":"<p>White-box tests are those that exercise the internals of the code; they inspect it down to a very fine level of detail. On the other hand, black -box tests are those that consider the software under test as if within a box, the internals of which are ignored. Even the technology, or the language used inside the box, is not important for black-box tests. What they do is plug input into one end of the box and verify the output at the other end\u2014that\u2019s it. There is also an in-between category, called gray-box testing, which involves testing a system in the same way we do with the black-box approach, but having some knowledge about the algorithms and datastructures used to write the software and only partial access to its source code.</p>"},{"location":"python/testing/introduction/#in-depth-testing-categories","title":"In-depth testing categories","text":"<ul> <li>Frontend tests: Make sure that the client side of your application is exposing the information that it should, all the links, the buttons, the advertising, everything that needs to be shown to the client. It may also verify that it is possible to walk a certain path through the user interface.</li> <li>Scenario tests: Make use of stories (or scenarios) that help the tester  work through a complex problem or test a part of the system. </li> <li>Integration tests: Verify the behavior of the various components of your application when they are working together sending messages through interfaces.</li> <li>Smoke tests: Particularly useful when you deploy a new update on your application. They check whether the most essential, vital parts of your application are still working as they should and that they are not on fire . This term comes from when engineers tested circuits by making sure nothing was smoking.</li> <li>Acceptance tests, or user acceptance testing (UAT): What a developer does with a product owner (for example, in a SCRUM environment) to determine whether the work that was commissioned was carried out correctly.</li> <li>Functional tests: Verify the features or functionalities of your software.</li> <li>Destructive tests: Take down parts of your system, simulating a failure , to establish how well the remaining parts of the system perform. These kinds of tests are performed extensively by companies that need to provide an extremely reliable service, such as Amazon and Netflix, for example.</li> <li>Performance tests: Aim to verify how well the system performs under a specific load of data or traffic so that, for example, engineers can get a better understanding of the bottlenecks in the system that could bring it to its knees in a heavy-load situation, or those that prevent scalability.</li> <li>Usability tests, and the closely related user experience (UX) tests: Aim to check whether the user interface is simple and easy to understand and use. They aim to provide input to the designers so that the user experience is improved.</li> <li>Security and penetration tests: Aim to verify how well the system is protected against attacks and intrusions.</li> <li>Unit tests: Help the developer to write the code in a robust and consistent way, providing the first line of feedback and defense against coding mistakes, refactoring mistakes, and so on.</li> <li>Regression tests: Provide the developer with useful information about a feature being compromised in the system after an update. Some of the causes for a system being said to have a regression are an old bug coming back to life, an existing feature being compromised, or a new issue being introduced.</li> </ul>"},{"location":"python/testing/introduction/#testing-guidelines","title":"Testing guidelines","text":"<ul> <li>Keep them as simple as possible. It\u2019s okay to violate some good coding rules, such as hardcoding values or duplicating code. Tests need, first and foremost, to be as readable as possible and easy to understand. When tests are hard to read or understand, you can never be confident they are actually making sure your code is performing correctly.</li> <li>Tests should verify one thing and one thing only. It\u2019s very important that you keep them short and contained. It\u2019s perfectly fine to write multiple tests to exercise a single object or function. Just make sure that each test has one and only one purpose.</li> <li>Tests should not make any unnecessary assumption when verifying data.</li> <li>Tests should exercise the what, rather than the how. Tests should focus on checking what a function is supposed to do, rather than how it is doing it. The type of test you have to write when you concentrate on the how is more likely to degrade the quality of your testing code base when you amend your software frequently.</li> <li>Tests should use the minimal set of fixtures needed to do the job.</li> <li>Tests should run as fast as possible.</li> <li>Tests should use up the least possible amount of resources.</li> <li>Anything that crosses the boundaries of your application needs to be simulated. We don\u2019t want to talk to a real data source, and we don\u2019t want to actually run real functions if they are communicating with anything that is not contained in our application. A few examples would be a database, a search service, an external API, and a file in the filesystem.</li> </ul>"},{"location":"python/testing/introduction/#test-driven-development-tdd","title":"Test-driven development (TDD)","text":"<p>TDD is a software development methodology that is based on the continuous repetition of a very short development cycle.</p> <p>First, the developer writes a test, and makes it run. The test is supposed to check a feature that is not yet part of the code. Maybe it is a new feature to be added, or something to be removed or amended. Running the test will make it fail and, because of this, this phase is called Red.</p> <p>When the test has failed, the developer writes the minimal amount of code to make it pass. When running the test succeeds, we have the so-called Green phase. In this phase, it is okay to write code that cheats, just to make the test pass. This technique is called fake it \u2018till you make it. In a second moment, tests are enriched with different edge cases, and the cheating code then has to be rewritten with proper logic. Adding other test cases is called triangulation.</p> <p>The last piece of the cycle is where the developer takes care of both the code and the tests (in separate times) and refactors them until they are in the desired state. This last phase is called Refactor.</p> <p>The TDD mantra therefore is Red-Green-Refactor.</p> <p>When you write your code before the tests, you have to take care of what the code has to do and how it has to do it, both at the same time. On the other hand, when you write tests before the code, you can concentrate on the what part alone, while you write them. When you write the code afterward, you will mostly have to take care of how the code has to implement what is required by the tests. This shift in focus allows your mind to concentrate on the what and how parts in separate moments , yielding a brain power boost that will surprise you.</p>"},{"location":"python/testing/introduction/#benifits-of-tdd","title":"Benifits of TDD","text":"<ul> <li>You will refactor with much more confidence: Tests will break if you introduce bugs. Moreover, the architectural refactor will also benefit from having tests that act as guardians.</li> <li>The code will be more readable: This is crucial in our time, when coding is a social activity and every professional developer spends much more time reading code than writing it.</li> <li>The code will be more loosely coupled and easier to test and maintain:  Writing the tests first forces you to think more deeply about code structure.</li> <li>Writing tests first requires you to have a better understanding of the business requirements: If your understanding of the requirements is lacking information, you\u2019ll find writing a test extremely challenging and this situation acts as a sentinel for you.</li> <li>Having everything unit tested means the code will be easier to debug:  Moreover, small tests are perfect for providing alternative documentation.  English can be misleading, but five lines of Python in a simple test are very hard to misunderstand.</li> <li>Higher speed: It\u2019s faster to write tests and code than it is to write the code first and then lose time debugging it. If you don\u2019t write tests, you will probably deliver the code sooner, but then you will have to track the bugs down and solve them (and, rest assured, there will be bugs). The combined time taken to write the code and then debug it is usually longer than the time taken to develop the code with TDD, where having tests running before the code is written, ensuring that the amount of bugs in it will be much lower than in the other case.</li> </ul>"},{"location":"python/testing/tox/","title":"Tox","text":""},{"location":"python/testing/tox/#introduction","title":"Introduction","text":"<p>tox is a generic virtualenv management and test command line tool you can use for:</p> <ul> <li>checking your package installs correctly with different Python versions and interpreters.</li> <li>running your tests in each of the environments, configuring your test tool of choice.</li> <li>acting as a frontend to Continuous Integration servers, greatly reducing boilerplate and merging CI and shell-based testing.</li> </ul>"},{"location":"python/testing/tox/#installation","title":"Installation","text":"<pre><code>pip install tox\n</code></pre>"},{"location":"python/testing/tox/#basic-example","title":"Basic example","text":"<p>Put basic information about your project and the test environments you want your project to run in into a <code>tox.ini</code> file residing right next to your  <code>setup.py</code> file: <pre><code># content of: tox.ini , put in same dir as setup.py\n[tox]\nenvlist = py27,py36\n\n[testenv]\n# install pytest in the virtualenv where commands will be executed\ndeps = pytest\ncommands =\n# NOTE: you can run any command line tool here - not just tests\npytest\n</code></pre></p> <p>You can also try generating a <code>tox.ini</code> file automatically, by running  <code>tox-quickstart</code> and then answering a few simple questions.</p>"},{"location":"python/testing/tox/#system-overview","title":"System overview","text":"<p>tox roughly follows the following phases:</p> <ol> <li>configuration: load <code>tox.ini</code> and merge it with options from the command line and the operating system environment variables.</li> <li>environment - for each tox environment (e.g. py27, py36) do:<ol> <li>environment creation: create a fresh environment, by default    virtualenv is used. tox will automatically try to discover a valid    Python interpreter version by using the environment name (e.g. <code>py27</code>    means Python 2.7 and the basepython configuration value) and the    current operating system <code>PATH</code> value. This is created at first run    only to be re-used at subsequent runs. If certain aspects of the    project change, a re-creation of the environment is automatically    triggered. To force the recreation tox can be invoked with     <code>-r</code>/<code>--recreate</code>.</li> <li>install (optional): install the environment dependencies specified    inside the deps configuration section, and then the earlier packaged    source distribution. By default pip is used to install packages,     however one can customise this via <code>install_command</code>. Note pip will    not update project dependencies (specified either in the    <code>install_requires</code> or the extras section of the <code>setup.py</code>) if any    version already exists in the virtual environment; therefore we    recommend to recreate your environments whenever your project    dependencies change.</li> <li>commands: run the specified commands in the specified order. Whenever    the exit code of any of them is not zero stop, and mark the    environment failed. Note, starting a command with a single dash    character means ignore exit code.</li> </ol> </li> <li>report: print out a report of outcomes for each tox environment: <pre><code>____________________ summary ____________________\npy27: commands succeeded\nERROR:   py36: commands failed\n</code></pre> Only if all environments ran successfully tox will return exit code <code>0</code> (success). In this case you\u2019ll also see the message congratulations :).</li> </ol>"},{"location":"python/testing/tox/#links","title":"Links","text":"<ul> <li>Official Docs</li> </ul>"},{"location":"python/testing/pytest/fixtures/","title":"Fixtures","text":""},{"location":"python/testing/pytest/fixtures/#introduction","title":"Introduction","text":"<p>Tests in the real world often need to create resources or data to work on:  a temporary directory to output some files to, a database connection to test the I/O layer of an application, a web server for integration testing.  Those are all examples of resources that are required in more complex testing scenarios. More complex resources often need to be cleaned up at the end of the test session: removing a temporary directory, cleaning up and disconnecting from a database, shutting down a web server. Also, these resources should be easily shared across tests, because during testing we often need to reuse a resource for different test scenarios. Some resources are costly to create, but because they are immutable or can be restored to a pristine state, they should be created only once and shared with all the tests that require it, only being destroyed when the last test that needs them finishes. </p> <p>All of the previous requirements and more are covered by one of the most important of pytest\u2019s features: fixtures.</p>"},{"location":"python/testing/pytest/fixtures/#problem","title":"Problem","text":"<p>Most tests need some kind of data or resource to operate on: <pre><code>def test_highest_rated():\n    series = [\n        (\"The Office\", 2005, 8.8),\n        (\"Scrubs\", 2001, 8.4),\n        (\"IT Crowd\", 2006, 8.5),\n        (\"Parks and Recreation\", 2009, 8.6),\n        (\"Seinfeld\", 1989, 8.9),\n    ]\n    assert highest_rated(series) == \"Seinfeld\"\n\ndef test_oldest():\n    series = [\n        (\"The Office\", 2005, 8.8),\n        ...,\n    ] # same data as above\n    assert oldest(series) == \"Seinfeld\"\n</code></pre></p> <p>Here, we have a list of (<code>series name</code>, <code>year</code>, <code>rating</code>) tuples that we use to test the <code>highest_rated</code> function. Inlining data into the test code as we do here works well for isolated tests, but often you have a dataset that can be used by multiple tests. One solution would be to copy over the dataset to each test, like it\u2019s done in the above code. But this gets old quickly\u2014plus, copying  and pasting things around will hurt maintainability in the long run, for example, if the data layout changes (adding a new item to the tuple like the cast size, for example).</p>"},{"location":"python/testing/pytest/fixtures/#solution","title":"Solution","text":"<p>Fixtures are used to provide resources that test the functions and methods we need to execute. They are created using normal Python functions and the <code>@pytest.fixture</code> decorator:</p> <pre><code>@pytest.fixture\ndef comedy_series():\n    return [\n        (\"The Office\", 2005, 8.8),\n        (\"Scrubs\", 2001, 8.4),\n        (\"IT Crowd\", 2006, 8.5),\n        (\"Parks and Recreation\", 2009, 8.6),\n        (\"Seinfeld\", 1989, 8.9),\n    ]\n</code></pre> <p>Here, we are creating a fixture named <code>comedy_series</code>, which returns the same list we were using in the previous section.</p> <p>Tests can access fixtures by declaring the fixture name in their parameter list. The test function then receives the return value of the fixture function as a parameter. Here is the <code>comedy_series</code> fixture in action:</p> <pre><code>def test_highest_rated(comedy_series):\n    assert highest_rated(comedy_series) == \"Seinfeld\"\n\ndef test_oldest(comedy_series):\n    assert oldest(comedy_series) == \"Seinfeld\"\n</code></pre> <p>Here\u2019s how things work:</p> <ul> <li>Pytest looks at the test function parameters before calling it. Here, we have one parameter: <code>comedy_series</code>.</li> <li>For each parameter, pytest gets the fixture function of same name and executes it.</li> <li>The return value of each fixture function becomes a named parameter, and the test function is called.</li> </ul> <p>Note that <code>test_highest_rated</code> and <code>test_oldest</code> each get their own copy of the comedy series list, so they don\u2019t risk interfering with each other if they change the list inside the test.</p> <p>It is also possible to create fixtures in classes using methods:</p> <pre><code>class Test:\n\n    @pytest.fixture\n    def drama_series(self):\n        return [\n            (\"The Mentalist\", 2008, 8.1),\n            (\"Game of Thrones\", 2011, 9.5),\n            (\"The Newsroom\", 2012, 8.6),\n            (\"Cosmos\", 1980, 9.3),\n        ]\n\n    ...\n\n    def test_highest_rated(self, drama_series):\n        assert highest_rated(drama_series) == \"Game of Thrones\"\n\n    def test_oldest(self, drama_series):\n        assert oldest(drama_series) == \"Cosmos\"\n</code></pre> <p>Fixtures defined in test classes are only accessible by test methods of the class or subclasses. Note that test classes might have other non-test methods, like any other class.</p>"},{"location":"python/testing/pytest/fixtures/#setupteardown","title":"Setup/teardown","text":"<p>As we\u2019ve seen in the introduction, it is very common for resources that are used in testing to require some sort of clean up after a test is done with them. One example for such would be generate comedy_series data from a csv. <pre><code>@pytest.fixture\ndef comedy_series():\n    with open(\"series.csv\", \"r\", newline=\"\") as file:\n        yield list(csv.reader(file))\n</code></pre></p> <p>By using <code>yield</code> instead of <code>return</code>, this is what happens:</p> <ul> <li>The fixture function is called</li> <li>It executes until the yield statement, where it pauses and yields the fixture value</li> <li>The test executes, receiving the fixture value as parameter</li> <li>Regardless of whether the test passes or fails, the function is resumed so it can perform its teardown actions</li> </ul> <p>The file will be closed automatically by the with statement after the test completes.</p>"},{"location":"python/testing/pytest/fixtures/#composability","title":"Composability","text":"<p>Suppose we receive a new series.csv file that now contains a much larger number of TV series, including the comedy series we had before and many other genres as well. We want to use this new data for some other tests,  but we would like to keep existing tests working as they did previously.</p> <p>Fixtures in pytest can easily depend on other fixtures just by declaring them as parameters. Using this property, we are able to create a new series fixture that reads all the data from <code>series.csv</code> (which now contains more genres), and change our <code>comedy_series</code> fixture to filter out only comedy series: <pre><code>@pytest.fixture\ndef series():\n    with open(\"series.csv\", \"r\", newline=\"\") as file:\n        return list(csv.reader(file))\n\n@pytest.fixture\ndef comedy_series(series):\n    return [x for x in series if x[GENRE] == \"comedy\"]\n</code></pre></p> <p>Note that, because of those characteristics, fixtures are a prime example of dependency injection, which is a technique where a function or an object declares its dependencies, but otherwise doesn\u2019t know or care how those dependencies will be created, or by who. This makes them extremely modular and reusable.</p>"},{"location":"python/testing/pytest/fixtures/#conftestpy","title":"<code>conftest.py</code>","text":"<p>Suppose we need to use our <code>comedy_series</code> fixture from the previous section in other test modules. In pytest, sharing fixtures is easily done by just moving the fixture code to a <code>conftest.py</code> file.</p> <p>A <code>conftest.py</code> file is a normal Python module, except that it is loaded automatically by pytest, and any fixtures defined in it are available to test modules in the same directory and below automatically. Consider this test module hierarchy: <pre><code>tests/\n +-- ratings/\n    +-- series.csv\n    +-- test_ranking.py\n +-- io/\n    +-- conftest.py\n    +-- test_formats.py\n +-- conftest.py\n</code></pre></p> <p>The <code>tests/conftest.py</code> file is at the root of the hierarchy, so any fixtures defined on it are automatically available to all other test modules in this project. Fixtures in <code>tests/io/conftest.py</code> will be available only to modules at and below <code>tests/io</code>, so only to  <code>test_formats.py</code>.</p>"},{"location":"python/testing/pytest/fixtures/#prefer-local-imports-in-conftest-files","title":"Prefer local imports in conftest files","text":"<p><code>conftest.py</code> files are imported during collection, so they directly affect your experience when running tests from the command line. For this reason,  I suggest using local imports in <code>conftest.py</code> files as often as possible, to keep import times low.</p> <p>So, don\u2019t use this: <pre><code>import pytest\nimport tempfile\nfrom myapp import setup\n\n@pytest.fixture\ndef setup_app():\n    ...\n</code></pre></p> <p>Prefer local imports: <pre><code>import pytest\n\n@pytest.fixture\ndef setup_app():\n    import tempfile\n    from myapp import setup\n    ...\n</code></pre></p> <p>This practice has a noticeable impact on test startup in large test suites.</p>"},{"location":"python/testing/pytest/fixtures/#renaming-fixtures","title":"Renaming fixtures","text":"<p>The <code>@pytest.fixture</code> decorator accepts a <code>name</code> parameter that can be used to specify a name for the fixture, different from the fixture function: <pre><code>@pytest.fixture(name=\"venv_dir\")\ndef _venv_dir():\n    ...\n</code></pre></p> <p>This is useful, because there are some annoyances that might affect users when using fixtures declared in the same module as the test functions that use them:</p> <ul> <li>If users forget to declare the fixture in the parameter list of a test function, they will get a <code>NameError</code> instead of the fixture function object (because they are in the same module).</li> <li>Some linters complain that the test function parameter is shadowing the fixture function.</li> </ul> <p>You might adopt this as a good practice in your team if the previous annoyances are frequent. Keep in mind that these problems only happen with fixtures defined in test modules, not in <code>conftest.py</code> files.</p>"},{"location":"python/testing/pytest/fixtures/#scope","title":"Scope","text":"<p>The scope of a fixture defines when the fixture should be cleaned up. While the fixture is not cleaned up, tests requesting the fixture will receive the same fixture value.</p> <p>The <code>scope</code> parameter of the <code>@pytest.fixture</code> decorator is used to set the fixture\u2019s scope: <pre><code>@pytest.fixture(scope=\"session\")\ndef db_connection():\n    ...\n</code></pre></p> <p>The following scopes are available:</p> <ul> <li><code>scope=\"session\"</code>: fixture is teardown when all tests finish.</li> <li><code>scope=\"module\"</code>: fixture is teardown when the last test function of a module finishes.</li> <li><code>scope=\"class\"</code>: fixture is teardown when the last test method of a class finishes.</li> <li><code>scope=\"function\"</code>: fixture is teardown when the test function requesting it finishes. This is the default.</li> </ul> <p>It is important to emphasize that, regardless of scope, each fixture will be created only when a test function requires it. For example, session-scoped  fixtures are not necessarily created at the start of the session, but only when the first test that requests it is about to be called. This makes sense when you consider that not all tests might need a session-scoped fixture, and there are various forms to run only a subset of tests, as we have seen in previous chapters.</p>"},{"location":"python/testing/pytest/fixtures/#autouse","title":"Autouse","text":"<p>It is possible to apply a fixture to all of the tests in a hierarchy, even if the tests don\u2019t explicitly request a fixture, by passing <code>autouse=True</code> to the <code>@pytest.fixture</code> decorator. This is useful when we need to apply a  side-effect before and/or after each test unconditionally. <pre><code>@pytest.fixture(autouse=True)\ndef setup_dev_environment():\n    previous = os.environ.get('APP_ENV', '')\n    os.environ['APP_ENV'] = 'TESTING'\n    yield\n    os.environ['APP_ENV'] = previous\n</code></pre></p> <p>If a test can access an autouse fixture by declaring it in the parameter list, the autouse fixture will be automatically used by that test. Note that it is possible for a test function to add the autouse fixture to its parameter list if it is interested in the return value of the fixture, as normal.</p>"},{"location":"python/testing/pytest/fixtures/#pytestmarkusefixtures","title":"<code>@pytest.mark.usefixtures</code>","text":"<p>The <code>@pytest.mark.usefixtures</code> mark can be used to apply one or more fixtures to tests, as if they have the fixture name declared in their parameter list.  This can be an alternative in situations where you want all tests in a group to always use a fixture that is not <code>autouse</code>.</p> <p>For example, the code below will ensure all tests methods in the <code>TestVirtualEnv</code> class execute in a brand new virtual environment: <pre><code>@pytest.fixture\ndef venv_dir():\n    import venv\n\n    with tempfile.TemporaryDirectory() as d:\n        venv.create(d)\n        pwd = os.getcwd()\n        os.chdir(d)\n        yield d\n        os.chdir(pwd)\n\n@pytest.mark.usefixtures('venv_dir')\nclass TestVirtualEnv:\n    ...\n</code></pre></p> <p>As the name indicates, you can pass multiple fixtures names to the decorator:  <pre><code>@pytest.mark.usefixtures(\"venv_dir\", \"config_python_debug\")\nclass Test:\n    ...\n</code></pre></p>"},{"location":"python/testing/pytest/fixtures/#parametrizing-fixtures","title":"Parametrizing fixtures","text":"<p>Fixtures can also be parametrized directly. When a fixture is parametrized,  all tests that use the fixture will now run multiple times, once for each parameter. This is an excellent tool to use when we have variants of a fixture and each test that uses the fixture should also run with all variants.</p> <p>Consider the following example: <pre><code>@pytest.mark.parametrize(\n    \"serializer_class\",\n    [JSONSerializer, XMLSerializer, YAMLSerializer],\n)\nclass Test:\n\n    def test_quantity(self, serializer_class):\n        serializer = serializer_class()\n        quantity = Quantity(10, \"m\")\n        data = serializer.serialize_quantity(quantity)\n        new_quantity = serializer.deserialize_quantity(data)\n        assert new_quantity == quantity\n\n    def test_pipe(self, serializer_class):\n        serializer = serializer_class()\n        pipe = Pipe(\n            length=Quantity(1000, \"m\"), diameter=Quantity(35, \"cm\")\n        )\n       data = serializer.serialize_pipe(pipe)\n       new_pipe = serializer.deserialize_pipe(data)\n       assert new_pipe == pipe\n</code></pre></p> <p>We can update the example to parametrize on a fixture instead: <pre><code>class Test:\n\n    @pytest.fixture(params=[JSONSerializer, XMLSerializer,\n                            YAMLSerializer])\n    def serializer(self, request):\n        return request.param()\n\n    def test_quantity(self, serializer):\n        quantity = Quantity(10, \"m\")\n        data = serializer.serialize_quantity(quantity)\n        new_quantity = serializer.deserialize_quantity(data)\n        assert new_quantity == quantity\n\n    def test_pipe(self, serializer):\n        pipe = Pipe(\n            length=Quantity(1000, \"m\"), diameter=Quantity(35, \"cm\")\n        )\n        data = serializer.serialize_pipe(pipe)\n        new_pipe = serializer.deserialize_pipe(data)\n        assert new_pipe == pipe\n</code></pre></p> <p>Note the following:</p> <ul> <li>We pass a <code>params</code> parameter to the fixture definition.</li> <li>We access the parameter inside the fixture, using the <code>param</code> attribute of the special <code>request</code> object. This built-in fixture provides access to the requesting test function and the parameter when the fixture is parametrized. </li> <li>In this case, we instantiate the serializer inside the fixture, instead of explicitly in each test.</li> </ul> <p>As can be seen, parametrizing a fixture is very similar to parametrizing a test, but there is one key difference: by parametrizing a fixture we make all tests that use that fixture run against all the parametrized instances,  making them an excellent solution for fixtures shared in <code>conftest.py</code> files.</p>"},{"location":"python/testing/pytest/fixtures/#using-marks-from-fixtures","title":"Using marks from fixtures","text":"<p>We can use the <code>request</code> fixture to access marks that are applied to test functions.</p> <p>Suppose we have an <code>autouse</code> fixture that always initializes the current locale to English: <pre><code>@pytest.fixture(autouse=True)\ndef setup_locale():\n    locale.setlocale(locale.LC_ALL, \"en_US\")\n    yield\n    locale.setlocale(locale.LC_ALL, None)\n\ndef test_currency_us():\n    assert locale.currency(10.5) == \"$10.50\"\n</code></pre></p> <p>But what if we want to use a different locale for just a few tests?</p> <p>One way to do that is to use a custom mark, and access the <code>mark</code> object from within our fixture: <pre><code>@pytest.fixture(autouse=True)\ndef setup_locale(request):\n    mark = request.node.get_closest_marker(\"change_locale\")\n    loc = mark.args[0] if mark is not None else \"en_US\"\n    locale.setlocale(locale.LC_ALL, loc)\n    yield\n    locale.setlocale(locale.LC_ALL, None)\n\n@pytest.mark.change_locale(\"pt_BR\")\ndef test_currency_br():\n    assert locale.currency(10.5) == \"R$ 10,50\"\n</code></pre></p> <p>Marks can be used that way to pass information to fixtures. Because it is somewhat implicit though, I recommend using it sparingly, because it might lead to hard-to-understand code.</p>"},{"location":"python/testing/pytest/fixtures/#built-in-fixtures","title":"Built-in fixtures","text":""},{"location":"python/testing/pytest/fixtures/#tmpdir","title":"<code>tmpdir</code>","text":"<p>The <code>tmpdir</code> function-scoped fixture provides an empty directory that is removed automatically at the end of each test. <pre><code>def test_empty(tmpdir):\n    assert os.path.isdir(tmpdir)\n    assert os.listdir(tmpdir) == []\n</code></pre></p> <p>The fixture provides a <code>py.local</code>object, from the <code>py</code> library, which provides convenient methods to deal with file paths, such as joining, reading, writing, getting the extension,  and so on; it is similar in philosophy to the  <code>pathlib.Path</code> object from the standard library. <pre><code>def test_save_curves(tmpdir):\n    data = dict(status_code=200, values=[225, 300])\n    fn = tmpdir.join('somefile.json')\n    write_json(fn, data)\n    assert fn.read() == '{\"status_code\": 200, \"values\": [225, 300]}'\n</code></pre></p> <p>Note</p> <p>Why pytest use <code>py.local</code> instead of <code>pathlib.Path</code>? Pytest had been around for years before <code>pathlib.Path</code> came along and was incorporated into the standard library, and the py library  was one the best solutions for  path-like objects at the time. Core pytest developers are looking into how to adapt pytest to the now-standard <code>pathlib.Path</code> API.</p>"},{"location":"python/testing/pytest/fixtures/#tmpdir_factory","title":"<code>tmpdir_factory</code>","text":"<p>The <code>tmpdir</code> fixture is very handy, but it is only function-scoped: this has the downside that it can only be used by other function-scoped fixtures.</p> <p>The <code>tmpdir_factory</code> fixture is a session-scoped fixture that allows creating empty and unique directories at any scope. This can be useful when we need to store data on to a disk in fixtures of other scopes, for example a session-scoped cache or a database file. <pre><code>@pytest.fixture(scope='session')\ndef images_dir(tmpdir_factory):\n    directory = tmpdir_factory.mktemp('images')\n    download_images('https://example.com/samples.zip', directory)\n    extract_images(directory / 'samples.zip')\n    return directory\n</code></pre></p> <p>Keep in mind however that a directory created by this fixture is shared and will only be deleted at the end of the test session. This means that tests should not modify the contents of the directory; otherwise, they risk affecting other tests.</p>"},{"location":"python/testing/pytest/fixtures/#monkeypatch","title":"<code>monkeypatch</code>","text":"<p>In some situations, tests need features that are complex or hard to set up in a testing environment, for example:</p> <ul> <li>Clients to an external resource (for example GitHub\u2019s API), where access during testing might be impractical or too expensive</li> <li>Forcing a piece of code to behave as if on another platform, such as error handling</li> <li>Complex conditions or environments that are hard to reproduce locally or in the CI</li> </ul> <p>The <code>monkeypatch</code> fixture allows you to cleanly overwrite functions,  objects, and dictionary entries of the system being tested with other objects and functions, undoing all changes during test teardown. For example: <pre><code># login.py\nimport getpass\n\ndef user_login(name):\n    password = getpass.getpass()\n    check_credentials(name, password)\n    ...\n</code></pre></p> <p>In this code, user_login uses the <code>getpass.getpass()</code> function from the standard library to prompt for the user\u2019s password in the most secure manner available in the system. It is hard to simulate the actual entering of the password during testing because getpass tries to read directly from the terminal (as opposed to from <code>sys.stdin</code>) when possible.</p> <p>We can use the monkeypatch fixture to bypass the call to getpass in the tests, transparently and without changing the application code: <pre><code>def test_login_success(monkeypatch):\n    monkeypatch.setattr(getpass, \"getpass\", lambda: \"valid-pass\")\n    assert user_login(\"test-user\")\n\ndef test_login_wrong_password(monkeypatch):\n    monkeypatch.setattr(getpass, \"getpass\", lambda: \"wrong-pass\")\n    with pytest.raises(AuthenticationError, match=\"wrong password\"):\n        user_login(\"test-user\")\n</code></pre></p> <p>In the tests, we use monkeypatch.setattr to replace the real <code>getpass()</code>  function of the <code>getpass</code> module with a dummy <code>lambda</code>, which returns a  hard-coded password. In <code>test_login_success</code>, we return a known, good password to ensure the user can authenticate successfully, while in <code>test_login_wrong_password</code>, we use a bad password to ensure the authentication error is handled correctly. As mentioned before, the original <code>getpass()</code> function is restored automatically at the end of the test, ensuring we don\u2019t leak that change to other tests in the system.</p> <p>The monkeypatch fixture works by replacing an attribute of an object by another object (often called a mock), restoring the original object at the end of the test. A common problem when using this fixture is patching the wrong object, which causes the original function/object to be called instead of the mock one. If the above example would have been: <pre><code># login.py\nfrom getpass import getpass\n\ndef user_login(name):\n    password = getpass()\n    check_credentials(name, password)\n    ...\n</code></pre> our test cases would not work. As now we\u2019ve to mock the <code>getpass</code> attribute of <code>login.py</code> namespace instead of <code>getpass</code> namespace</p> <pre><code>import login\n\ndef test_login_success(monkeypatch):\n    monkeypatch.setattr(login, \"getpass\", lambda: \"valid-pass\")\n    assert user_login(\"test-user\")\n\ndef test_login_wrong_password(monkeypatch):\n    monkeypatch.setattr(login, \"getpass\", lambda: \"wrong-pass\")\n    with pytest.raises(AuthenticationError, match=\"wrong password\"):\n        user_login(\"test-user\")\n</code></pre> <p>How the code being tested imports code that needs to be monkeypatched is the reason why people are tripped by this so often, so make sure you take a look at the code first.</p>"},{"location":"python/testing/pytest/fixtures/#capsys","title":"<code>capsys</code>","text":"<p>The <code>capsys</code> fixture captures all text written to <code>sys.stdout</code> and  <code>sys.stderr</code> and makes it available during testing. The name is short for capture system streams.</p> <p>Suppose we have a small command-line script and want to check the usage instructions are correct when the script is invoked without arguments: <pre><code>from textwrap import dedent\n\ndef script_main(args):\n    if not args:\n        show_usage()\n        return 0\n    ...\n\ndef show_usage():\n    print(\"Create/update webhooks.\")\n    print(\" Usage: hooks REPO URL\")\n</code></pre> During testing, we can access the captured output, using the capsys fixture.  This fixture has a <code>capsys.readouterr()</code> method that returns a namedtuple with <code>out</code> and <code>err</code> attributes, containing the captured text from  <code>sys.stdout</code> and <code>sys.stderr</code> respectively: <pre><code>def test_usage(capsys):\n    script_main([])\n    captured = capsys.readouterr()\n    assert captured.out == dedent(\"\"\"\\\n        Create/update webhooks.\n          Usage: hooks REPO URL\n    \"\"\")\n</code></pre></p>"},{"location":"python/testing/pytest/fixtures/#capfd","title":"<code>capfd</code>","text":"<p>There\u2019s also the <code>capfd</code> fixture that works similarly to <code>capsys</code>, except that it also captures the output of file descriptors <code>1</code> and <code>2</code>. This makes it possible to capture the standard output and standard errors, even for extension modules. The name is short for capture file desciptors.</p>"},{"location":"python/testing/pytest/fixtures/#capsysbinarycapfdbinary","title":"<code>capsysbinary</code>/<code>capfdbinary</code>","text":"<p><code>capsysbinary</code> and <code>capfdbinary</code> are fixtures identical to <code>capsys</code> and <code>capfd</code>, except that they capture output in binary mode, and their <code>readouterr()</code> methods return raw bytes instead of text. It might be useful in specialized situations, for example, when running an external process that produces binary output, such as <code>tar</code>.</p>"},{"location":"python/testing/pytest/fixtures/#request","title":"<code>request</code>","text":"<p>The <code>request</code> fixture is an internal pytest fixture that provides useful information about the requesting test. It can be declared in test functions and fixtures, and provides attributes such as the following:</p> <ul> <li><code>function</code>: the Python test function object, available for  function-scoped fixtures.</li> <li><code>cls</code>/<code>instance</code>: the Python class/instance of a test method object,  available for function- and class-scoped fixtures. It can be <code>None</code> if the fixture is being requested from a test function, as opposed to a test method.</li> <li><code>module</code>: the Python module object of the requesting test method, available for module-, function-, and class-scoped fixtures.</li> <li><code>session</code>: pytest\u2019s internal Session object, which is a singleton for the test session and represents the root of the collection tree. It is available to fixtures of all scopes.</li> <li><code>node</code>: the pytest collection node, which wraps one of the Python objects discussed that matches the fixture scope.</li> <li><code>addfinalizer(func)</code>: adds a new <code>finalizer</code> function that will be called at the end of the test. The <code>finalizer</code> function is called without arguments.  <code>addfinalizer</code> was the original way to execute teardown in fixtures, but has since then been superseded by the <code>yield</code> statement, remaining in use mostly for backward compatibility.</li> </ul> <p>Fixtures can use those attributes to customize their own behavior based on the test being executed. For example, we can create a fixture that provides a temporary directory using the current test name as the prefix of the temporary directory, somewhat similar to the built-in <code>tmpdir</code> fixture: <pre><code>@pytest.fixture\ndef tmp_path(request) -&gt; Path:\n    with TemporaryDirectory(prefix=request.node.name) as d:\n        yield Path(d)\n\ndef test_tmp_path(tmp_path):\n    assert list(tmp_path.iterdir()) == []\n</code></pre></p> <p>The <code>request</code> fixture can be used whenever you want to customize a fixture based on the attributes of the test being executed, or to access the marks applied to the test function, as we have seen in the previous sections.</p>"},{"location":"python/testing/pytest/markers/","title":"Markers","text":""},{"location":"python/testing/pytest/markers/#introduction","title":"Introduction","text":"<p>Pytest allows you to mark functions and classes with metadata. This metadata can be used to selectively run tests, and is also available for fixtures and plugins, to perform different tasks. </p>"},{"location":"python/testing/pytest/markers/#creating-marks","title":"Creating marks","text":"<p>Marks are created with the <code>@pytest.mark</code> decorator. It works as a factory,  so any access to it will automatically create a new mark and apply it to a function.</p> <p>Example: <pre><code>@pytest.mark.slow\ndef test_long_computation():\n    ...\n</code></pre></p> <p>By using the <code>@pytest.mark.slow</code> decorator, you are applying a mark named <code>slow</code> to <code>test_long_computation</code>.</p> <p>Marks can also receive parameters: <pre><code>@pytest.mark.timeout(10, method=\"thread\")\ndef test_topology_sort():\n    ...\n</code></pre> With this, we define that <code>test_topology_sort</code> should not take more than 10 seconds, in which case it should be terminated using the thread method.</p> <p>You can add more than one mark to a test by applying the @pytest.mark decorator multiple times:</p> <pre><code>@pytest.mark.slow\n@pytest.mark.timeout(10, method=\"thread\")\ndef test_topology_sort():\n    ...\n</code></pre>"},{"location":"python/testing/pytest/markers/#running-tests-based-on-marks","title":"Running tests based on marks","text":"<p>Run tests by using marks as selection factors with the -m flag. For example,  to run all tests with the slow mark: <pre><code>pytest -m slow\n</code></pre></p> <p>The <code>-m</code> flag also accepts expressions, so you can do a more advanced selection. To run all tests with the slow mark but not the tests with the serial mark you can use:</p> <pre><code>pytest -m \"slow and not serial\"\n</code></pre> <p>The expression is limited to the <code>and</code>, <code>not</code>, and <code>or</code> operators.</p> <p>Custom marks can be useful for optimizing test runs on your CI system.  Oftentimes, environment problems, missing dependencies, or even some code committed by mistake might cause the entire test suite to fail. By using marks, you can choose a few tests that are fast and/or broad enough to detect problems in a good portion of the code and then run those first,  before all the other tests. If any of those tests fail, we abort the job and avoid wasting potentially a lot of time by running all tests that are doomed to fail anyway.</p> <p>We start by applying a custom mark to those tests. Any name will do, but a common name used is <code>smoke</code>, as in smoke detector, to detect problems before everything bursts into flames.</p> <p>You then run smoke tests first, and only after they pass do, you run the complete test suite:</p> <pre><code>pytest -m \"smoke\"\n...\npytest -m \"not smoke\"\n</code></pre>"},{"location":"python/testing/pytest/markers/#applying-marks-to-classes","title":"Applying marks to classes","text":"<p>You can apply the <code>@pytest.mark</code> decorator to a class. This will apply that same mark to all tests methods in that class, avoiding have to copy and paste the mark code over all test methods:</p> <pre><code>@pytest.mark.timeout(10)\nclass TestCore:\n\n    def test_simple_simulation(self):\n        ...\n\n    def test_compute_tracers(self):\n        ...\n</code></pre> <p>However, there is one difference: applying the <code>@pytest.mark</code> decorator to a class means that all its subclasses will inherit the mark. Subclassing test classes is not common, but it is sometimes a useful technique to avoid repeating test code, or to ensure that implementations comply with a certain interface</p>"},{"location":"python/testing/pytest/markers/#applying-marks-to-modules","title":"Applying marks to modules","text":"<p>We can also apply a mark to all test functions and test classes in a module.  Just declare a global variable named <code>pytestmark</code>:</p> <pre><code>import pytest\n\npytestmark = pytest.mark.timeout(10)\n\n\nclass TestCore:\n\n    def test_simple_simulation(self):\n        ...\n\n\ndef test_compute_tracers():\n    ...\n</code></pre> <p>You can use a tuple or list of marks to apply multiple marks as well:</p> <pre><code>import pytest\n\npytestmark = [pytest.mark.slow, pytest.mark.timeout(10)]\n</code></pre>"},{"location":"python/testing/pytest/markers/#custom-marks-and-pytestini","title":"Custom marks and <code>pytest.ini</code>","text":"<p>Being able to declare new marks on the fly just by applying the <code>pytest.mark</code> decorator is convenient. It makes it a breeze to quickly start enjoying the benefits of using marks.</p> <p>This convenience comes at a price: it is possible for a user to make a typo in the mark name, for example <code>@pytest.mark.smoek</code>, instead of  <code>@pytest.mark.smoke</code>. Depending on the project under testing, this typo might be a mere annoyance or a more serious problem.</p> <p>Mature test suites that have a fixed set of marks might declare them in the <code>pytest.ini</code> file:</p> <pre><code>[pytest]\nmarkers =\nslow\nserial\nsmoke: quick tests that cover a good portion of the code\nunittest: unit tests for basic functionality\nintegration: cover to cover functionality testing    </code></pre> <p>The markers option accepts a list of markers in the form of  <code>&lt;name&gt;: description</code>, with the description part being optional (<code>slow</code> and <code>serial</code> in the above example don\u2019t have a description).</p> <p>A full list of marks can be displayed by using the <code>--markers</code> flag:</p> <pre><code>\u03bb pytest --markers\n@pytest.mark.slow:\n\n@pytest.mark.serial:\n\n@pytest.mark.smoke: quick tests that cover a good portion of the code\n\n@pytest.mark.unittest: unit tests for basic functionality\n\n@pytest.mark.integration: cover to cover functionality testing\n\n...\n</code></pre> <p>The <code>--strict</code> flag makes it an error to use marks not declared in the <code>pytest.ini</code> file. Using our previous example with a typo, we now obtain an error, instead of pytest silently creating the mark when running with <code>--strict</code>:</p> <pre><code>pytest --strict tests\\test_wrong_mark.py\n...\ncollected 0 items / 1 errors\n\n============================== ERRORS ===============================\n_____________ ERROR collecting tests/test_wrong_mark.py _____________\ntests\\test_wrong_mark.py:4: in &lt;module&gt;\n    @pytest.mark.smoek\n..\\..\\.env36\\lib\\site-packages\\_pytest\\mark\\structures.py:311: in __getattr__\n    self._check(name)\n..\\..\\.env36\\lib\\site-packages\\_pytest\\mark\\structures.py:327: in _check\n    raise AttributeError(\"%r not a registered marker\" % (name,))\nE AttributeError: 'smoek' not a registered marker\n!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!\n====================== 1 error in 0.09 seconds ======================\n</code></pre> <p>Test suites that want to ensure that all marks are registered in pytest.ini should also use addopts:</p> <pre><code>[pytest]\naddopts = --strict\nmarkers =\nslow\nserial\nsmoke: quick tests that cover a good portion of the code\nunittest: unit tests for basic functionality\nintegration: cover to cover functionality testing\n</code></pre>"},{"location":"python/testing/pytest/markers/#built-in-marks","title":"Built-in marks","text":""},{"location":"python/testing/pytest/markers/#pytestmarkskipif","title":"<code>@pytest.mark.skipif</code>","text":"<p>You might have tests that should not be executed unless some condition is met. For example, some tests might depend on certain libraries that are not always installed, or a local database that might not be online, or are executed only on certain platforms. </p> <p>Pytest provides a built-in mark, <code>skipif</code>, that can be used to skip tests based on specific conditions. Skipped tests are not executed if the condition is true, and are not counted towards test suite failures.</p> <p>Example: you can use the <code>skipif</code> mark to always skip a test when executing on Windows:</p> <pre><code>import sys\nimport pytest\n\n@pytest.mark.skipif(\n    sys.platform.startswith(\"win\"),\n    reason=\"fork not available on Windows\",\n)\ndef test_spawn_server_using_fork():\n    ...\n</code></pre> <p>The first argument to <code>@pytest.mark.skipif</code> is the condition: in this example,  we are telling pytest to skip this test in Windows. The <code>reason=</code> keyword argument is mandatory and is used to display why the test was skipped when using the <code>-ra</code> flag:</p> <pre><code> tests\\test_skipif.py s                                        [100%]\n====================== short test summary info ======================\nSKIP [1] tests\\test_skipif.py:6: fork not available on Windows\n===================== 1 skipped in 0.02 seconds =====================\n</code></pre> <p>Checking capabilities and features is usually a better approach, instead of checking platforms and library version numbers.</p>"},{"location":"python/testing/pytest/markers/#pytestskip","title":"<code>pytest.skip</code>","text":"<p>The <code>@pytest.mark.skipif</code> decorator is very handy, but the mark must evaluate the condition at <code>import</code>/<code>collection</code> time, to determine whether the test should be skipped.</p> <p>Sometimes, it might even be almost impossible (without some gruesome hack)  to check whether a test should be skipped during import time. For example,  you can make the decision to skip a test based on the capabilities of the graphics driver only after initializing the underlying graphics library,  and initializing the graphics subsystem is definitely not something you want to do at import time.</p> <p>For those cases, pytest lets you skip tests imperatively in the test body by using the <code>pytest.skip</code> function</p> <pre><code>def test_shaders():\n    initialize_graphics()\n    if not supports_shaders():\n        pytest.skip(\"shades not supported in this driver\")\n# rest of the test code\n    ...\n</code></pre> <p><code>pytest.skip</code> works by raising an internal exception, so it follows normal Python exception semantics, and nothing else needs to be done for the test to be skipped properly.</p>"},{"location":"python/testing/pytest/markers/#pytestimportorskip","title":"<code>pytest.importorskip</code>","text":"<p>It is common for libraries to have tests that depend on a certain library being installed. For example, pytest\u2019s own test suite has some tests for <code>numpy</code> arrays, which should be skipped if <code>numpy</code> is not installed.</p> <pre><code>def test_tracers_as_arrays():\n    numpy = pytest.importorskip(\"numpy\")\n    ...\n</code></pre> <p><code>pytest.importorskip</code> will import the module and return the module object, or skip the test entirely if the module could not be imported.</p> <p>If your test requires a minimum version of the library, <code>pytest.importorskip</code> also supports a <code>minversion</code> argument:</p> <pre><code>def test_tracers_as_arrays_114():\n    numpy = pytest.importorskip(\"numpy\", minversion=\"1.14\")\n    ...\n</code></pre>"},{"location":"python/testing/pytest/markers/#pytestmarkxfail","title":"<code>@pytest.mark.xfail</code>","text":"<p>You can use <code>@pytest.mark.xfail</code> decorator to indicate that a test is expected to fail.</p> <pre><code>@pytest.mark.xfail\ndef test_simulation_34():\n    ...\n</code></pre> <p>This mark supports some parameters, all of which we will see later in this section; but one in particular warrants discussion now: the <code>strict</code> parameter. This parameter defines two distinct behaviors for the mark:</p> <ul> <li><code>strict=False</code>(the default), the test will be counted separately as an <code>XPASS</code>(if it passes) or an <code>XFAIL</code>(if it fails), and willnot fail the test suite</li> <li><code>strict=True</code>, the test will be marked as <code>XFAIL</code> if it fails, but if it unexpectedly passes, it will fail the test suite, as a normal failing test would.</li> </ul> <p>There are a few situations where this comes in handy.</p> <p>The first situation is when a test always fails, and you want to be told (loudly) if it suddenly starts passing. This can happen when:</p> <ul> <li> <p>You found out that the cause of a bug in your code is due to a problem in a third-party library. In this situation, you can write a failing test that demonstrates the problem, and mark it with <code>@pytest.mark.xfail(strict=True)</code>.  If the test fails, the test will be marked as <code>XFAIL</code> in the test session summary, but if the test passes, it will fail the test suite. This test might start to pass when you upgrade the library that was causing the problem, and this will alert you that the issue has been fixed and needs your attention.</p> </li> <li> <p>You have thought about a new feature, and design one or more test cases that exercise it even before your start implementing it. You can commit the tests with the <code>@pytest.mark.xfail(strict=True)</code> mark applied, and remove the mark from the tests as you code the new feature. This is very useful in a collaborative environment, where one person supplies tests on how they envision a new feature/API, and another person implements it based on the test cases.</p> </li> <li> <p>You discover a bug in your application and write a test case demonstrating the problem. You might not have the time to tackle it right now or another person would be better suited to work in that part of the code. In this situation, marking the test as <code>@pytest.mark.xfail(strict=True)</code> would be a good approach.</p> </li> </ul> <p>The other situation where the xfail mark is useful is when you have tests that fail sometimes, also called flakytests. Flaky tests are tests that fail on occasion, even if the underlying code has not changed. There are many reasons why tests fail in a way that appears to be random; the following are a few:</p> <ul> <li>Timing issues in multi threaded code</li> <li>Intermittent network connectivity problems</li> <li>Tests that don\u2019t deal with asynchronous events correctly</li> <li>Relying on non-deterministic behavior</li> </ul> <p>Flaky tests are a serious problem, because the test suite is supposed to be an indicator that the code is working as intended and that it can detect real problems when they happen. Flaky tests destroy that image, because often developers will see flaky tests failing that don\u2019t have anything to do with recent code changes. When this becomes commonplace, people begin to just run the test suite again in the hope that this time the flaky test passes (and it often does), but this erodes the trust in the test suite as a whole, and brings frustration to the development team. You should treat flaky tests as a menace that should be contained and dealt with.</p> <p>Here are some suggestions regarding how to deal with flaky tests within a development team:</p> <ul> <li>First, you need to be able to correctly identify flaky tests. If a test fails that apparently doesn\u2019t have anything to do with the recent changes,  run the tests again. If the test that failed previously now passes, it means the test is flaky.</li> <li>Create an issue to deal with that particular flaky test in your ticket system. Use a naming convention or other means to label that issue as related to a flaky test (for example GitHub or JIRA labels).</li> <li>Apply the <code>@pytest.mark.xfail(reason=\"flaky test #123\", strict=False)</code> mark making sure to include the issue ticket number or identifier. Feel free to add more information to the description, if you like.</li> <li>Make sure to periodically assign issues about flaky tests to yourself or other team members (for example, during sprint planning). The idea is to take care of flaky tests at a comfortable pace, eventually reducing or eliminating them altogether.</li> </ul> <p><code>@pytest.mark.xfail</code> full signature: <pre><code>@pytest.mark.xfail(condition=None, *, reason=None, raises=None, run=True, strict=False)\n</code></pre></p> <ul> <li> <p><code>condition</code>: the first parameter, if given, is a True/False condition,  similar to the one used by <code>@pytest.mark.skipif</code>: if <code>False</code>, the <code>xfail</code> mark is ignored. It is useful to mark a test as <code>xfail</code> based on an external condition, such as the platform, Python version, library version, and so on.  <pre><code>@pytest.mark.xfail(\n    sys.platform.startswith(\"win\"),\n    reason=\"flaky on Windows #42\", strict=False\n)\ndef test_login_dialog():\n    ...\n</code></pre></p> </li> <li> <p><code>reason</code>: a string that will be shown in the short test summary when the <code>-ra</code> flag is used. It is highly recommended to always use this parameter to explain the reason why the test has been marked as <code>xfail</code> and/or include a ticket number. <pre><code>@pytest.mark.xfail(\n    sys.platform.startswith(\"win\"),\n    reason=\"flaky on Windows #42\", strict=False\n)\ndef test_login_dialog():\n    ...\n</code></pre></p> </li> <li> <p><code>raises</code>: given an exception type, it declares that we expect the test to raise an instance of that exception. If the test raises another type of exception (even <code>AssertionError</code>), the test will fail normally. It is especially useful for missing functionality or to test for known bugs. <pre><code>@pytest.mark.xfail(raises=NotImplementedError,\n                   reason='will be implemented in #987')\ndef test_credential_check():\n    check_credentials('Hawkwood') # not implemented yet\n</code></pre></p> </li> <li> <p><code>run</code>: if <code>False</code>, the test will not even be executed and will fail as  <code>XFAIL</code>. This is particularly useful for tests that run code that might crash the test-suite process (for example, C/C++ extensions causing a segmentation fault due to a known problem).  <pre><code>@pytest.mark.xfail(\n    run=False, reason=\"undefined particles cause a crash #625\"\n)\ndef test_undefined_particle_collision_crash():\n    collide(Particle(), Particle())\n</code></pre></p> </li> <li> <p><code>strict</code>: if <code>True</code>, a passing test will fail the test suite. If <code>False</code>,  the test will not fail the test suite regardless of the outcome (the default is <code>False</code>).</p> </li> </ul> <p>The configuration variable <code>xfail_strict</code> controls the default value of the <code>strict</code> parameter of <code>xfail</code> marks:</p> <pre><code>[pytest]\nxfail_strict = True\n</code></pre> <p>Setting it to <code>True</code> means that all <code>xfail</code>-marked tests without an explicit strict parameter are considered an actual failure expectation instead of a flaky test. Any <code>xfail</code> mark that explicitly passes the <code>strict</code> parameter overrides the configuration value.</p> <p>Finally, you can imperatively trigger an <code>XFAIL</code> result within a test by calling the <code>pytest.xfail</code> function:</p> <pre><code>def test_particle_splitting():\n    initialize_physics()\n    import numpy\n    if numpy.__version__ &lt; \"1.13\":\n        pytest.xfail(\"split computation fails with numpy &lt; 1.13\")\n    ...\n</code></pre> <p>Similar to <code>pytest.skip</code>, this is useful when you can only determine whether you need to mark a test as xfail at runtime.</p>"},{"location":"python/testing/pytest/parametrization/","title":"Parametrization","text":""},{"location":"python/testing/pytest/parametrization/#problem","title":"Problem","text":"<p>A common testing activity is passing multiple values to the same test function and asserting the outcome.</p> <p>Consider the following test: <pre><code>def test_sum():\n    assert sum((1, 2, 3,)) == 6\n</code></pre></p> <p>Now we want to provide multiple test cases for the same. One approach can be:</p> <pre><code>def test_sum():\n    test_cases = (\n        ((1, 2, 3,), 6),\n        ((1, 2, 3, 4,), 11),\n        ((1, 2, 5, 4,), 12),\n    )\n    for values, sum_value in test_cases:\n        assert sum(values) == sum_value\n</code></pre> <p>But in this case when the second test case fails the <code>test_sum</code> would be marked as failed, and the remaining test cases won\u2019t in the test won\u2019t be executed. So, we make the same test body for each test case but that would bring a lot of duplicate code.</p>"},{"location":"python/testing/pytest/parametrization/#solution","title":"Solution","text":"<p>To solve all the above problems, pytest provides the much-loved  <code>@pytest.mark.parametrize</code> mark. With this mark, you are able to provide a list of input values to the test, and pytest automatically generates multiple test functions for each input value.</p> <pre><code>@pytest.mark.parametrize(\n    \"values, sum_value\",\n    (((1, 2, 3,), 6),\n     ((1, 2, 3, 4,), 10),\n     ((1, 2, 5, 4,), 12),)\n)\ndef test_sum(values, sum_value):\n    print(values)\n    print(sum_value)\n    assert sum(values) == sum_value\n</code></pre> <p>The <code>@pytest.mark.parametrize</code> mark automatically generates multiple test functions, parametrizing them with the arguments given to the mark. The call receives two parameters:</p> <ul> <li><code>argnames</code>: a comma-separated string of argument names that will be passed to the test function.</li> <li><code>argvalues</code>: a sequence of tuples, with each tuple generating a new test invocation. Each item in the tuple corresponds to an argument name, so the first tuple ((1, 2, 3,), 6) will generate a call to the test function with the arguments:<ul> <li><code>values</code> = (1, 2, 3, )</li> <li><code>sum_value</code> = 6</li> </ul> </li> </ul> <p>Using this mark, pytest will run test_formula_parsing three times, passing one set of arguments given by the argvalues parameter each time. It will also automatically generate a different node ID for each test, making it easy to distinguish between them:</p> <pre><code>======================= test session starts ========================\nplatform linux -- Python 3.7.7, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 \\\n -- /venv/bin/python3.7\ncachedir: .pytest_cache\nrootdir: /home/mickey\ncollected 3 items                                                  \n\n../../test_.py::test_sum[values0-6] PASSED [ 33%]\n../../test_.py::test_sum[values1-10] PASSED [ 66%]\n../../test_.py::test_sum[values2-12] PASSED [100%]\n\n======================== 3 passed in 0.01s =========================\n</code></pre>"},{"location":"python/testing/pytest/parametrization/#applying-marks-to-value-sets","title":"Applying marks to value sets","text":"<p>Often, in parametrized tests, you find the need to apply one or more marks to a set of parameters as you would to a normal test function. For example,  you want to apply a <code>timeout</code> mark to one set of parameters because it takes too long to run, or <code>xfail</code> a set of parameters, because it has not been implemented yet.</p> <p>In those cases, use <code>pytest.param</code> to wrap the set of values and apply the marks you want:</p> <pre><code>@pytest.mark.parametrize(\n    \"formula, inputs, result\",\n    [\n        ...\n        (\"log(x) + 3\", dict(x=2.71828182846), 4.0),\npytest.param(\n            \"hypot(x, y)\", dict(x=3, y=4), 5.0,\n            marks=pytest.mark.xfail(reason=\"not implemented: #102\"),\n        ),\n    ],\n)\n</code></pre> <p>The signature of pytest.param is this: <pre><code>pytest.param(*values, **kw)\n</code></pre> where:</p> <ul> <li><code>*values</code> is the parameter set: <code>\"hypot(x, y)\", dict(x=3, y=4), 5.0</code>.</li> <li><code>**kw</code> are options as keyword arguments:  <code>marks=pytest.mark.xfail(reason=\"not implemented: #102\")</code>. It accepts a single mark or a sequence of marks. There is another option, <code>id</code>, which will override the automatically generated IDs by pytest.</li> </ul> <p>Behind the scenes, every tuple of parameters passed to  <code>@pytest.mark.parametrize</code> is converted to a <code>pytest.param</code> without extra options</p>"},{"location":"python/testing/pytest/plugins/","title":"Plugins","text":""},{"location":"python/testing/pytest/plugins/#pytest-xdist","title":"pytest-xdist","text":"<p>This is a very popular plugin and is maintained by the core developers; it allows you to run tests under multiple CPUs, to speed up the test run.</p> <p>After installing it, simply use the <code>-n</code> command-line flag to use the given number of CPUs to run the tests: <pre><code>pytest -n 4\n</code></pre></p> <p>And that\u2019s it! Now, your tests will run across four cores and hopefully speed up the test suite quite a bit, if it is CPU intensive, thought  I/O-bound tests won\u2019t see much improvement, though. You can also use  <code>-n auto</code> to let pytest-xdist automatically figure out the number of CPUs you have available.</p> <p>Note Keep in mind that when your tests are running concurrently, and in random order, they must be careful to avoid stepping on each other\u2019s toes, for example, reading/writing to the same directory. While they should be idempotent anyway, running the tests in a random order often brings attention to problems that were lying dormant until then.</p>"},{"location":"python/testing/pytest/plugins/#pytest-mock","title":"pytest-mock","text":"<p>The pytest-mock plugin provides a fixture that allows a smoother integration between pytest and the  unittest.mock module of the standard library.  It provides functionality similar to the built-in monkeypatch fixture, but the mock objects produced by unittest.mock also record information on how they are accessed. This makes many common testing tasks easier, such as verifying that a mocked function has been called, and with which arguments.</p> <p>The plugin provides a <code>mocker</code> fixture that can be used for patching classes and methods. Using the <code>getpass</code> example from earlier, here is how you could write it using this plugin: <pre><code>import getpass\n\ndef test_login_success(mocker):\n    mocked = mocker.patch.object(getpass, \"getpass\", \n                                 return_value=\"valid-pass\")\n    assert user_login(\"test-user\")\n    mocked.assert_called_with(\"enter password: \")\n</code></pre></p> <p>Note that besides replacing <code>getpass.getpass()</code> and always returning the same value, we can also ensure that the getpass function has been called with the correct arguments.</p>"},{"location":"python/testing/pytest/tests/","title":"Introduction","text":""},{"location":"python/testing/pytest/tests/#running-tests","title":"Running tests","text":"<p><pre><code>pytest\n</code></pre> This will find all of the <code>test_*.py</code> and <code>*_test.py</code> modules in the current directory and below recursively, and will run all of the tests found in those files.</p> <p>You can reduce the search to specific directories:</p> <pre><code>pytest tests/core tests/contrib\n</code></pre> <p>You can also mix any number of files and directories:</p> <pre><code>pytest tests/core tests/contrib/test_text_plugin.py\n</code></pre> <p>You can execute specific tests by using the syntax <code>&lt;test-file&gt;::&lt;test-function-name&gt;</code>:</p> <pre><code>pytest tests/core/test_core.py::test_regex_matching\n</code></pre> <p>To see which tests there are without running them, use the <code>--collect-only</code>  flag</p>"},{"location":"python/testing/pytest/tests/#simple-test-case","title":"Simple test case","text":"<pre><code>def test_sum():\n    assert 3 + 2 = 5\n</code></pre>"},{"location":"python/testing/pytest/tests/#checking-exceptions-pytestraises","title":"Checking exceptions: <code>pytest.raises</code>","text":"<p>A good API documentation will clearly explain what the purpose of each function is, its parameters, and return values. Great API documentation also clearly explains which exceptions are raised and when.</p> <p>For that reason, testing that exceptions are raised in the appropriate circumstances is just as important as testing the main functionality of APIs. It is also important to make sure that exceptions contain an appropriate and clear message to help users understand the issue.</p> <p>Example: <pre><code># function\n\ndef create_character(name: str, class_name: str) -&gt; Character:\n\"\"\"\n    Creates a new character and inserts it into the database.\n\n    :raise InvalidCharacterNameError:\n        if the character name is empty.\n\n    :raise InvalidClassNameError:\n        if the class name is invalid.\n\n    :return: the newly created Character.\n    \"\"\"\n    if not name:\n        raise InvalidCharacterNameError('character name empty')\n\n    if class_name not in VALID_CLASSES:\n        msg = f'invalid class name: \"{class_name}\"'\n        raise InvalidClassNameError(msg)\n    ...\n\n# test cases\n\ndef test_empty_name():\n    with pytest.raises(InvalidCharacterNameError):\n        create_character(name='', class_name='warrior')\n\n\ndef test_invalid_class_name():\n    with pytest.raises(InvalidClassNameError):\n        create_character(name='Solaire', class_name='mage')\n</code></pre></p> <p><code>pytest.raises</code> is a with-statement that ensures the exception class passed to it will be raised inside its execution block.</p> <p><code>pytest.raises</code> can receive an optional <code>match</code> argument, which is a regular expression string that will be matched against the exception message, as well as checking the exception type.</p> <p>Example: <pre><code>def test_empty_name():\n    with pytest.raises(InvalidCharacterNameError,\n                       match='character name empty'):\n        create_character(name='', class_name='warrior')\n\n\ndef test_invalid_class_name():\n    with pytest.raises(InvalidClassNameError,\n                       match='invalid class name: \"mage\"'):\n        create_character(name='Solaire', class_name='mage')\n</code></pre></p>"},{"location":"python/testing/pytest/tests/#checking-warnings-pytestwarns","title":"Checking warnings: <code>pytest.warns</code>","text":"<p>APIs also evolve. New and better alternatives to old functions are provided,  arguments are removed, old ways of using a certain functionality evolve into better ways, and so on.</p> <p>API writers have to strike a balance between keeping old code working to avoid breaking clients and providing better ways of doing things, while all the while keeping their own API code maintainable. For this reason, a solution often adopted is to start to issue warnings when API clients use the old behavior, in the hope that they update their code to the new constructs. Warning messages are shown in situations where the current usage is not wrong to warrant an exception, it just happens that there are new and better ways of doing it. Often, warning messages are shown during a grace period for this update to take place, and afterward the old way is no longer supported.</p> <p>Python provides the standard warnings module exactly for this purpose,  making it easy to warn developers about forthcoming changes in APIs.</p>"},{"location":"python/testing/pytest/tests/#types-of-python-warnings","title":"Types of python warnings","text":"Class Description Warning This is the base class of all warning category classes. It is a subclass of <code>Exception</code>. UserWarning The default category for <code>warn()</code>. DeprecationWarning Base category for warnings about deprecated features when those warnings are intended for other Python developers (ignored by default, unless triggered by code in main). SyntaxWarning Base category for warnings about dubious syntactic features. RuntimeWarning Base category for warnings about dubious runtime features. FutureWarning Base category for warnings about deprecated features when those warnings are intended for end users of applications that are written in Python. PendingDeprecationWarning Base category for warnings about features that will be deprecated in the future (ignored by default). ImportWarning Base category for warnings triggered during the process of importing a module (ignored by default). UnicodeWarning Base category for warnings related to Unicode. BytesWarning Base category for warnings related to bytes and bytearray. ResourceWarning Base category for warnings related to resource usage. <p>Let\u2019s we decide to use <code>enum</code> instead of <code>str</code> for <code>class_name</code> while creating a character. But changing this suddenly would break all clients,  so we wisely decide to support both forms for the next release: <code>str</code> and the <code>PlayerClassenum</code>. We don\u2019t want to keep supporting this forever, so we start showing a warning whenever a character class is passed as a str.</p> <p>Example: <pre><code>def get_initial_hit_points(player_class: Union[PlayerClass, str]) -&gt; int:\n    if isinstance(player_class, str):\n        msg = 'Using player_class as str has been deprecated' \\\n              'and will be removed in the future'\n        warnings.warn(DeprecationWarning(msg))\n        player_class = get_player_enum_from_string(player_class)\n    ...\n\n# test\ndef test_get_initial_hit_points_warning():\n    with pytest.warns(DeprecationWarning, match='.*str has been deprecated.*'):\n        get_initial_hit_points('warrior')\n</code></pre></p>"},{"location":"python/testing/pytest/tests/#comparing-floating-point-numbers-pytestapprox","title":"Comparing floating point numbers: <code>pytest.approx</code>","text":"<p>Comparing floating point numbers can be tricky. Numbers that we consider equal in the real world are not so when represented by computer hardware: <pre><code>&gt;&gt;&gt; 0.1 + 0.2 == 0.3\nFalse\n</code></pre></p> <p><code>pytest.approx</code> solves this problem by automatically choosing a tolerance appropriate for the values involved in the expression, providing a very nice syntax to boot:</p> <pre><code>def test_approx_simple():\n    assert 0.1 + 0.2 == approx(0.3)\n</code></pre> <p>But the  approx function does not stop there; it can be used to compare:</p> <pre><code>def test_approx_list():\n    assert [0.1 + 1.2, 0.2 + 0.8] == approx([1.3, 1.0])\n</code></pre> <p>Dictionary <code>values</code> (not keys):</p> <pre><code>def test_approx_dict():\n    values = {'v1': 0.1 + 1.2, 'v2': 0.2 + 0.8}\n    assert values == approx(dict(v1=1.3, v2=1.0))\n</code></pre> <p><code>numpy</code> arrays:</p> <pre><code>def test_approx_numpy():\n    import numpy as np\n    values = np.array([0.1, 0.2]) + np.array([1.2, 0.8])\n    assert values == approx(np.array([1.3, 1.0]))\n</code></pre> <p>When a test fails, <code>approx</code> provides a nice error message displaying the values that failed and the tolerance used:</p> <pre><code>def test_approx_simple_fail():\n    assert 0.1 + 0.2 == approx(0.35)\nE   assert (0.1 + 0.2) == 0.35 \u00b1 3.5e-07\nE   + where 0.35 \u00b1 3.5e-07 = approx(0.35)\n</code></pre>"},{"location":"python/testing/pytest/tests/#useful-command-line-options","title":"Useful command-line options","text":""},{"location":"python/testing/pytest/tests/#keyword-expressions-k","title":"Keyword expressions: -k","text":"<p>Often, you don\u2019t exactly remember the full path or name of a test that you want to execute. At other times, many tests in your suite follow a similar pattern and you want to execute all of them because you just refactored a sensitive area of the code.</p> <p>By using the <code>-k &lt;EXPRESSION&gt;</code> flag (from keyword expression), you can run tests whose <code>item id</code> loosely matches the given expression:</p> <pre><code>pytest -k \"test_parse\"\n</code></pre> <p>This will execute all tests that contain the string <code>parse</code> in their item IDs.  You can also write simple Python expressions using Boolean operators:</p> <pre><code>pytest -k \"parse and not num\"\n</code></pre> <p>This will execute all tests that contain <code>parse</code> but <code>not num</code> in their item IDs.</p>"},{"location":"python/testing/pytest/tests/#stop-soon-x-maxfail","title":"Stop soon: -x, \u2013maxfail","text":"<p>This allows you to quickly see the first failing test and deal with the failure. After fixing the reason for the failure, you can continue running with <code>-x</code> to deal with the next problem.</p> <p>In some situations, you might try using the <code>--maxfail=N</code> command-line flag,  which stops the test session automatically after <code>N</code> failures or errors, or the shortcut <code>-x</code>, which equals <code>--maxfail=1</code></p>"},{"location":"python/testing/pytest/tests/#last-failed-failed-first-lf-ff","title":"Last failed, failed first: \u2013lf, \u2013ff","text":"<p>Pytest always remembers tests that failed in previous sessions, and can reuse that information to skip right to the tests that have failed previously. This is excellent news if you are incrementally fixing a test suite after a large refactoring, as mentioned in the previous section.</p> <p>You can run the tests that failed before by passing the <code>--lf</code> flag (meaning last failed)</p> <p>When used together with <code>-x</code> (<code>--maxfail=1</code>) these two flags are refactoring heaven. This lets you start executing the full suite and then pytest stops at the first test that fails. You fix the code, and execute the same command line again. Pytest starts right at the failed test, and goes on if it passes (or stops again if you haven\u2019t yet managed to fix the code yet). It will then stop at the next failure. Rinse and repeat until all tests pass again.</p> <p>Keep in mind that it doesn\u2019t matter if you execute another subset of tests in the middle of your refactoring; pytest always remembers which tests failed, regardless of the command-line executed.</p> <p>Finally, the <code>--ff</code> flag is similar to <code>--lf</code>, but it will reorder your tests so the previous failures are run first, followed by the tests that passed or that were not run yet.</p>"},{"location":"python/testing/pytest/tests/#output-capturing-s-and-capture","title":"Output capturing: -s and \u2013capture","text":"<p>Sometimes, developers leave print statements laying around by mistake, or even on purpose, to be used later for debugging. Some applications also may write to <code>stdout</code> or <code>stderr</code> as part of their normal operation or logging.</p> <p>All that output would make understanding the test suite display much harder.  For this reason, by default, pytest captures all output written to <code>stdout</code> and <code>stderr</code> automatically.</p> <p>While running your tests locally, you might want to disable output capturing to see what messages are being printed in real-time, or whether the capturing is interfering with other capturing your code might be doing. In those cases, just pass -s to pytest to completely disable capturing</p> <p>Pytest has two methods to capture output. Which method is used can be chosen with the <code>--capture</code> command-line flag</p> <ul> <li><code>--capture=fd:</code> captures output at the file-descriptor level, which means that all output written to the file descriptors, 1 (<code>stdout</code>) and 2 (<code>stderr</code>), is captured. This will capture output even from C extensions and is the default.</li> <li><code>--capture=sys:</code> captures output written directly to <code>sys.stdout</code> and  <code>sys.stderr</code> at the Python level, without trying to capture system-level file descriptors.</li> </ul> <p>For completeness, there\u2019s also <code>--capture=no</code>, which is the same as <code>-s</code>.</p>"},{"location":"python/testing/pytest/tests/#traceback-modes-and-locals-tb-showlocals","title":"Traceback modes and locals: \u2013tb, \u2013showlocals","text":"<p>Pytest will show a complete traceback of a failing test, as expected from a testing framework. However, by default, it doesn\u2019t show the standard traceback that most Python programmers are used to; it shows a different traceback.</p> <p>Options:</p> <ul> <li><code>--tb=auto</code>: default mode; pytest\u2019s own traceback.</li> <li><code>--tb=long</code>: This mode will show a portion of the code for all frames of failure tracebacks, making it quite verbose.</li> <li><code>--tb=short</code>: This mode will show a single line of code from all the frames of the failure traceback, providing short and concise output.</li> <li><code>--tb=native</code>: This mode will output the exact same traceback normally used by Python to report exceptions and is loved by purists.</li> <li><code>--tb=line</code>: This mode will output a single line per failing test,  showing only the exception message and the file location of the error.</li> <li><code>--tb=no</code>: This does not show any traceback or failure message at all,  making it also useful to run the suite first to get a glimpse of how many failures there are.</li> </ul> <p>Finally, while this is not a traceback mode flag specifically, <code>--showlocals</code> (or <code>-l</code> as shortcut) augments the traceback modes by showing a list of the local variables and their values when using <code>--tb=auto</code>, <code>--tb=long</code>, and <code>--tb=short</code> modes.</p> <p><code>--showlocals</code> is extremely useful both when running your tests locally and in CI, being a firm favorite. Be careful, though, as this might be a security risk: local variables might expose passwords and other sensitive information, so make sure to transfer tracebacks using secure connections and be careful to make them public.</p>"},{"location":"python/testing/pytest/tests/#slow-tests-with-durations","title":"Slow tests with \u2013durations","text":"<p>Using <code>--durations=N</code> provides a summary of the <code>N</code> longest running tests,  or uses (<code>0</code>)zero to see a summary of all tests.</p> <p>By default, pytest will not show test durations that are too small  (&lt;0.01s) unless <code>-vv</code> is passed on the command-line.</p>"},{"location":"python/testing/pytest/tests/#configuration-file-pytestini","title":"Configuration file: pytest.ini","text":"<p>Users can customize some pytest behavior using a configuration file called <code>pytest.ini</code>. This file is usually placed at the root of the repository and contains a number of configuration values that are applied to all test runs for that project. It is meant to be kept under version control and committed with the rest of the code.</p> <p>The format follows a simple ini-style format with all pytest-related options under a <code>[pytest]</code> section.</p> <p>To make config file using python refer to  configparser module.</p> <p>The location of this file also defines what pytest calls the root directory (<code>rootdir</code>): if present, the directory that contains the configuration file is considered the root directory.</p> <p>Without the configuration file, the root directory will depend on which directory you execute pytest from and which arguments are passed. For this  reason, it is always recommended to have a pytest.ini file in all but the simplest projects, even if empty.</p> <p>If you are using tox, you can put a [pytest] section in the traditional  <code>tox.ini</code> file and it will work just as well.</p> <p>We learned some very useful command-line options. Some of them might become personal favorites, but having to type them all the time would be annoying.</p> <p>The <code>addopts</code> configuration option can be used instead to always add a set of options to the command line</p> <pre><code>[pytest]\naddopts=--tb=native --maxfail=10 -v\n</code></pre> <p>Note that, despite its name, <code>addopts</code> actually inserts the options before other options typed in the command line. This makes it possible to override most options in addopts when passing them in explicitly. </p>"},{"location":"python/testing/pytest/tests/#customizing-a-collection","title":"Customizing a collection","text":"<p>By default, pytest collects tests using this heuristic:</p> <ul> <li>Files that match <code>test_*.py</code> and <code>*_test.py</code></li> <li>Inside test modules, functions that match test* and classes that match Test*</li> <li>Inside test classes, methods that match test*</li> </ul> <p>This convention is simple to understand and works for most projects, but they can be overwritten by these configuration options:</p> <ul> <li><code>python_files:</code> a list of patterns to use to collect test modules</li> <li><code>python_functions:</code> a list of patterns to use to collect test functions and test methods</li> <li><code>python_classes:</code> a list of patterns to use to collect test classes</li> </ul> <p>Here\u2019s an example of a configuration file changing the defaults:</p> <pre><code>[pytest]\npython_files = unittests_*.py\npython_functions = check_*\npython_classes = *TestSuite\n</code></pre> <p>The recommendation is to only use these configuration options for legacy projects that follow a different convention, and stick with the defaults for new projects. Using the defaults is less work and avoids confusing other collaborators.</p>"},{"location":"python/testing/pytest/tests/#cache-directory-cache_dir","title":"Cache directory: cache_dir","text":"<p>The <code>--lf</code> and <code>--ff</code> options shown previously are provided by an internal plugin named cacheprovider, which saves data on a directory on disk so it can be accessed in future sessions. This directory by default is located in the root directory under the name <code>.pytest_cache</code>. This directory should never be committed to version control.</p> <p>If you would like to change the location of that directory, you can use the <code>cache_dir</code> option. This option also expands environment variables automatically.</p> <pre><code>[pytest]\ncache_dir=$TMP/pytest-cache\n</code></pre>"},{"location":"python/testing/pytest/tests/#avoid-recursing-into-directories-norecursedirs","title":"Avoid recursing into directories: norecursedirs","text":"<p>pytest by default will recurse over all subdirectories of the arguments given on the command line. This might make test collection take more time than desired when recursing into directories that never contain any tests.</p> <p>pytest by default tries to be smart and will not recurse inside folders with the patterns <code>.*</code>, <code>build</code>, <code>dist</code>, <code>CVS</code>, <code>_darcs</code>, <code>{arch}</code>, <code>*.egg</code>, <code>venv</code>. It also tries to detect virtualenvs automatically by looking at known locations for activation scripts.</p> <p>The <code>norecursedirs</code> option can be used to override the default list of pattern names that pytest should never recurse into</p> <pre><code>[pytest]\nnorecursedirs = artifacts _build docs\n</code></pre> <p>You can also use the <code>--collect-in-virtualenv</code> flag to skip the <code>virtualenv</code> detection.</p>"},{"location":"python/testing/pytest/tests/#pick-the-right-place-by-default-testpaths","title":"Pick the right place by default: testpaths","text":"<p>With tests separated from the application/library code in a tests or similarly named directory. In that layout it is useful to use the testpaths configuration option.</p> <pre><code>[pytest]\ntestpaths = tests\n</code></pre> <p>This will tell pytest where to look for tests when no files, directories,  or node ids are given in the command line, which might speed up test collection. Note that you can configure more than one directory, separated by spaces.</p>"},{"location":"python/testing/pytest/tests/#override-options-with-o-override","title":"Override options with -o/\u2013override","text":"<p>Finally, a little known feature is that you can override any configuration option directly in the command-line using the <code>-o</code> /<code>--override</code> flags. This flag can be passed multiple times to override more than one option.</p> <pre><code>pytest -o python_classes=Suite -o cache_dir=$TMP/pytest-cache\n</code></pre>"}]}