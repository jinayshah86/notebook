<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is mickey mouse's notebook"><meta name=author content="Mickey Mouse"><link href=https://jinayshah.netlify.com/python/testing/pytest/tests/ rel=canonical><link href=../../introduction/ rel=prev><link href=../markers/ rel=next><link rel=icon href=../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.4.3, mkdocs-material-9.1.18"><title>Introduction - Mickey Mouse's Notebook</title><link rel=stylesheet href=../../../../assets/stylesheets/main.26e3688c.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,300i,400,400i,700,700i%7CHack:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto Mono";--md-code-font:"Hack"}</style><link rel=stylesheet href=../../../../css/timeago.css><link rel=stylesheet href=https://unpkg.com/mermaid@7.1.2/dist/mermaid.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#running-tests class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Mickey Mouse's Notebook" class="md-header__button md-logo" aria-label="Mickey Mouse's Notebook" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Mickey Mouse's Notebook </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Introduction </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Mickey Mouse's Notebook" class="md-nav__button md-logo" aria-label="Mickey Mouse's Notebook" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> Mickey Mouse's Notebook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> Python <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../functions/ class=md-nav__link> Functions </a> </li> <li class=md-nav__item> <a href=../../../decorators/ class=md-nav__link> Decorators </a> </li> <li class=md-nav__item> <a href=../../../oop/ class=md-nav__link> OOP </a> </li> <li class=md-nav__item> <a href=../../../iterators/ class=md-nav__link> Iterators </a> </li> <li class=md-nav__item> <a href=../../../files/ class=md-nav__link> Working with files </a> </li> <li class=md-nav__item> <a href=../../../custom-json/ class=md-nav__link> Custom JSON Encoder/Decoder </a> </li> <li class=md-nav__item> <a href=../../../pickling/ class=md-nav__link> Pickling </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8 checked> <label class=md-nav__link for=__nav_2_8 id=__nav_2_8_label tabindex=0> Testing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=true> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> Testing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../introduction/ class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8_2 checked> <label class=md-nav__link for=__nav_2_8_2 id=__nav_2_8_2_label tabindex=0> Pytest <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_8_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2_8_2> <span class="md-nav__icon md-icon"></span> Pytest </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Introduction <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Introduction </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#running-tests class=md-nav__link> Running tests </a> </li> <li class=md-nav__item> <a href=#simple-test-case class=md-nav__link> Simple test case </a> </li> <li class=md-nav__item> <a href=#checking-exceptions-pytestraises class=md-nav__link> Checking exceptions: pytest.raises </a> </li> <li class=md-nav__item> <a href=#checking-warnings-pytestwarns class=md-nav__link> Checking warnings: pytest.warns </a> <nav class=md-nav aria-label="Checking warnings: pytest.warns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#types-of-python-warnings class=md-nav__link> Types of python warnings </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#comparing-floating-point-numbers-pytestapprox class=md-nav__link> Comparing floating point numbers: pytest.approx </a> </li> <li class=md-nav__item> <a href=#useful-command-line-options class=md-nav__link> Useful command-line options </a> <nav class=md-nav aria-label="Useful command-line options"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#keyword-expressions-k class=md-nav__link> Keyword expressions: -k </a> </li> <li class=md-nav__item> <a href=#stop-soon-x-maxfail class=md-nav__link> Stop soon: -x, --maxfail </a> </li> <li class=md-nav__item> <a href=#last-failed-failed-first-lf-ff class=md-nav__link> Last failed, failed first: --lf, --ff </a> </li> <li class=md-nav__item> <a href=#output-capturing-s-and-capture class=md-nav__link> Output capturing: -s and --capture </a> </li> <li class=md-nav__item> <a href=#traceback-modes-and-locals-tb-showlocals class=md-nav__link> Traceback modes and locals: --tb, --showlocals </a> </li> <li class=md-nav__item> <a href=#slow-tests-with-durations class=md-nav__link> Slow tests with --durations </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configuration-file-pytestini class=md-nav__link> Configuration file: pytest.ini </a> <nav class=md-nav aria-label="Configuration file: pytest.ini"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#customizing-a-collection class=md-nav__link> Customizing a collection </a> </li> <li class=md-nav__item> <a href=#cache-directory-cache_dir class=md-nav__link> Cache directory: cache_dir </a> </li> <li class=md-nav__item> <a href=#avoid-recursing-into-directories-norecursedirs class=md-nav__link> Avoid recursing into directories: norecursedirs </a> </li> <li class=md-nav__item> <a href=#pick-the-right-place-by-default-testpaths class=md-nav__link> Pick the right place by default: testpaths </a> </li> <li class=md-nav__item> <a href=#override-options-with-o-override class=md-nav__link> Override options with -o/--override </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../markers/ class=md-nav__link> Markers </a> </li> <li class=md-nav__item> <a href=../parametrization/ class=md-nav__link> Parametrization </a> </li> <li class=md-nav__item> <a href=../fixtures/ class=md-nav__link> Fixtures </a> </li> <li class=md-nav__item> <a href=../plugins/ class=md-nav__link> Plugins </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../tox/ class=md-nav__link> Tox </a> </li> <li class=md-nav__item> <a href=../../coverage/ class=md-nav__link> Coverage </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../exceptions/ class=md-nav__link> Exceptions </a> </li> <li class=md-nav__item> <a href=../../../profiling/ class=md-nav__link> Profiling </a> </li> <li class=md-nav__item> <a href=../../../concurrency/ class=md-nav__link> Concurrent Execution </a> </li> <li class=md-nav__item> <a href=../../../logging/ class=md-nav__link> Logging </a> </li> <li class=md-nav__item> <a href=../../../references/ class=md-nav__link> References </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#running-tests class=md-nav__link> Running tests </a> </li> <li class=md-nav__item> <a href=#simple-test-case class=md-nav__link> Simple test case </a> </li> <li class=md-nav__item> <a href=#checking-exceptions-pytestraises class=md-nav__link> Checking exceptions: pytest.raises </a> </li> <li class=md-nav__item> <a href=#checking-warnings-pytestwarns class=md-nav__link> Checking warnings: pytest.warns </a> <nav class=md-nav aria-label="Checking warnings: pytest.warns"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#types-of-python-warnings class=md-nav__link> Types of python warnings </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#comparing-floating-point-numbers-pytestapprox class=md-nav__link> Comparing floating point numbers: pytest.approx </a> </li> <li class=md-nav__item> <a href=#useful-command-line-options class=md-nav__link> Useful command-line options </a> <nav class=md-nav aria-label="Useful command-line options"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#keyword-expressions-k class=md-nav__link> Keyword expressions: -k </a> </li> <li class=md-nav__item> <a href=#stop-soon-x-maxfail class=md-nav__link> Stop soon: -x, --maxfail </a> </li> <li class=md-nav__item> <a href=#last-failed-failed-first-lf-ff class=md-nav__link> Last failed, failed first: --lf, --ff </a> </li> <li class=md-nav__item> <a href=#output-capturing-s-and-capture class=md-nav__link> Output capturing: -s and --capture </a> </li> <li class=md-nav__item> <a href=#traceback-modes-and-locals-tb-showlocals class=md-nav__link> Traceback modes and locals: --tb, --showlocals </a> </li> <li class=md-nav__item> <a href=#slow-tests-with-durations class=md-nav__link> Slow tests with --durations </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#configuration-file-pytestini class=md-nav__link> Configuration file: pytest.ini </a> <nav class=md-nav aria-label="Configuration file: pytest.ini"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#customizing-a-collection class=md-nav__link> Customizing a collection </a> </li> <li class=md-nav__item> <a href=#cache-directory-cache_dir class=md-nav__link> Cache directory: cache_dir </a> </li> <li class=md-nav__item> <a href=#avoid-recursing-into-directories-norecursedirs class=md-nav__link> Avoid recursing into directories: norecursedirs </a> </li> <li class=md-nav__item> <a href=#pick-the-right-place-by-default-testpaths class=md-nav__link> Pick the right place by default: testpaths </a> </li> <li class=md-nav__item> <a href=#override-options-with-o-override class=md-nav__link> Override options with -o/--override </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Introduction</h1> <h3 id=running-tests>Running tests<a class=headerlink href=#running-tests title="Permanent link">&para;</a></h3> <p><div class=highlight><pre><span></span><code>pytest
</code></pre></div> This will find all of the <code>test_*.py</code> and <code>*_test.py</code> modules in the current directory and below recursively, and will run all of the tests found in those files.</p> <p>You can reduce the search to specific directories:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>tests/core<span class=w> </span>tests/contrib
</code></pre></div> <p>You can also mix any number of files and directories:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>tests/core<span class=w> </span>tests/contrib/test_text_plugin.py
</code></pre></div> <p>You can execute specific tests by using the syntax <code>&lt;test-file&gt;::&lt;test-function-name&gt;</code>:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>tests/core/test_core.py::test_regex_matching
</code></pre></div> <p>To see which tests there are without running them, use the <code>--collect-only</code> flag</p> <h3 id=simple-test-case>Simple test case<a class=headerlink href=#simple-test-case title="Permanent link">&para;</a></h3> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_sum</span><span class=p>():</span>
    <span class=k>assert</span> <span class=mi>3</span> <span class=o>+</span> <span class=mi>2</span> <span class=o>=</span> <span class=mi>5</span>
</code></pre></div> <h3 id=checking-exceptions-pytestraises>Checking exceptions: <code>pytest.raises</code><a class=headerlink href=#checking-exceptions-pytestraises title="Permanent link">&para;</a></h3> <p>A good API documentation will clearly explain what the purpose of each function is, its parameters, and return values. Great API documentation also clearly explains which exceptions are raised and when.</p> <p>For that reason, testing that exceptions are raised in the appropriate circumstances is just as important as testing the main functionality of APIs. It is also important to make sure that exceptions contain an appropriate and clear message to help users understand the issue.</p> <p><strong>Example:</strong> <div class=highlight><pre><span></span><code><span class=c1># function</span>

<span class=k>def</span> <span class=nf>create_character</span><span class=p>(</span><span class=n>name</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>class_name</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Character</span><span class=p>:</span>
<span class=w>    </span><span class=sd>&quot;&quot;&quot;</span>
<span class=sd>    Creates a new character and inserts it into the database.</span>

<span class=sd>    :raise InvalidCharacterNameError:</span>
<span class=sd>        if the character name is empty.</span>

<span class=sd>    :raise InvalidClassNameError:</span>
<span class=sd>        if the class name is invalid.</span>

<span class=sd>    :return: the newly created Character.</span>
<span class=sd>    &quot;&quot;&quot;</span>
    <span class=k>if</span> <span class=ow>not</span> <span class=n>name</span><span class=p>:</span>
        <span class=k>raise</span> <span class=n>InvalidCharacterNameError</span><span class=p>(</span><span class=s1>&#39;character name empty&#39;</span><span class=p>)</span>

    <span class=k>if</span> <span class=n>class_name</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>VALID_CLASSES</span><span class=p>:</span>
        <span class=n>msg</span> <span class=o>=</span> <span class=sa>f</span><span class=s1>&#39;invalid class name: &quot;</span><span class=si>{</span><span class=n>class_name</span><span class=si>}</span><span class=s1>&quot;&#39;</span>
        <span class=k>raise</span> <span class=n>InvalidClassNameError</span><span class=p>(</span><span class=n>msg</span><span class=p>)</span>
    <span class=o>...</span>

<span class=c1># test cases</span>

<span class=k>def</span> <span class=nf>test_empty_name</span><span class=p>():</span>
    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>raises</span><span class=p>(</span><span class=n>InvalidCharacterNameError</span><span class=p>):</span>
        <span class=n>create_character</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>class_name</span><span class=o>=</span><span class=s1>&#39;warrior&#39;</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>test_invalid_class_name</span><span class=p>():</span>
    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>raises</span><span class=p>(</span><span class=n>InvalidClassNameError</span><span class=p>):</span>
        <span class=n>create_character</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;Solaire&#39;</span><span class=p>,</span> <span class=n>class_name</span><span class=o>=</span><span class=s1>&#39;mage&#39;</span><span class=p>)</span>
</code></pre></div></p> <p><code>pytest.raises</code> is a with-statement that ensures the exception class passed to it will be raised inside its execution block.</p> <p><code>pytest.raises</code> can receive an optional <code>match</code> argument, which is a regular expression string that will be matched against the exception message, as well as checking the exception type.</p> <p><strong>Example:</strong> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_empty_name</span><span class=p>():</span>
    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>raises</span><span class=p>(</span><span class=n>InvalidCharacterNameError</span><span class=p>,</span>
                       <span class=n>match</span><span class=o>=</span><span class=s1>&#39;character name empty&#39;</span><span class=p>):</span>
        <span class=n>create_character</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>class_name</span><span class=o>=</span><span class=s1>&#39;warrior&#39;</span><span class=p>)</span>


<span class=k>def</span> <span class=nf>test_invalid_class_name</span><span class=p>():</span>
    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>raises</span><span class=p>(</span><span class=n>InvalidClassNameError</span><span class=p>,</span>
                       <span class=n>match</span><span class=o>=</span><span class=s1>&#39;invalid class name: &quot;mage&quot;&#39;</span><span class=p>):</span>
        <span class=n>create_character</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;Solaire&#39;</span><span class=p>,</span> <span class=n>class_name</span><span class=o>=</span><span class=s1>&#39;mage&#39;</span><span class=p>)</span>
</code></pre></div></p> <h3 id=checking-warnings-pytestwarns>Checking warnings: <code>pytest.warns</code><a class=headerlink href=#checking-warnings-pytestwarns title="Permanent link">&para;</a></h3> <p>APIs also evolve. New and better alternatives to old functions are provided, arguments are removed, old ways of using a certain functionality evolve into better ways, and so on.</p> <p>API writers have to strike a balance between keeping old code working to avoid breaking clients and providing better ways of doing things, while all the while keeping their own API code maintainable. For this reason, a solution often adopted is to start to issue warnings when API clients use the old behavior, in the hope that they update their code to the new constructs. Warning messages are shown in situations where the current usage is not wrong to warrant an exception, it just happens that there are new and better ways of doing it. Often, warning messages are shown during a grace period for this update to take place, and afterward the old way is no longer supported.</p> <p>Python provides the standard warnings module exactly for this purpose, making it easy to warn developers about forthcoming changes in APIs.</p> <h4 id=types-of-python-warnings>Types of python warnings<a class=headerlink href=#types-of-python-warnings title="Permanent link">&para;</a></h4> <table> <thead> <tr> <th>Class</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Warning</td> <td>This is the base class of all warning category classes. It is a subclass of <code>Exception</code>.</td> </tr> <tr> <td>UserWarning</td> <td>The default category for <code>warn()</code>.</td> </tr> <tr> <td>DeprecationWarning</td> <td>Base category for warnings about deprecated features when those warnings are intended for other Python developers (ignored by default, unless triggered by code in <strong>main</strong>).</td> </tr> <tr> <td>SyntaxWarning</td> <td>Base category for warnings about dubious syntactic features.</td> </tr> <tr> <td>RuntimeWarning</td> <td>Base category for warnings about dubious runtime features.</td> </tr> <tr> <td>FutureWarning</td> <td>Base category for warnings about deprecated features when those warnings are intended for end users of applications that are written in Python.</td> </tr> <tr> <td>PendingDeprecationWarning</td> <td>Base category for warnings about features that will be deprecated in the future (ignored by default).</td> </tr> <tr> <td>ImportWarning</td> <td>Base category for warnings triggered during the process of importing a module (ignored by default).</td> </tr> <tr> <td>UnicodeWarning</td> <td>Base category for warnings related to Unicode.</td> </tr> <tr> <td>BytesWarning</td> <td>Base category for warnings related to bytes and bytearray.</td> </tr> <tr> <td>ResourceWarning</td> <td>Base category for warnings related to resource usage.</td> </tr> </tbody> </table> <p>Let&rsquo;s we decide to use <code>enum</code> instead of <code>str</code> for <code>class_name</code> while creating a character. But changing this suddenly would break all clients, so we wisely decide to support both forms for the next release: <code>str</code> and the <code>PlayerClassenum</code>. We don&rsquo;t want to keep supporting this forever, so we start showing a warning whenever a character class is passed as a str.</p> <p><strong>Example:</strong> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>get_initial_hit_points</span><span class=p>(</span><span class=n>player_class</span><span class=p>:</span> <span class=n>Union</span><span class=p>[</span><span class=n>PlayerClass</span><span class=p>,</span> <span class=nb>str</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>int</span><span class=p>:</span>
    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>player_class</span><span class=p>,</span> <span class=nb>str</span><span class=p>):</span>
        <span class=n>msg</span> <span class=o>=</span> <span class=s1>&#39;Using player_class as str has been deprecated&#39;</span> \
              <span class=s1>&#39;and will be removed in the future&#39;</span>
        <span class=n>warnings</span><span class=o>.</span><span class=n>warn</span><span class=p>(</span><span class=ne>DeprecationWarning</span><span class=p>(</span><span class=n>msg</span><span class=p>))</span>
        <span class=n>player_class</span> <span class=o>=</span> <span class=n>get_player_enum_from_string</span><span class=p>(</span><span class=n>player_class</span><span class=p>)</span>
    <span class=o>...</span>

<span class=c1># test</span>
<span class=k>def</span> <span class=nf>test_get_initial_hit_points_warning</span><span class=p>():</span>
    <span class=k>with</span> <span class=n>pytest</span><span class=o>.</span><span class=n>warns</span><span class=p>(</span><span class=ne>DeprecationWarning</span><span class=p>,</span> <span class=n>match</span><span class=o>=</span><span class=s1>&#39;.*str has been deprecated.*&#39;</span><span class=p>):</span>
        <span class=n>get_initial_hit_points</span><span class=p>(</span><span class=s1>&#39;warrior&#39;</span><span class=p>)</span>
</code></pre></div></p> <h3 id=comparing-floating-point-numbers-pytestapprox>Comparing floating point numbers: <code>pytest.approx</code><a class=headerlink href=#comparing-floating-point-numbers-pytestapprox title="Permanent link">&para;</a></h3> <p>Comparing floating point numbers can be tricky. Numbers that we consider equal in the real world are not so when represented by computer hardware: <div class=highlight><pre><span></span><code><span class=o>&gt;&gt;&gt;</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.2</span> <span class=o>==</span> <span class=mf>0.3</span>
<span class=kc>False</span>
</code></pre></div></p> <p><code>pytest.approx</code> solves this problem by automatically choosing a tolerance appropriate for the values involved in the expression, providing a very nice syntax to boot:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_approx_simple</span><span class=p>():</span>
    <span class=k>assert</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.2</span> <span class=o>==</span> <span class=n>approx</span><span class=p>(</span><span class=mf>0.3</span><span class=p>)</span>
</code></pre></div> <p>But the approx function does not stop there; it can be used to compare:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_approx_list</span><span class=p>():</span>
    <span class=k>assert</span> <span class=p>[</span><span class=mf>0.1</span> <span class=o>+</span> <span class=mf>1.2</span><span class=p>,</span> <span class=mf>0.2</span> <span class=o>+</span> <span class=mf>0.8</span><span class=p>]</span> <span class=o>==</span> <span class=n>approx</span><span class=p>([</span><span class=mf>1.3</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>])</span>
</code></pre></div> <p>Dictionary <code>values</code> (not keys):</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_approx_dict</span><span class=p>():</span>
    <span class=n>values</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;v1&#39;</span><span class=p>:</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>1.2</span><span class=p>,</span> <span class=s1>&#39;v2&#39;</span><span class=p>:</span> <span class=mf>0.2</span> <span class=o>+</span> <span class=mf>0.8</span><span class=p>}</span>
    <span class=k>assert</span> <span class=n>values</span> <span class=o>==</span> <span class=n>approx</span><span class=p>(</span><span class=nb>dict</span><span class=p>(</span><span class=n>v1</span><span class=o>=</span><span class=mf>1.3</span><span class=p>,</span> <span class=n>v2</span><span class=o>=</span><span class=mf>1.0</span><span class=p>))</span>
</code></pre></div> <p><code>numpy</code> arrays:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_approx_numpy</span><span class=p>():</span>
    <span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
    <span class=n>values</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>])</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>1.2</span><span class=p>,</span> <span class=mf>0.8</span><span class=p>])</span>
    <span class=k>assert</span> <span class=n>values</span> <span class=o>==</span> <span class=n>approx</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mf>1.3</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>]))</span>
</code></pre></div> <p>When a test fails, <code>approx</code> provides a nice error message displaying the values that failed and the tolerance used:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_approx_simple_fail</span><span class=p>():</span>
    <span class=k>assert</span> <span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.2</span> <span class=o>==</span> <span class=n>approx</span><span class=p>(</span><span class=mf>0.35</span><span class=p>)</span>
<span class=n>E</span>   <span class=k>assert</span> <span class=p>(</span><span class=mf>0.1</span> <span class=o>+</span> <span class=mf>0.2</span><span class=p>)</span> <span class=o>==</span> <span class=mf>0.35</span> <span class=err>±</span> <span class=mf>3.5e-07</span>
<span class=n>E</span>   <span class=o>+</span> <span class=n>where</span> <span class=mf>0.35</span> <span class=err>±</span> <span class=mf>3.5e-07</span> <span class=o>=</span> <span class=n>approx</span><span class=p>(</span><span class=mf>0.35</span><span class=p>)</span>
</code></pre></div> <h3 id=useful-command-line-options>Useful command-line options<a class=headerlink href=#useful-command-line-options title="Permanent link">&para;</a></h3> <h4 id=keyword-expressions-k>Keyword expressions: -k<a class=headerlink href=#keyword-expressions-k title="Permanent link">&para;</a></h4> <p>Often, you don&rsquo;t exactly remember the full path or name of a test that you want to execute. At other times, many tests in your suite follow a similar pattern and you want to execute all of them because you just refactored a sensitive area of the code.</p> <p>By using the <code>-k &lt;EXPRESSION&gt;</code> flag (from <strong>keyword expression</strong>), you can run tests whose <code>item id</code> loosely matches the given expression:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>-k<span class=w> </span><span class=s2>&quot;test_parse&quot;</span>
</code></pre></div> <p>This will execute all tests that contain the string <code>parse</code> in their item IDs. You can also write simple Python expressions using Boolean operators:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>-k<span class=w> </span><span class=s2>&quot;parse and not num&quot;</span>
</code></pre></div> <p>This will execute all tests that contain <code>parse</code> but <code>not num</code> in their item IDs.</p> <h4 id=stop-soon-x-maxfail>Stop soon: -x, &ndash;maxfail<a class=headerlink href=#stop-soon-x-maxfail title="Permanent link">&para;</a></h4> <p>This allows you to quickly see the first failing test and deal with the failure. After fixing the reason for the failure, you can continue running with <code>-x</code> to deal with the next problem.</p> <p>In some situations, you might try using the <code>--maxfail=N</code> command-line flag, which stops the test session automatically after <code>N</code> failures or errors, or the shortcut <code>-x</code>, which equals <code>--maxfail=1</code></p> <h4 id=last-failed-failed-first-lf-ff>Last failed, failed first: &ndash;lf, &ndash;ff<a class=headerlink href=#last-failed-failed-first-lf-ff title="Permanent link">&para;</a></h4> <p>Pytest always remembers tests that failed in previous sessions, and can reuse that information to skip right to the tests that have failed previously. This is excellent news if you are incrementally fixing a test suite after a large refactoring, as mentioned in the previous section.</p> <p>You can run the tests that failed before by passing the <code>--lf</code> flag (meaning last failed)</p> <p>When used together with <code>-x</code> (<code>--maxfail=1</code>) these two flags are refactoring heaven. This lets you start executing the full suite and then pytest stops at the first test that fails. You fix the code, and execute the same command line again. Pytest starts right at the failed test, and goes on if it passes (or stops again if you haven&rsquo;t yet managed to fix the code yet). It will then stop at the next failure. Rinse and repeat until all tests pass again.</p> <p>Keep in mind that it doesn&rsquo;t matter if you execute another subset of tests in the middle of your refactoring; pytest always remembers which tests failed, regardless of the command-line executed.</p> <p>Finally, the <code>--ff</code> flag is similar to <code>--lf</code>, but it will reorder your tests so the previous failures are run first, followed by the tests that passed or that were not run yet.</p> <h4 id=output-capturing-s-and-capture>Output capturing: -s and &ndash;capture<a class=headerlink href=#output-capturing-s-and-capture title="Permanent link">&para;</a></h4> <p>Sometimes, developers leave print statements laying around by mistake, or even on purpose, to be used later for debugging. Some applications also may write to <code>stdout</code> or <code>stderr</code> as part of their normal operation or logging.</p> <p>All that output would make understanding the test suite display much harder. For this reason, by default, pytest captures all output written to <code>stdout</code> and <code>stderr</code> automatically.</p> <p>While running your tests locally, you might want to disable output capturing to see what messages are being printed in real-time, or whether the capturing is interfering with other capturing your code might be doing. In those cases, just pass -s to pytest to completely disable capturing</p> <p>Pytest has two methods to capture output. Which method is used can be chosen with the <code>--capture</code> command-line flag</p> <ul> <li><code>--capture=fd:</code> captures output at the file-descriptor level, which means that all output written to the file descriptors, 1 (<code>stdout</code>) and 2 (<code>stderr</code>), is captured. This will capture output even from C extensions and is the default.</li> <li><code>--capture=sys:</code> captures output written directly to <code>sys.stdout</code> and <code>sys.stderr</code> at the Python level, without trying to capture system-level file descriptors.</li> </ul> <p>For completeness, there&rsquo;s also <code>--capture=no</code>, which is the same as <code>-s</code>.</p> <h4 id=traceback-modes-and-locals-tb-showlocals>Traceback modes and locals: &ndash;tb, &ndash;showlocals<a class=headerlink href=#traceback-modes-and-locals-tb-showlocals title="Permanent link">&para;</a></h4> <p>Pytest will show a complete traceback of a failing test, as expected from a testing framework. However, by default, it doesn&rsquo;t show the standard traceback that most Python programmers are used to; it shows a different traceback.</p> <p><strong>Options:</strong></p> <ul> <li><code>--tb=auto</code>: default mode; pytest&rsquo;s own traceback.</li> <li><code>--tb=long</code>: This mode will show a portion of the code for all frames of failure tracebacks, making it quite verbose.</li> <li><code>--tb=short</code>: This mode will show a single line of code from all the frames of the failure traceback, providing short and concise output.</li> <li><code>--tb=native</code>: This mode will output the exact same traceback normally used by Python to report exceptions and is loved by purists.</li> <li><code>--tb=line</code>: This mode will output a single line per failing test, showing only the exception message and the file location of the error.</li> <li><code>--tb=no</code>: This does not show any traceback or failure message at all, making it also useful to run the suite first to get a glimpse of how many failures there are.</li> </ul> <p>Finally, while this is not a traceback mode flag specifically, <code>--showlocals</code> (or <code>-l</code> as shortcut) augments the traceback modes by showing a list of the local variables and their values when using <code>--tb=auto</code>, <code>--tb=long</code>, and <code>--tb=short</code> modes.</p> <p><code>--showlocals</code> is extremely useful both when running your tests locally and in CI, being a firm favorite. Be careful, though, as this might be a security risk: local variables might expose passwords and other sensitive information, so make sure to transfer tracebacks using secure connections and be careful to make them public.</p> <h4 id=slow-tests-with-durations>Slow tests with &ndash;durations<a class=headerlink href=#slow-tests-with-durations title="Permanent link">&para;</a></h4> <p>Using <code>--durations=N</code> provides a summary of the <code>N</code> longest running tests, or uses (<code>0</code>)zero to see a summary of all tests.</p> <p>By default, pytest will not show test durations that are too small (&lt;<strong>0.01s</strong>) unless <code>-vv</code> is passed on the command-line.</p> <h3 id=configuration-file-pytestini>Configuration file: pytest.ini<a class=headerlink href=#configuration-file-pytestini title="Permanent link">&para;</a></h3> <p>Users can customize some pytest behavior using a configuration file called <code>pytest.ini</code>. This file is usually placed at the root of the repository and contains a number of configuration values that are applied to all test runs for that project. It is meant to be kept under version control and committed with the rest of the code.</p> <p>The format follows a simple ini-style format with all pytest-related options under a <code>[pytest]</code> section.</p> <blockquote> <p>To make config file using python refer to <a href=https://docs.python.org/3/library/configparser.html target=_blank>configparser module</a>.</p> </blockquote> <p>The location of this file also defines what pytest calls the root directory (<code>rootdir</code>): if present, the directory that contains the configuration file is considered the root directory.</p> <p>Without the configuration file, the root directory will depend on which directory you execute pytest from and which arguments are passed. For this reason, it is always recommended to have a pytest.ini file in all but the simplest projects, even if empty.</p> <p>If you are using tox, you can put a [pytest] section in the traditional <code>tox.ini</code> file and it will work just as well.</p> <p>We learned some very useful command-line options. Some of them might become personal favorites, but having to type them all the time would be annoying.</p> <p>The <code>addopts</code> configuration option can be used instead to always add a set of options to the command line</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>addopts</span><span class=o>=</span><span class=s>--tb=native --maxfail=10 -v</span>
</code></pre></div> <p>Note that, despite its name, <code>addopts</code> actually inserts the options before other options typed in the command line. This makes it possible to override most options in addopts when passing them in explicitly. </p> <h4 id=customizing-a-collection>Customizing a collection<a class=headerlink href=#customizing-a-collection title="Permanent link">&para;</a></h4> <p>By default, pytest collects tests using this heuristic:</p> <ul> <li>Files that match <code>test_*.py</code> and <code>*_test.py</code></li> <li>Inside test modules, functions that match test* and classes that match Test*</li> <li>Inside test classes, methods that match test*</li> </ul> <p>This convention is simple to understand and works for most projects, but they can be overwritten by these configuration options:</p> <ul> <li><code>python_files:</code> a list of patterns to use to collect test modules</li> <li><code>python_functions:</code> a list of patterns to use to collect test functions and test methods</li> <li><code>python_classes:</code> a list of patterns to use to collect test classes</li> </ul> <p>Here&rsquo;s an example of a configuration file changing the defaults:</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>python_files</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>unittests_*.py</span>
<span class=na>python_functions</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>check_*</span>
<span class=na>python_classes</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>*TestSuite</span>
</code></pre></div> <p>The recommendation is to only use these configuration options for legacy projects that follow a different convention, and stick with the defaults for new projects. Using the defaults is less work and avoids confusing other collaborators.</p> <h4 id=cache-directory-cache_dir>Cache directory: cache_dir<a class=headerlink href=#cache-directory-cache_dir title="Permanent link">&para;</a></h4> <p>The <code>--lf</code> and <code>--ff</code> options shown previously are provided by an internal plugin named cacheprovider, which saves data on a directory on disk so it can be accessed in future sessions. This directory by default is located in the <strong>root directory</strong> under the name <code>.pytest_cache</code>. This directory should never be committed to version control.</p> <p>If you would like to change the location of that directory, you can use the <code>cache_dir</code> option. This option also expands environment variables automatically.</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>cache_dir</span><span class=o>=</span><span class=s>$TMP/pytest-cache</span>
</code></pre></div> <h4 id=avoid-recursing-into-directories-norecursedirs>Avoid recursing into directories: norecursedirs<a class=headerlink href=#avoid-recursing-into-directories-norecursedirs title="Permanent link">&para;</a></h4> <p>pytest by default will recurse over all subdirectories of the arguments given on the command line. This might make test collection take more time than desired when recursing into directories that never contain any tests.</p> <p>pytest by default tries to be smart and will not recurse inside folders with the patterns <code>.*</code>, <code>build</code>, <code>dist</code>, <code>CVS</code>, <code>_darcs</code>, <code>{arch}</code>, <code>*.egg</code>, <code>venv</code>. It also tries to detect virtualenvs automatically by looking at known locations for activation scripts.</p> <p>The <code>norecursedirs</code> option can be used to override the default list of pattern names that pytest should never recurse into</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>norecursedirs</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>artifacts _build docs</span>
</code></pre></div> <p>You can also use the <code>--collect-in-virtualenv</code> flag to skip the <code>virtualenv</code> detection.</p> <h4 id=pick-the-right-place-by-default-testpaths>Pick the right place by default: testpaths<a class=headerlink href=#pick-the-right-place-by-default-testpaths title="Permanent link">&para;</a></h4> <p>With tests separated from the application/library code in a tests or similarly named directory. In that layout it is useful to use the testpaths configuration option.</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>testpaths</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>tests</span>
</code></pre></div> <p>This will tell pytest where to look for tests when no files, directories, or node ids are given in the command line, which might speed up test collection. Note that you can configure more than one directory, separated by spaces.</p> <h4 id=override-options-with-o-override>Override options with -o/&ndash;override<a class=headerlink href=#override-options-with-o-override title="Permanent link">&para;</a></h4> <p>Finally, a little known feature is that you can override any configuration option directly in the command-line using the <code>-o</code> /<code>--override</code> flags. This flag can be passed multiple times to override more than one option.</p> <div class=highlight><pre><span></span><code><span class=na>pytest -o python_classes</span><span class=o>=</span><span class=s>Suite -o cache_dir=$TMP/pytest-cache</span>
</code></pre></div> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class=timeago datetime=2020-04-26T11:07:28+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2020-04-26</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.220ee61c.min.js></script> <script src=../../../../js/timeago.min.js></script> <script src=../../../../js/timeago_mkdocs_material.js></script> <script src=https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js></script> </body> </html>