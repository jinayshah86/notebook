<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This is mickey mouse's notebook"><meta name=author content="Mickey Mouse"><link href=https://jinayshah.netlify.com/python/testing/pytest/markers/ rel=canonical><link href=../tests/ rel=prev><link href=../parametrization/ rel=next><link rel=icon href=../../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.4.3, mkdocs-material-9.1.18"><title>Markers - Mickey Mouse's Notebook</title><link rel=stylesheet href=../../../../assets/stylesheets/main.26e3688c.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,300i,400,400i,700,700i%7CHack:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto Mono";--md-code-font:"Hack"}</style><link rel=stylesheet href=../../../../css/timeago.css><link rel=stylesheet href=https://unpkg.com/mermaid@7.1.2/dist/mermaid.css><script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#introduction class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../.. title="Mickey Mouse's Notebook" class="md-header__button md-logo" aria-label="Mickey Mouse's Notebook" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Mickey Mouse's Notebook </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Markers </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../.. title="Mickey Mouse's Notebook" class="md-nav__button md-logo" aria-label="Mickey Mouse's Notebook" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> Mickey Mouse's Notebook </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../.. class=md-nav__link> Home </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> Python <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../functions/ class=md-nav__link> Functions </a> </li> <li class=md-nav__item> <a href=../../../decorators/ class=md-nav__link> Decorators </a> </li> <li class=md-nav__item> <a href=../../../oop/ class=md-nav__link> OOP </a> </li> <li class=md-nav__item> <a href=../../../iterators/ class=md-nav__link> Iterators </a> </li> <li class=md-nav__item> <a href=../../../files/ class=md-nav__link> Working with files </a> </li> <li class=md-nav__item> <a href=../../../custom-json/ class=md-nav__link> Custom JSON Encoder/Decoder </a> </li> <li class=md-nav__item> <a href=../../../pickling/ class=md-nav__link> Pickling </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8 checked> <label class=md-nav__link for=__nav_2_8 id=__nav_2_8_label tabindex=0> Testing <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=true> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> Testing </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../introduction/ class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8_2 checked> <label class=md-nav__link for=__nav_2_8_2 id=__nav_2_8_2_label tabindex=0> Pytest <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_8_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2_8_2> <span class="md-nav__icon md-icon"></span> Pytest </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../tests/ class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Markers <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Markers </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#creating-marks class=md-nav__link> Creating marks </a> </li> <li class=md-nav__item> <a href=#running-tests-based-on-marks class=md-nav__link> Running tests based on marks </a> </li> <li class=md-nav__item> <a href=#applying-marks-to-classes class=md-nav__link> Applying marks to classes </a> </li> <li class=md-nav__item> <a href=#applying-marks-to-modules class=md-nav__link> Applying marks to modules </a> </li> <li class=md-nav__item> <a href=#custom-marks-and-pytestini class=md-nav__link> Custom marks and pytest.ini </a> </li> <li class=md-nav__item> <a href=#built-in-marks class=md-nav__link> Built-in marks </a> <nav class=md-nav aria-label="Built-in marks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#pytestmarkskipif class=md-nav__link> @pytest.mark.skipif </a> </li> <li class=md-nav__item> <a href=#pytestskip class=md-nav__link> pytest.skip </a> </li> <li class=md-nav__item> <a href=#pytestimportorskip class=md-nav__link> pytest.importorskip </a> </li> <li class=md-nav__item> <a href=#pytestmarkxfail class=md-nav__link> @pytest.mark.xfail </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../parametrization/ class=md-nav__link> Parametrization </a> </li> <li class=md-nav__item> <a href=../fixtures/ class=md-nav__link> Fixtures </a> </li> <li class=md-nav__item> <a href=../plugins/ class=md-nav__link> Plugins </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../tox/ class=md-nav__link> Tox </a> </li> <li class=md-nav__item> <a href=../../coverage/ class=md-nav__link> Coverage </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../exceptions/ class=md-nav__link> Exceptions </a> </li> <li class=md-nav__item> <a href=../../../profiling/ class=md-nav__link> Profiling </a> </li> <li class=md-nav__item> <a href=../../../concurrency/ class=md-nav__link> Concurrent Execution </a> </li> <li class=md-nav__item> <a href=../../../logging/ class=md-nav__link> Logging </a> </li> <li class=md-nav__item> <a href=../../../references/ class=md-nav__link> References </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> Introduction </a> </li> <li class=md-nav__item> <a href=#creating-marks class=md-nav__link> Creating marks </a> </li> <li class=md-nav__item> <a href=#running-tests-based-on-marks class=md-nav__link> Running tests based on marks </a> </li> <li class=md-nav__item> <a href=#applying-marks-to-classes class=md-nav__link> Applying marks to classes </a> </li> <li class=md-nav__item> <a href=#applying-marks-to-modules class=md-nav__link> Applying marks to modules </a> </li> <li class=md-nav__item> <a href=#custom-marks-and-pytestini class=md-nav__link> Custom marks and pytest.ini </a> </li> <li class=md-nav__item> <a href=#built-in-marks class=md-nav__link> Built-in marks </a> <nav class=md-nav aria-label="Built-in marks"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#pytestmarkskipif class=md-nav__link> @pytest.mark.skipif </a> </li> <li class=md-nav__item> <a href=#pytestskip class=md-nav__link> pytest.skip </a> </li> <li class=md-nav__item> <a href=#pytestimportorskip class=md-nav__link> pytest.importorskip </a> </li> <li class=md-nav__item> <a href=#pytestmarkxfail class=md-nav__link> @pytest.mark.xfail </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>Markers</h1> <h3 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h3> <p>Pytest allows you to mark functions and classes with metadata. This metadata can be used to selectively run tests, and is also available for fixtures and plugins, to perform different tasks. </p> <h3 id=creating-marks>Creating marks<a class=headerlink href=#creating-marks title="Permanent link">&para;</a></h3> <p>Marks are created with the <code>@pytest.mark</code> decorator. It works as a factory, so any access to it will automatically create a new mark and apply it to a function.</p> <p><strong>Example:</strong> <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>slow</span>
<span class=k>def</span> <span class=nf>test_long_computation</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div></p> <p>By using the <code>@pytest.mark.slow</code> decorator, you are applying a mark named <code>slow</code> to <code>test_long_computation</code>.</p> <p>Marks can also receive <strong>parameters</strong>: <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>timeout</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&quot;thread&quot;</span><span class=p>)</span>
<span class=k>def</span> <span class=nf>test_topology_sort</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div> With this, we define that <code>test_topology_sort</code> should not take more than <strong>10</strong> seconds, in which case it should be terminated using the <strong>thread</strong> method.</p> <p>You can add more than one mark to a test by applying the @pytest.mark decorator multiple times:</p> <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>slow</span>
<span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>timeout</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>method</span><span class=o>=</span><span class=s2>&quot;thread&quot;</span><span class=p>)</span>
<span class=k>def</span> <span class=nf>test_topology_sort</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div> <h3 id=running-tests-based-on-marks>Running tests based on marks<a class=headerlink href=#running-tests-based-on-marks title="Permanent link">&para;</a></h3> <p>Run tests by using marks as selection factors with the -m flag. For example, to run all tests with the slow mark: <div class=highlight><pre><span></span><code>pytest<span class=w> </span>-m<span class=w> </span>slow
</code></pre></div></p> <p>The <code>-m</code> flag also accepts expressions, so you can do a more advanced selection. To run all tests with the slow mark but not the tests with the serial mark you can use:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>-m<span class=w> </span><span class=s2>&quot;slow and not serial&quot;</span>
</code></pre></div> <p>The expression is limited to the <code>and</code>, <code>not</code>, and <code>or</code> operators.</p> <p>Custom marks can be useful for optimizing test runs on your CI system. Oftentimes, environment problems, missing dependencies, or even some code committed by mistake might cause the entire test suite to fail. By using marks, you can choose a few tests that are fast and/or broad enough to detect problems in a good portion of the code and then run those first, before all the other tests. If any of those tests fail, we abort the job and avoid wasting potentially a lot of time by running all tests that are doomed to fail anyway.</p> <p>We start by applying a custom mark to those tests. Any name will do, but a common name used is <code>smoke</code>, as in <strong>smoke detector</strong>, to detect problems before everything bursts into flames.</p> <p>You then run smoke tests first, and only after they pass do, you run the complete test suite:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>-m<span class=w> </span><span class=s2>&quot;smoke&quot;</span>
...
pytest<span class=w> </span>-m<span class=w> </span><span class=s2>&quot;not smoke&quot;</span>
</code></pre></div> <h3 id=applying-marks-to-classes>Applying marks to classes<a class=headerlink href=#applying-marks-to-classes title="Permanent link">&para;</a></h3> <p>You can apply the <code>@pytest.mark</code> decorator to a class. This will apply that same mark to all tests methods in that class, avoiding have to copy and paste the mark code over all test methods:</p> <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>timeout</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
<span class=k>class</span> <span class=nc>TestCore</span><span class=p>:</span>

    <span class=k>def</span> <span class=nf>test_simple_simulation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=o>...</span>

    <span class=k>def</span> <span class=nf>test_compute_tracers</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=o>...</span>
</code></pre></div> <p>However, there is one difference: applying the <code>@pytest.mark</code> decorator to a class means that all its subclasses will inherit the mark. Subclassing test classes is not common, but it is sometimes a useful technique to avoid repeating test code, or to ensure that implementations comply with a certain interface</p> <h3 id=applying-marks-to-modules>Applying marks to modules<a class=headerlink href=#applying-marks-to-modules title="Permanent link">&para;</a></h3> <p>We can also apply a mark to all test functions and test classes in a module. Just declare a global variable named <code>pytestmark</code>:</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>pytest</span>

<span class=n>pytestmark</span> <span class=o>=</span> <span class=n>pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>timeout</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>


<span class=k>class</span> <span class=nc>TestCore</span><span class=p>:</span>

    <span class=k>def</span> <span class=nf>test_simple_simulation</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=o>...</span>


<span class=k>def</span> <span class=nf>test_compute_tracers</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div> <p>You can use a tuple or list of marks to apply multiple marks as well:</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>pytest</span>

<span class=n>pytestmark</span> <span class=o>=</span> <span class=p>[</span><span class=n>pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>slow</span><span class=p>,</span> <span class=n>pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>timeout</span><span class=p>(</span><span class=mi>10</span><span class=p>)]</span>
</code></pre></div> <h3 id=custom-marks-and-pytestini>Custom marks and <code>pytest.ini</code><a class=headerlink href=#custom-marks-and-pytestini title="Permanent link">&para;</a></h3> <p>Being able to declare new marks on the fly just by applying the <code>pytest.mark</code> decorator is convenient. It makes it a breeze to quickly start enjoying the benefits of using marks.</p> <p>This convenience comes at a price: it is possible for a user to make a typo in the mark name, for example <code>@pytest.mark.smoek</code>, instead of <code>@pytest.mark.smoke</code>. Depending on the project under testing, this typo might be a mere annoyance or a more serious problem.</p> <p>Mature test suites that have a fixed set of marks might declare them in the <code>pytest.ini</code> file:</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>markers</span><span class=w> </span><span class=o>=</span>
<span class=w>    </span><span class=na>slow</span>
<span class=w>    </span><span class=na>serial</span>
<span class=w>    </span><span class=na>smoke</span><span class=o>:</span><span class=w> </span><span class=s>quick tests that cover a good portion of the code</span>
<span class=w>    </span><span class=na>unittest</span><span class=o>:</span><span class=w> </span><span class=s>unit tests for basic functionality</span>
<span class=w>    </span><span class=na>integration</span><span class=o>:</span><span class=w> </span><span class=s>cover to cover functionality testing</span><span class=w>    </span>
</code></pre></div> <p>The markers option accepts a list of markers in the form of <code>&lt;name&gt;: description</code>, with the description part being optional (<code>slow</code> and <code>serial</code> in the above example don&rsquo;t have a description).</p> <p>A full list of marks can be displayed by using the <code>--markers</code> flag:</p> <div class=highlight><pre><span></span><code>Î»<span class=w> </span>pytest<span class=w> </span>--markers
@pytest.mark.slow:

@pytest.mark.serial:

@pytest.mark.smoke:<span class=w> </span>quick<span class=w> </span>tests<span class=w> </span>that<span class=w> </span>cover<span class=w> </span>a<span class=w> </span>good<span class=w> </span>portion<span class=w> </span>of<span class=w> </span>the<span class=w> </span>code

@pytest.mark.unittest:<span class=w> </span>unit<span class=w> </span>tests<span class=w> </span><span class=k>for</span><span class=w> </span>basic<span class=w> </span>functionality

@pytest.mark.integration:<span class=w> </span>cover<span class=w> </span>to<span class=w> </span>cover<span class=w> </span>functionality<span class=w> </span>testing

...
</code></pre></div> <p>The <code>--strict</code> flag makes it an error to use marks not declared in the <code>pytest.ini</code> file. Using our previous example with a typo, we now obtain an error, instead of pytest silently creating the mark when running with <code>--strict</code>:</p> <div class=highlight><pre><span></span><code>pytest<span class=w> </span>--strict<span class=w> </span>tests<span class=se>\t</span>est_wrong_mark.py
...
collected<span class=w> </span><span class=m>0</span><span class=w> </span>items<span class=w> </span>/<span class=w> </span><span class=m>1</span><span class=w> </span><span class=nv>errors</span>

<span class=o>==============================</span><span class=w> </span><span class=nv>ERRORS</span><span class=w> </span><span class=o>===============================</span>
_____________<span class=w> </span>ERROR<span class=w> </span>collecting<span class=w> </span>tests/test_wrong_mark.py<span class=w> </span>_____________
tests<span class=se>\t</span>est_wrong_mark.py:4:<span class=w> </span><span class=k>in</span><span class=w> </span>&lt;module&gt;
<span class=w>    </span>@pytest.mark.smoek
..<span class=se>\.</span>.<span class=se>\.</span>env36<span class=se>\l</span>ib<span class=se>\s</span>ite-packages<span class=se>\_</span>pytest<span class=se>\m</span>ark<span class=se>\s</span>tructures.py:311:<span class=w> </span><span class=k>in</span><span class=w> </span>__getattr__
<span class=w>    </span>self._check<span class=o>(</span>name<span class=o>)</span>
..<span class=se>\.</span>.<span class=se>\.</span>env36<span class=se>\l</span>ib<span class=se>\s</span>ite-packages<span class=se>\_</span>pytest<span class=se>\m</span>ark<span class=se>\s</span>tructures.py:327:<span class=w> </span><span class=k>in</span><span class=w> </span>_check
<span class=w>    </span>raise<span class=w> </span>AttributeError<span class=o>(</span><span class=s2>&quot;%r not a registered marker&quot;</span><span class=w> </span>%<span class=w> </span><span class=o>(</span>name,<span class=o>))</span>
E<span class=w> </span>AttributeError:<span class=w> </span><span class=s1>&#39;smoek&#39;</span><span class=w> </span>not<span class=w> </span>a<span class=w> </span>registered<span class=w> </span>marker
!!!!!!!!!!!!!!<span class=w> </span>Interrupted:<span class=w> </span><span class=m>1</span><span class=w> </span>errors<span class=w> </span>during<span class=w> </span>collection<span class=w> </span>!!!!!!!!!!!!!!
<span class=o>======================</span><span class=w> </span><span class=m>1</span><span class=w> </span>error<span class=w> </span><span class=k>in</span><span class=w> </span><span class=m>0</span>.09<span class=w> </span><span class=nv>seconds</span><span class=w> </span><span class=o>======================</span>
</code></pre></div> <p>Test suites that want to ensure that all marks are registered in pytest.ini should also use addopts:</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>addopts</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>--strict</span>
<span class=na>markers</span><span class=w> </span><span class=o>=</span>
<span class=w>    </span><span class=na>slow</span>
<span class=w>    </span><span class=na>serial</span>
<span class=w>    </span><span class=na>smoke</span><span class=o>:</span><span class=w> </span><span class=s>quick tests that cover a good portion of the code</span>
<span class=w>    </span><span class=na>unittest</span><span class=o>:</span><span class=w> </span><span class=s>unit tests for basic functionality</span>
<span class=w>    </span><span class=na>integration</span><span class=o>:</span><span class=w> </span><span class=s>cover to cover functionality testing</span>
</code></pre></div> <h3 id=built-in-marks>Built-in marks<a class=headerlink href=#built-in-marks title="Permanent link">&para;</a></h3> <h4 id=pytestmarkskipif><code>@pytest.mark.skipif</code><a class=headerlink href=#pytestmarkskipif title="Permanent link">&para;</a></h4> <p>You might have tests that should not be executed unless some condition is met. For example, some tests might depend on certain libraries that are not always installed, or a local database that might not be online, or are executed only on certain platforms. </p> <p>Pytest provides a built-in mark, <code>skipif</code>, that can be used to <strong>skip</strong> tests based on specific conditions. Skipped tests are not executed if the condition is true, and are not counted towards test suite failures.</p> <p><strong>Example:</strong> you can use the <code>skipif</code> mark to always skip a test when executing on Windows:</p> <div class=highlight><pre><span></span><code><span class=kn>import</span> <span class=nn>sys</span>
<span class=kn>import</span> <span class=nn>pytest</span>

<span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>skipif</span><span class=p>(</span>
    <span class=n>sys</span><span class=o>.</span><span class=n>platform</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&quot;win&quot;</span><span class=p>),</span>
    <span class=n>reason</span><span class=o>=</span><span class=s2>&quot;fork not available on Windows&quot;</span><span class=p>,</span>
<span class=p>)</span>
<span class=k>def</span> <span class=nf>test_spawn_server_using_fork</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div> <p>The first argument to <code>@pytest.mark.skipif</code> is the condition: in this example, we are telling pytest to skip this test in Windows. The <code>reason=</code> keyword argument is mandatory and is used to display why the test was skipped when using the <code>-ra</code> flag:</p> <div class=highlight><pre><span></span><code><span class=w> </span>tests<span class=se>\t</span>est_skipif.py<span class=w> </span>s<span class=w>                                        </span><span class=o>[</span><span class=m>100</span>%<span class=o>]</span>
<span class=o>======================</span><span class=w> </span>short<span class=w> </span><span class=nb>test</span><span class=w> </span>summary<span class=w> </span><span class=nv>info</span><span class=w> </span><span class=o>======================</span>
SKIP<span class=w> </span><span class=o>[</span><span class=m>1</span><span class=o>]</span><span class=w> </span>tests<span class=se>\t</span>est_skipif.py:6:<span class=w> </span>fork<span class=w> </span>not<span class=w> </span>available<span class=w> </span>on<span class=w> </span><span class=nv>Windows</span>
<span class=o>=====================</span><span class=w> </span><span class=m>1</span><span class=w> </span>skipped<span class=w> </span><span class=k>in</span><span class=w> </span><span class=m>0</span>.02<span class=w> </span><span class=nv>seconds</span><span class=w> </span><span class=o>=====================</span>
</code></pre></div> <p>Checking capabilities and features is usually a better approach, instead of checking platforms and library version numbers.</p> <h4 id=pytestskip><code>pytest.skip</code><a class=headerlink href=#pytestskip title="Permanent link">&para;</a></h4> <p>The <code>@pytest.mark.skipif</code> decorator is very handy, but the mark must evaluate the condition at <code>import</code>/<code>collection</code> time, to determine whether the test should be skipped.</p> <p>Sometimes, it might even be almost impossible (without some gruesome hack) to check whether a test should be skipped during import time. For example, you can make the decision to skip a test based on the capabilities of the graphics driver only after initializing the underlying graphics library, and initializing the graphics subsystem is definitely not something you want to do at import time.</p> <p>For those cases, pytest lets you skip tests imperatively in the test body by using the <code>pytest.skip</code> function</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_shaders</span><span class=p>():</span>
    <span class=n>initialize_graphics</span><span class=p>()</span>
    <span class=k>if</span> <span class=ow>not</span> <span class=n>supports_shaders</span><span class=p>():</span>
        <span class=n>pytest</span><span class=o>.</span><span class=n>skip</span><span class=p>(</span><span class=s2>&quot;shades not supported in this driver&quot;</span><span class=p>)</span>
<span class=c1># rest of the test code</span>
    <span class=o>...</span>
</code></pre></div> <p><code>pytest.skip</code> works by raising an internal exception, so it follows normal Python exception semantics, and nothing else needs to be done for the test to be skipped properly.</p> <h4 id=pytestimportorskip><code>pytest.importorskip</code><a class=headerlink href=#pytestimportorskip title="Permanent link">&para;</a></h4> <p>It is common for libraries to have tests that depend on a certain library being installed. For example, pytest&rsquo;s own test suite has some tests for <code>numpy</code> arrays, which should be skipped if <code>numpy</code> is not installed.</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_tracers_as_arrays</span><span class=p>():</span>
    <span class=n>numpy</span> <span class=o>=</span> <span class=n>pytest</span><span class=o>.</span><span class=n>importorskip</span><span class=p>(</span><span class=s2>&quot;numpy&quot;</span><span class=p>)</span>
    <span class=o>...</span>
</code></pre></div> <p><code>pytest.importorskip</code> will import the module and return the module object, or skip the test entirely if the module could not be imported.</p> <p>If your test requires a minimum version of the library, <code>pytest.importorskip</code> also supports a <code>minversion</code> argument:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_tracers_as_arrays_114</span><span class=p>():</span>
    <span class=n>numpy</span> <span class=o>=</span> <span class=n>pytest</span><span class=o>.</span><span class=n>importorskip</span><span class=p>(</span><span class=s2>&quot;numpy&quot;</span><span class=p>,</span> <span class=n>minversion</span><span class=o>=</span><span class=s2>&quot;1.14&quot;</span><span class=p>)</span>
    <span class=o>...</span>
</code></pre></div> <h4 id=pytestmarkxfail><code>@pytest.mark.xfail</code><a class=headerlink href=#pytestmarkxfail title="Permanent link">&para;</a></h4> <p>You can use <code>@pytest.mark.xfail</code> decorator to indicate that a test is <strong>expected to fail</strong>.</p> <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span>
<span class=k>def</span> <span class=nf>test_simulation_34</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div> <p>This mark supports some parameters, all of which we will see later in this section; but one in particular warrants discussion now: the <code>strict</code> parameter. This parameter defines two distinct behaviors for the mark:</p> <ul> <li><code>strict=False</code>(the default), the test will be counted separately as an <code>XPASS</code>(if it passes) or an <code>XFAIL</code>(if it fails), and willnot fail the test suite</li> <li><code>strict=True</code>, the test will be marked as <code>XFAIL</code> if it fails, but if it unexpectedly passes, it will fail the test suite, as a normal failing test would.</li> </ul> <p>There are a few situations where this comes in handy.</p> <p>The first situation is when a test always fails, and you want to be told (loudly) if it suddenly starts passing. This can happen when:</p> <ul> <li> <p>You found out that the cause of a bug in your code is due to a problem in a third-party library. In this situation, you can write a failing test that demonstrates the problem, and mark it with <code>@pytest.mark.xfail(strict=True)</code>. If the test fails, the test will be marked as <code>XFAIL</code> in the test session summary, but if the test passes, it will fail the test suite. This test might start to pass when you upgrade the library that was causing the problem, and this will alert you that the issue has been fixed and needs your attention.</p> </li> <li> <p>You have thought about a new feature, and design one or more test cases that exercise it even before your start implementing it. You can commit the tests with the <code>@pytest.mark.xfail(strict=True)</code> mark applied, and remove the mark from the tests as you code the new feature. This is very useful in a collaborative environment, where one person supplies tests on how they envision a new feature/API, and another person implements it based on the test cases.</p> </li> <li> <p>You discover a bug in your application and write a test case demonstrating the problem. You might not have the time to tackle it right now or another person would be better suited to work in that part of the code. In this situation, marking the test as <code>@pytest.mark.xfail(strict=True)</code> would be a good approach.</p> </li> </ul> <p>The other situation where the xfail mark is useful is when you have tests that fail sometimes, also called <strong>flakytests</strong>. Flaky tests are tests that fail on occasion, even if the underlying code has not changed. There are many reasons why tests fail in a way that appears to be random; the following are a few:</p> <ul> <li>Timing issues in multi threaded code</li> <li>Intermittent network connectivity problems</li> <li>Tests that don&rsquo;t deal with asynchronous events correctly</li> <li>Relying on non-deterministic behavior</li> </ul> <p>Flaky tests are a serious problem, because the test suite is supposed to be an indicator that the code is working as intended and that it can detect real problems when they happen. Flaky tests destroy that image, because often developers will see flaky tests failing that don&rsquo;t have anything to do with recent code changes. When this becomes commonplace, people begin to just run the test suite again in the hope that this time the flaky test passes (and it often does), but this erodes the trust in the test suite as a whole, and brings frustration to the development team. You should treat flaky tests as a menace that should be contained and dealt with.</p> <p>Here are some suggestions regarding how to deal with flaky tests within a development team:</p> <ul> <li>First, you need to be able to correctly identify flaky tests. If a test fails that apparently doesn&rsquo;t have anything to do with the recent changes, run the tests again. If the test that failed previously now <strong>passes</strong>, it means the test is <strong>flaky</strong>.</li> <li>Create an issue to deal with that particular flaky test in your ticket system. Use a naming convention or other means to label that issue as related to a flaky test (for example GitHub or JIRA labels).</li> <li>Apply the <code>@pytest.mark.xfail(reason="flaky test #123", strict=False)</code> mark making sure to include the issue ticket number or identifier. Feel free to add more information to the description, if you like.</li> <li>Make sure to periodically assign issues about flaky tests to yourself or other team members (for example, during sprint planning). The idea is to take care of flaky tests at a comfortable pace, eventually reducing or eliminating them altogether.</li> </ul> <p><strong><code>@pytest.mark.xfail</code> full signature:</strong> <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span><span class=n>condition</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=o>*</span><span class=p>,</span> <span class=n>reason</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>raises</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>run</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>strict</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</code></pre></div></p> <ul> <li> <p><code>condition</code>: the first parameter, if given, is a True/False condition, similar to the one used by <code>@pytest.mark.skipif</code>: if <code>False</code>, the <code>xfail</code> mark is ignored. It is useful to mark a test as <code>xfail</code> based on an external condition, such as the platform, Python version, library version, and so on. <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span>
    <span class=n>sys</span><span class=o>.</span><span class=n>platform</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&quot;win&quot;</span><span class=p>),</span>
    <span class=n>reason</span><span class=o>=</span><span class=s2>&quot;flaky on Windows #42&quot;</span><span class=p>,</span> <span class=n>strict</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=k>def</span> <span class=nf>test_login_dialog</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div></p> </li> <li> <p><code>reason</code>: a string that will be shown in the short test summary when the <code>-ra</code> flag is used. It is highly recommended to always use this parameter to explain the reason why the test has been marked as <code>xfail</code> and/or include a ticket number. <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span>
    <span class=n>sys</span><span class=o>.</span><span class=n>platform</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&quot;win&quot;</span><span class=p>),</span>
    <span class=n>reason</span><span class=o>=</span><span class=s2>&quot;flaky on Windows #42&quot;</span><span class=p>,</span> <span class=n>strict</span><span class=o>=</span><span class=kc>False</span>
<span class=p>)</span>
<span class=k>def</span> <span class=nf>test_login_dialog</span><span class=p>():</span>
    <span class=o>...</span>
</code></pre></div></p> </li> <li> <p><code>raises</code>: given an exception type, it declares that we expect the test to raise an instance of that exception. If the test raises another type of exception (even <code>AssertionError</code>), the test will fail normally. It is especially useful for missing functionality or to test for known bugs. <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span><span class=n>raises</span><span class=o>=</span><span class=ne>NotImplementedError</span><span class=p>,</span>
                   <span class=n>reason</span><span class=o>=</span><span class=s1>&#39;will be implemented in #987&#39;</span><span class=p>)</span>
<span class=k>def</span> <span class=nf>test_credential_check</span><span class=p>():</span>
    <span class=n>check_credentials</span><span class=p>(</span><span class=s1>&#39;Hawkwood&#39;</span><span class=p>)</span> <span class=c1># not implemented yet</span>
</code></pre></div></p> </li> <li> <p><code>run</code>: if <code>False</code>, the test will not even be executed and will fail as <code>XFAIL</code>. This is particularly useful for tests that run code that might crash the test-suite process (for example, C/C++ extensions causing a segmentation fault due to a known problem). <div class=highlight><pre><span></span><code><span class=nd>@pytest</span><span class=o>.</span><span class=n>mark</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span>
    <span class=n>run</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>reason</span><span class=o>=</span><span class=s2>&quot;undefined particles cause a crash #625&quot;</span>
<span class=p>)</span>
<span class=k>def</span> <span class=nf>test_undefined_particle_collision_crash</span><span class=p>():</span>
    <span class=n>collide</span><span class=p>(</span><span class=n>Particle</span><span class=p>(),</span> <span class=n>Particle</span><span class=p>())</span>
</code></pre></div></p> </li> <li> <p><code>strict</code>: if <code>True</code>, a passing test will fail the test suite. If <code>False</code>, the test will not fail the test suite regardless of the outcome (the default is <code>False</code>).</p> </li> </ul> <p>The configuration variable <code>xfail_strict</code> controls the default value of the <code>strict</code> parameter of <code>xfail</code> marks:</p> <div class=highlight><pre><span></span><code><span class=k>[pytest]</span>
<span class=na>xfail_strict</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=s>True</span>
</code></pre></div> <p>Setting it to <code>True</code> means that all <code>xfail</code>-marked tests without an explicit strict parameter are considered an actual failure expectation instead of a flaky test. Any <code>xfail</code> mark that explicitly passes the <code>strict</code> parameter overrides the configuration value.</p> <p>Finally, you can imperatively trigger an <code>XFAIL</code> result within a test by calling the <code>pytest.xfail</code> function:</p> <div class=highlight><pre><span></span><code><span class=k>def</span> <span class=nf>test_particle_splitting</span><span class=p>():</span>
    <span class=n>initialize_physics</span><span class=p>()</span>
    <span class=kn>import</span> <span class=nn>numpy</span>
    <span class=k>if</span> <span class=n>numpy</span><span class=o>.</span><span class=n>__version__</span> <span class=o>&lt;</span> <span class=s2>&quot;1.13&quot;</span><span class=p>:</span>
        <span class=n>pytest</span><span class=o>.</span><span class=n>xfail</span><span class=p>(</span><span class=s2>&quot;split computation fails with numpy &lt; 1.13&quot;</span><span class=p>)</span>
    <span class=o>...</span>
</code></pre></div> <p>Similar to <code>pytest.skip</code>, this is useful when you can only determine whether you need to mark a test as xfail at runtime.</p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class=timeago datetime=2020-04-26T11:07:28+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2020-04-26</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../..", "features": [], "search": "../../../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../../assets/javascripts/bundle.220ee61c.min.js></script> <script src=../../../../js/timeago.min.js></script> <script src=../../../../js/timeago_mkdocs_material.js></script> <script src=https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js></script> </body> </html>